{
  "id": "1a2b3c4d5e6f20253VibETen",
  "title": "3VibETensor: System Software for Deep Learning, Fully Generated by AI Agents",
  "authors": ["Bing Xu", "Terry Chen", "Fengzhe Zhou", "Tianqi Chen", "Yangqing Jia", "Vinod Grover", "Haicheng Wu", "Wei Liu", "Craig Wittenbrink", "Wen- mei Hwu", "Roger Bringmann", "Ming- Yu Liu", "Luis Ceze", "Michael Lightstone", "Humphrey Shi"],
  "year": 2026,
  "conference": "arXiv",
  "category": "机器学习系统",
  "keywords": ["AI辅助软件工程", "深度学习系统软件", "大语言模型", "编程智能体", "系统生成", "VIBETENSOR", "GPU运行时"],
  "abstract": "VIBETensor is an open- source research system software stack for deep learning, generated by LLM- powered coding agents under high- level human guidance. In this paper, \"fully generated\" refers to code provenance: implementation changes were produced and applied as agent- proposed diffs; validation relied on builds, tests, and differential checks executed by the agent workflow, without per- change manual diff review. It implements a PyTorch- style eager tensor library with a \\(\\mathrm{C + + 20}\\) core (CPU+CUDA), a torch- like Python overlay via nanobind [1], and an experimental Node.js/TypeScript interface. Unlike thin bindings, VIBETensor includes its own tensor/storage system, schema- litc dispatcher, reverse- mode autograd engine, CUDA runtime (streams/events/graphs [2]), a stream- ordered caching allocator with diagnostics, and a stable C ABI for dynamically loaded operator plugins. We view this open- source release as a milestone for AI- assisted software engineering: it demonstrates that coding agents can generate a coherent deep learning runtime spanning language bindings down to CUDA memory management, with validation constrained by builds and tests. We describe the architecture, summarize the workflow used to produce and validate the system, and evaluate the resulting artifact. We report repository scale and test- suite composition, and summarize reproducible microbenchmarks from an accompanying AI- generated kernel suite, including fused attention measured against PyTorch SDPA/FlashAttention [3]. We also report end- to- end training sanity checks on three small workloads (sequence reversal, CIFAR- 10 ViT, and a miniGPT- style model) on NVIDIA H100 (Hopper, SM90) and Blackwell- class GPUs; multi- GPU results are Blackwell- only and rely on an optional CUTLASS- based ring- allreduce plugin gated on CUDA \\(^{13 + }\\) and sm103a toolchain support. Finally, we discuss failure modes that arise in generated system software—in particular a \"Frankenstein\" composition effect where locally correct subsystems can interact to produce globally suboptimal performance. We open- source the resulting system software and evaluation artifacts at https://github.com/NVlabs/vibetensor."
}