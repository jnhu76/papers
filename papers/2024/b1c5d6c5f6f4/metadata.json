{
  "id": "b1c5d6c5f6f4",
  "title": "λScale: Enabling Fast Scaling for Serverless Large Language Model Inference",
  "authors": ["Minchen Yu", "Rui Yang", "Chaobo Jia", "Zhaoyuan Su", "Sheng Yao", "Tingfeng Lan", "Yuchen Yang", "Yue Cheng", "Wei Wang", "Ao Wang", "Ruichuan Chen"],
  "year": 2024,
  "conference": "OSDI",
  "category": "分布式系统",
  "keywords": ["Serverless计算", "大语言模型推理", "快速扩缩容", "RDMA", "执行即加载", "分布式推理", "λPipe"],
  "abstract": "Serverless computing has emerged as a compelling solution for cloud-based model inference. However, as modern large language models (LLMs) continue to grow in size, existing serverless platforms often face substantial model startup overhead. This poses a significant challenge in efficiently scaling model instances to accommodate dynamic, bursty workloads commonly observed in real-world inference services. In this paper, we introduce λScale, an efficient serverless inference system to achieve fast model scaling. The key idea behind λScale is to leverage high-speed RDMA networks between GPU nodes for fast model multicast, while enabling distributed inference execution during model transmission--referred to as \"execute-while-load\". λScale proposes an efficient model scaling scheme, λPipe, which supports adaptive model multicast and dynamically constructs execution pipelines across receiving nodes for collaborative, distributed inference. Additionally, λScale supports efficient model management across GPU and host memory, allowing fast scaling for models across different storage tiers. Evaluation results show that λScale enables fast model scaling and effectively handles load spikes, achieving up to 5× tail-latency improvement and 31.3% cost reduction compared to state-of-the-art solutions on real-world LLM inference traces."
}