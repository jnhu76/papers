{
    "id": "db8a1f374b5e2025Efficient",
    "title": "Efficient Attention Methods: Hardware-efficient, Sparse, Compact, and Linear Attention",
    "authors": ["Jintao Zhang", "Rundong Su", "Chunyu Liu", "Jia Wei", "Ziteng Wang", "Haoxu Wang", "Pengle Zhang", "Huiqiang Jiang", "Haofeng Huang", "Chendong Xiang", "Haocheng Xi", "Shuo Yang", "Xingyang Li", "Yilong Zhao", "Yuezhou Hu", "Tianyu Fu", "Tianchen Zhao", "Yicheng Zhang", "Boqun Cao", "Youhe Jiang", "Kai Jiang", "Huayu Chen", "Min Zhao", "Xiaoming Xu", "Yi Wu", "Fan Bao", "Ion Stoica", "Joseph E. Gonzalez", "Jianfei Chen", "Jun Zhu"],
    "year": 2025,
    "conference": "IEEE",
    "category": "机器学习",
    "keywords": ["高效注意力", "Transformer", "稀疏注意力", "紧凑注意力", "线性注意力", "Efficient Attention", "Hardware-efficient Attention"],
    "abstract": "In modern transformer architectures, the attention operation is the only component with a time complexity of O(N^2), whereas all other operations scale linearly as O(N), where N is the sequence length. Recently, a plethora of methods have been proposed to improve the computational efficiency of the attention operation, which are very critical for developing large-scale models. In this paper, we present a unified taxonomy and a comprehensive survey of efficient attention methods. We categorize existing approaches into four classes: (1) Hardware-efficient attention: Optimizes attention computation efficiency by leveraging hardware characteristics; (2) Sparse attention: Selectively performs a subset of computations in attention while omitting others; (3) Compact attention: Compresses the KV cache (e.g., via weight sharing or low-rank decomposition) without changing the computation cost of using a full-sized KV cache; and (4) Linear attention: Reformulate the attention computation to achieve O(N) time complexity. For each category, we also develop a unified analysis framework and offer perspectives on open challenges and promising directions for future research on efficient attention. To the best of our knowledge, this survey is the most comprehensive survey on efficient attention. Our taxonomy of hardware-efficient, compact, sparse, and linear attention covers nearly all existing efficient attention methods, which prior surveys do not."
}