{
  "id": "3a1b5c8d9e0f2025conditio",
  "title": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
  "authors": ["Xin Cheng", "Wangding Zeng", "Damai Dai", "Qinyu Chen", "Bingxuan Wang", "Zhenda Xie", "Kezhao Huang", "Xingkai Yu", "Zhewen Hao", "Yukun Li", "Han Zhang", "Huishuai Zhang", "Dongyan Zhao", "Wenfeng Liang"],
  "year": 2025,
  "conference": "ICLR",
  "category": "自然语言处理",
  "keywords": ["条件记忆", "Conditional Memory", "稀疏性", "Sparsity", "大语言模型", "Large Language Models", "Engram"],
  "abstract": "While Mixture-of-Experts (MoE) scales capacity via conditional computation, Transformers lack a native primitive for knowledge lookup, forcing them to inefficiently simulate retrieval through computation. To address this, we introduce conditional memory as a complementary sparsity axis, instantiated via **Engram**, a module that modernizes classic $N$-gram embedding for $O(1)$ lookup. By formulating the _Sparsity Allocation_ problem, we uncover a U-shaped scaling law that optimizes the trade-off between neural computation (MoE) and static memory (Engram). Guided by this law, we scale Engram to 27B parameters, achieving superior performance over a strictly iso-parameter and iso-FLOPs MoE baseline. Most notably, while the memory module is expected to aid knowledge retrieval (e.g., MMLU +3.4; CMMLU +4.0), we observe even larger gains in general reasoning (e.g., BBH +5.0; ARC-Challenge +3.7) and code/math domains (HumanEval +3.0; MATH +2.4). Mechanistic analyses reveal that Engram relieves the backbone's early layers from static reconstruction, effectively deepening the network for complex reasoning. Furthermore, by delegating local dependencies to lookups, it frees up attention capacity for global context, substantially boosting long-context retrieval (e.g., Multi-Query NIAH: 84.2 → 97.0). Finally, Engram establishes infrastructure-aware efficiency: its deterministic addressing enables runtime prefetching from host memory, incurring negligible overhead. We envision conditional memory as an indispensable modeling primitive for next-generation sparse models."
}