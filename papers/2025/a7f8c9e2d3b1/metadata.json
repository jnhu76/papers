{
  "id": "a7f8c9e2d3b1",
  "title": "LLMs Encode How Difficult Problems Are",
  "authors": ["William Lugoloobi", "Chris Russell"],
  "year": 2025,
  "conference": "arXiv",
  "category": "自然语言处理",
  "keywords": ["大语言模型", "问题难度", "线性探针", "强化学习", "数学推理", "人类判断", "GRPO"],
  "abstract": "Large language models exhibit a puzzling inconsistency:they solve complex problems yet frequently fail on seemingly simpler ones.We investigate whether LLMs internally encode problem difficulty in a way that aligns with human judgment,and whether this representation tracks generalization during reinforcement learning post-training.We train linear probes across layers and token positions on 60models,evaluating on mathematical and coding subsets of Easy2HardBench.We find that humanlabeled difficulty is strongly linearly decodable (AMC:ρ ≈0.88)and exhibits clear modelsize scaling,whereas LLM-derived difficulty is substantially weaker and scales poorly.Steering along the difficulty direction reveals that pushing models toward \"easier\"representations reduces hallucination and improves accuracy.During GRPO training on Qwen2.5-Math-1.5B,the human-difficulty probe strengthens and positively correlates with test accuracy across training steps,while the LLM-difficulty probe degrades and negatively correlates with $\\mathrm{p e r f o r}$-mance.These results suggest that human annotations provide a stable difficulty signal that RL amplifies,while automated difficulty estimates derived from model performance become misaligned precisely as models improve.We release probe code and evaluation scripts to facilitate replication."
}