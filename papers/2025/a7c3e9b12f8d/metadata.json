{
  "id": "a7c3e9b12f8d",
  "title": "Nested Learning: The Illusion of Deep Learning Architectures",
  "authors": ["Ali Behrouz", "Meisam Razaviyayn", "Peiling Zhong", "Vahab Mirrokni"],
  "year": 2025,
  "conference": "NeurIPS",
  "category": "机器学习",
  "keywords": ["嵌套学习", "关联记忆", "上下文学习", "深度优化器", "自修改模型", "连续记忆系统", "Hope架构"],
  "abstract": "Over the last decades, developing more powerful neural architectures and simultaneously designing optimization algorithms to effectively train them have been the core of research efforts to enhance the capability of machine learning models. Despite the recent progresses, particularly in developing Language Models (LMs), there are fundamental challenges and unanswered questions about how such models can _continually learn/memorize_, _self-improved_, _and find \"effective solutions_\". In this paper, we present a new learning paradigm, called Nested Learning (NL), that coherently represents a model with a set of nested, multi-level, and/or parallel optimization problems, each of which with its own \"_context flow_\". NL reveals that existing deep learning methods learns from data through _compressing_ their own context flow, and explain how _in-context learning_ emerges in large models. NL suggests a path (a new dimension to deep learning) to design more expressive learning algorithms with more \"_levels_\", resulting in higher-order in-context learning abilities. In addition to its neuroscientifically plausible and mathematically white-box nature, we advocate for its importance by presenting three core contributions: (1) Deep Optimizers: Based on NL, we show that well-known gradient-based optimizers (e.g., Adam, SGD with Momentum, etc.) are in fact associative memory modules that aim to compress the gradients with gradient descent. Building on this insight, we present a set of more expressive optimizers with deep memory and/or more powerful learning rules; (2) Self-Modifying Titans: Taking advantage of NL's insights on learning algorithms, we present a novel sequence model that learns how to modify itself by learning its own update algorithm; and (3) Continuum Memory System: We present a new formulation for memory system that generalizes the traditional viewpoint of \"long-term/short-term memory\". Combining our self-modifying sequence model with the continuum memory system, we present a learning module, called Hope, showing promising results in language modeling, continual learning, and long-context reasoning tasks."
}