<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>JENGA: Effective Memory Management for Serving LLM with Heterogeneity</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
  <style>
    body { 
      font-family: 'Inter', sans-serif; 
      scroll-behavior: smooth;
    }
    .mobile-optimized { 
      margin-bottom: 2rem !important; 
    }
    @media (max-width: 768px) {
      .content-section { 
        padding: 1rem; 
        margin-bottom: 1.5rem;
      }
      .technical-details {
        margin: 1rem 0;
      }
    }
    .hide-scrollbar {
      -ms-overflow-style: none;
      scrollbar-width: none;
    }
    .hide-scrollbar::-webkit-scrollbar {
      display: none;
    }
    .nav-item {
      @apply px-4 py-2 rounded-lg transition-colors duration-200 text-gray-600 hover:text-blue-600 hover:bg-blue-50;
    }
    .nav-item.active {
      @apply text-blue-600 bg-blue-100;
    }
  </style>
</head>
<body class="bg-gradient-to-br from-blue-400 via-purple-500 to-pink-400 min-h-screen">
  <!-- 导航系统 -->
  <nav class="nav-scroll bg-white/90 backdrop-blur-sm sticky top-0 z-50 border-b border-gray-200">
    <div class="container mx-auto px-4 py-3">
      <div class="flex overflow-x-auto space-x-6 hide-scrollbar">
        <a href="#abstract" class="nav-item whitespace-nowrap">摘要</a>
        <a href="#background-motivation" class="nav-item whitespace-nowrap">背景与动机</a>
        <a href="#challenges" class="nav-item whitespace-nowrap">问题与挑战</a>
        <a href="#design-implementation" class="nav-item whitespace-nowrap">设计与实现</a>
        <a href="#evaluation" class="nav-item whitespace-nowrap">测试与评估</a>
        <a href="#conclusion" class="nav-item whitespace-nowrap">结论</a>
      </div>
    </div>
  </nav>

  <div class="container mx-auto px-4 py-8">
    <div class="bg-white/95 backdrop-blur-sm rounded-xl shadow-lg p-6">
      <!-- 论文标题和元数据 -->
      <div class="mb-12 md:mb-16">
        <h1 class="text-3xl md:text-4xl font-bold text-gray-800 mb-6 text-center md:text-left">
          JENGA: Effective Memory Management for Serving LLM with Heterogeneity
        </h1>
        
        <div class="bg-gradient-to-r from-blue-50 to-purple-50 border-l-4 border-blue-500 p-6 rounded-r-lg">
          <div class="grid grid-cols-1 md:grid-cols-2 gap-4 text-gray-700">
            <div class="space-y-3">
              <div>
                <strong class="text-blue-700 block mb-1">作者信息</strong>
                <div class="text-lg">Chen Zhang, Kuntai Du, Shu Liu, Woosuk Kwon, Xiangxi Mo, Yufeng Wang, Xiaoxuan Liu, Kaichao You, Zhuohan Li, Mingsheng Long, Jidong Zhai, Joseph Gonzalez, Ion Stoica</div>
                <div class="text-sm text-gray-600 mt-1">清华大学, UC Berkeley, 芝加哥大学, 独立研究者</div>
              </div>
              <div>
                <strong class="text-blue-700 block mb-1">发表信息</strong>
                <div>ACM SIGOPS 31st Symposium on Operating Systems Principles (SOSP '25), October 13–16, 2025, Seoul, Republic of Korea</div>
              </div>
            </div>
            <div class="space-y-3">
              <div>
                <strong class="text-blue-700 block mb-1">DOI标识</strong>
                <div class="font-mono text-sm bg-white px-3 py-2 rounded border">10.1145/3731569.3764823</div>
              </div>
              <div>
                <strong class="text-blue-700 block mb-1">关键词</strong>
                <div class="flex flex-wrap gap-2 mt-1">
                  <span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm">LLM Serving</span>
                  <span class="bg-green-100 text-green-800 px-3 py-1 rounded-full text-sm">Memory Management</span>
                  <span class="bg-purple-100 text-purple-800 px-3 py-1 rounded-full text-sm">Heterogeneous Models</span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      
      <!-- 核心贡献突出显示 -->
      <div class="bg-gradient-to-r from-green-50 to-blue-50 border-l-4 border-green-500 p-6 rounded-r-lg mb-8">
        <h4 class="font-bold text-green-700 mb-4 flex items-center text-xl">
          <i class="fas fa-trophy mr-2"></i>核心贡献
        </h4>
        <ul class="list-disc list-inside space-y-2 text-gray-700">
          <li>识别了新型LLM架构在嵌入大小和注意力机制方面的异构性增长趋势</li>
          <li>提出了基于LCM的两级内存分配系统，有效支持不同大小的嵌入</li>
          <li>实现了可定制的缓存策略，最大化不同注意力机制的命中率</li>
          <li>开源实现Jenga，相比现有推理引擎提升1.46倍吞吐量</li>
        </ul>
      </div>
      
      <!-- 摘要 -->
      <section id="abstract" class="content-section mobile-optimized mb-12 md:mb-16">
        <h2 class="text-2xl md:text-3xl font-bold text-gray-800 mb-6 md:mb-8 flex items-center">
          <i class="fas fa-file-alt mr-3 text-blue-500"></i>
          摘要
        </h2>
        <div class="prose max-w-none text-gray-700">
          <p class="mb-4">大型语言模型(LLM)被广泛应用但运行成本高昂。为了降低成本，通过高效的GPU内存管理最大化请求批处理大小至关重要。现有方法(如PagedAttention)难以应对现代LLM，因为模型内部嵌入和注意力机制的大小异构性日益增长。</p>
          <p class="mb-4">作者提出了Jenga，一个针对这些异构LLM的内存分配框架。Jenga解决了两个关键挑战：(1)由不同大小的嵌入引起的内存碎片，(2)跨层变化的注意力机制导致的不可预测内存使用。Jenga采用注意力属性感知分配器，利用嵌入大小的最小公倍数(LCM)优化内存使用，并根据注意力模式执行缓存驱逐以增强内存重用。</p>
          <p>在vLLM中实现Jenga，并使用多样化的LLM、数据集和GPU进行评估。评估显示Jenga将GPU内存利用率提高了高达83%，服务吞吐量提高了高达2.16倍(平均1.46倍)。</p>
        </div>
      </section>
      
      <!-- 背景与动机 -->
      <section id="background-motivation" class="content-section mobile-optimized mb-12 md:mb-16">
        <h2 class="text-2xl md:text-3xl font-bold text-gray-800 mb-6 md:mb-8 flex items-center">
          <i class="fas fa-layer-group mr-3 text-blue-500"></i>
          背景与动机
        </h2>
        
        <div class="prose max-w-none text-gray-700">
          <h3 class="text-xl font-semibold text-gray-800 mb-4">LLM服务的内存挑战</h3>
          <p class="mb-4">大型语言模型在各种应用中变得越来越流行。因此，高效地服务LLM变得极其重要。提高服务吞吐量的常见策略是并行批处理多个请求。然而，批处理对GPU内存容量施加了巨大压力，随着批处理大小的增加，内存需求线性增长，因为每个请求需要缓存其整个前缀。因此，GPU内存容量成为高吞吐量的主要瓶颈。</p>
          
          <h3 class="text-xl font-semibold text-gray-800 mb-4">PagedAttention的局限性</h3>
          <p class="mb-4">PagedAttention是LLM服务中最广泛使用的内存管理算法。它将内存划分为固定大小的页面，并根据请求中的令牌数量动态分配页面给请求。其设计依赖于早期Transformer同质性驱动的两个假设：</p>
          <ul class="list-disc list-inside mb-4 space-y-2">
            <li><strong>固定大小的嵌入</strong>：不同令牌和层的嵌入(如KV缓存)大小相同</li>
            <li><strong>完全注意力</strong>：所有层都使用完全注意力，意味着每层都有包含所有令牌的关联KV缓存，并需要相同数量的页面</li>
          </ul>
          
          <h3 class="text-xl font-semibold text-gray-800 mb-4">异构LLM的兴起</h3>
          <p class="mb-4">最近的LLM架构更加异构，使上述两个假设失效：</p>
          <ul class="list-disc list-inside mb-4 space-y-2">
            <li>最近的模型通常具有不同大小的异构嵌入</li>
            <li>并非所有层都使用相同的注意力机制</li>
          </ul>
          
          <!-- 图1占位符 -->
          <div class="original-figure-container bg-white p-4 rounded-lg border border-gray-200 shadow-sm mb-6">
            <div class="flex flex-col sm:flex-row sm:items-center justify-between mb-3 gap-2">
              <h5 class="font-semibold text-gray-700 text-base md:text-lg">
                <i class="fas fa-sitemap mr-2 text-blue-500"></i>
                原图 1: Traditional LLMs (left) vs. latest LLMs (right)
              </h5>
              <span class="text-xs bg-blue-100 text-blue-800 px-2 py-1 rounded self-start sm:self-auto">论文原图</span>
            </div>
            
            <div class="image-placeholder bg-gradient-to-br from-gray-100 to-gray-200 p-6 md:p-8 rounded-lg text-center border-2 border-dashed border-gray-300">
              <img src="./images/fig1.png" alt="论文图1: 传统LLM(左)与最新LLM(右)对比" class="max-w-full h-auto rounded-lg">
            </div>
            
            <div class="mt-3 text-sm text-gray-600 bg-gray-50 p-3 rounded border">
              <strong>原图图注:</strong> LLMs are becoming more heterogeneous and produce KV caches with different sizes and numbers of tokens, which demands a new GPU memory manager design.
            </div>
          </div>
        </div>
      </section>
      
      <!-- 问题与挑战 -->
      <section id="challenges" class="content-section mobile-optimized mb-12 md:mb-16">
        <h2 class="text-2xl md:text-3xl font-bold text-gray-800 mb-6 md:mb-8 flex items-center">
          <i class="fas fa-exclamation-triangle mr-3 text-blue-500"></i>
          问题与挑战
        </h2>
        
        <div class="prose max-w-none text-gray-700">
          <h3 class="text-xl font-semibold text-gray-800 mb-4">异构LLM的类型</h3>
          <p class="mb-4">现代LLM超越了堆叠同质全上下文注意力层。引入了许多新型注意力，使LLM架构变得异构。作者在图4中突出了四个主要的异构性来源：</p>
          
          <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-6">
            <div class="bg-blue-50 p-4 rounded-lg border border-blue-200">
              <h4 class="font-bold text-blue-700 mb-2 flex items-center">
                <i class="fas fa-window-restore mr-2"></i>稀疏注意力
              </h4>
              <p class="text-sm">如滑动窗口注意力，仅关注前缀令牌的子集</p>
            </div>
            <div class="bg-green-50 p-4 rounded-lg border border-green-200">
              <h4 class="font-bold text-green-700 mb-2 flex items-center">
                <i class="fas fa-brain mr-2"></i>状态空间模型
              </h4>
              <p class="text-sm">使用大型固定大小张量捕获所有令牌信息</p>
            </div>
            <div class="bg-purple-50 p-4 rounded-lg border border-purple-200">
              <h4 class="font-bold text-purple-700 mb-2 flex items-center">
                <i class="fas fa-eye mr-2"></i>视觉语言模型
              </h4>
              <p class="text-sm">管理视觉嵌入缓存和LLM部分的KV缓存</p>
            </div>
            <div class="bg-yellow-50 p-4 rounded-lg border border-yellow-200">
              <h4 class="font-bold text-yellow-700 mb-2 flex items-center">
                <i class="fas fa-cubes mr-2"></i>多模型服务
              </h4>
              <p class="text-sm">如推测解码中同时运行两个模型</p>
            </div>
          </div>
          
          <h3 class="text-xl font-semibold text-gray-800 mb-4">内存碎片挑战</h3>
          <p class="mb-4">上述异构性需要为不同令牌分配不同大小的KV缓存。以Llama 3.2 11B Vision模型为例分析PagedAttention的碎片：</p>
          
          <!-- 图5占位符 -->
          <div class="original-figure-container bg-white p-4 rounded-lg border border-gray-200 shadow-sm mb-6">
            <div class="flex flex-col sm:flex-row sm:items-center justify-between mb-3 gap-2">
              <h5 class="font-semibold text-gray-700 text-base md:text-lg">
                <i class="fas fa-memory mr-2 text-blue-500"></i>
                原图 5: 不同内存分配策略对比
              </h5>
              <span class="text-xs bg-blue-100 text-blue-800 px-2 py-1 rounded self-start sm:self-auto">论文原图</span>
            </div>
            
            <div class="image-placeholder bg-gradient-to-br from-gray-100 to-gray-200 p-6 md:p-8 rounded-lg text-center border-2 border-dashed border-gray-300">
              <img src="./images/fig5.png" alt="论文图5: 不同内存分配策略对比" class="max-w-full h-auto rounded-lg">
            </div>
            
            <div class="mt-3 text-sm text-gray-600 bg-gray-50 p-3 rounded border">
              <strong>原图图注:</strong> Different memory allocation strategies for Llama 3.2 vision model. Assume the KV cache size per token of text tokens is 1.5x that of image tokens.
            </div>
          </div>
          
          <h3 class="text-xl font-semibold text-gray-800 mb-4">前缀缓存挑战</h3>
          <p class="mb-4">异构注意力机制也给前缀缓存带来挑战：</p>
          <ul class="list-disc list-inside mb-4 space-y-2">
            <li><strong>定制前缀缓存策略以最大化命中率</strong>：不同注意力机制依赖不同的令牌子集</li>
            <li><strong>批处理大小与前缀缓存命中率之间的权衡</strong>：分配更多缓存空间给非活动令牌提高命中率，但限制了批处理大小</li>
          </ul>
        </div>
      </section>
      
      <!-- 设计与实现 -->
      <section id="design-implementation" class="content-section mobile-optimized mb-12 md:mb-16">
        <h2 class="text-2xl md:text-3xl font-bold text-gray-800 mb-6 md:mb-8 flex items-center">
          <i class="fas fa-cogs mr-3 text-blue-500"></i>
          设计与实现
        </h2>
        
        <div class="prose max-w-none text-gray-700">
          <h3 class="text-xl font-semibold text-gray-800 mb-4">JENGA系统概述</h3>
          <p class="mb-4">LLM的异构性促使作者设计JENGA，一个层属性感知的LLM内存管理系统，为不同注意力机制定制内存分配和前缀缓存策略。JENGA的概述如图7所示。</p>
          
          <!-- 图7占位符 -->
          <div class="original-figure-container bg-white p-4 rounded-lg border border-gray-200 shadow-sm mb-6">
            <div class="flex flex-col sm:flex-row sm:items-center justify-between mb-3 gap-2">
              <h5 class="font-semibold text-gray-700 text-base md:text-lg">
                <i class="fas fa-project-diagram mr-2 text-blue-500"></i>
                原图 7: JENGA系统概述
              </h5>
              <span class="text-xs bg-blue-100 text-blue-800 px-2 py-1 rounded self-start sm:self-auto">论文原图</span>
            </div>
            
            <div class="image-placeholder bg-gradient-to-br from-gray-100 to-gray-200 p-6 md:p-8 rounded-lg text-center border-2 border-dashed border-gray-300">
              <img src="./images/fig7.png" alt="论文图7: JENGA系统概述" class="max-w-full h-auto rounded-lg">
            </div>
            
            <div class="mt-3 text-sm text-gray-600 bg-gray-50 p-3 rounded border">
              <strong>原图图注:</strong> Overview of JENGA
            </div>
          </div>
          
          <h3 class="text-xl font-semibold text-gray-800 mb-4">层属性接口</h3>
          <p class="mb-4">JENGA的核心思想是引入统一的层属性接口，抽象每个注意力机制的行为。此接口指定三个属性：</p>
          <ul class="list-disc list-inside mb-4 space-y-2">
            <li><strong>页面大小</strong>：每令牌KV内存占用量</li>
            <li><strong>活动页面</strong>：计算未来令牌所需的页面集合</li>
            <li><strong>可能的前缀</strong>：有效的缓存命中长度</li>
          </ul>
          
          <h3 class="text-xl font-semibold text-gray-800 mb-4">LCM分配器</h3>
          <p class="mb-4">基于层属性的"页面大小"，JENGA引入LCM分配器来分配固定大小的大页面，并为每个特定层类型定制分配器，从大页面中分配专门大小的页面。</p>
          
          <!-- 图3占位符 -->
          <div class="original-figure-container bg-white p-4 rounded-lg border border-gray-200 shadow-sm mb-6">
            <div class="flex flex-col sm:flex-row sm:items-center justify-between mb-3 gap-2">
              <h5 class="font-semibold text-gray-700 text-base md:text-lg">
                <i class="fas fa-calculator mr-2 text-blue-500"></i>
                原图 3: LCM分配器在JENGA中的应用
              </h5>
              <span class="text-xs bg-blue-100 text-blue-800 px-2 py-1 rounded self-start sm:self-auto">论文原图</span>
            </div>
            
            <div class="image-placeholder bg-gradient-to-br from-gray-100 to-gray-200 p-6 md:p-8 rounded-lg text-center border-2 border-dashed border-gray-300">
              <img src="./images/fig3.png" alt="论文图3: LCM分配器在JENGA中的应用" class="max-w-full h-auto rounded-lg">
            </div>
            
            <div class="mt-3 text-sm text-gray-600 bg-gray-50 p-3 rounded border">
              <strong>原图图注:</strong> LCM allocator in Jenga
            </div>
          </div>
          
          <h3 class="text-xl font-semibold text-gray-800 mb-4">可定制前缀缓存</h3>
          <p class="mb-4">JENGA为异构注意力层定制前缀缓存策略，显著减少前缀缓存所需的内存。前缀缓存涉及三个操作：</p>
          <ul class="list-disc list-inside mb-4 space-y-2">
            <li><strong>添加页面</strong>：将释放的页面添加到"缓存页面池"</li>
            <li><strong>缓存驱逐</strong>：从"缓存页面池"中驱逐现有页面</li>
            <li><strong>缓存命中</strong>：匹配请求中最长的缓存前缀</li>
          </ul>
          
          <h3 class="text-xl font-semibold text-gray-800 mb-4">不同层的定制</h3>
          <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-6">
            <div class="bg-blue-50 p-4 rounded-lg border border-blue-200">
              <h4 class="font-bold text-blue-700 mb-2">滑动窗口层</h4>
              <p class="text-sm">仅更新滑动窗口内令牌的最后访问时间</p>
            </div>
            <div class="bg-green-50 p-4 rounded-lg border border-green-200">
              <h4 class="font-bold text-green-700 mb-2">Mamba层</h4>
              <p class="text-sm">仅缓存每512个令牌的状态</p>
            </div>
            <div class="bg-purple-50 p-4 rounded-lg border border-purple-200">
              <h4 class="font-bold text-purple-700 mb-2">局部注意力</h4>
              <p class="text-sm">仅在同一块内的页面被视为活动页面</p>
            </div>
            <div class="bg-yellow-50 p-4 rounded-lg border border-yellow-200">
              <h4 class="font-bold text-yellow-700 mb-2">视觉嵌入缓存</h4>
              <p class="text-sm">确保同一图像的所有页面共享相同的最后访问时间</p>
            </div>
          </div>
        </div>
      </section>
      
      <!-- 测试与评估 -->
      <section id="evaluation" class="content-section mobile-optimized mb-12 md:mb-16">
        <h2 class="text-2xl md:text-3xl font-bold text-gray-800 mb-6 md:mb-8 flex items-center">
          <i class="fas fa-chart-line mr-3 text-blue-500"></i>
          测试与评估
        </h2>
        
        <div class="prose max-w-none text-gray-700">
          <h3 class="text-xl font-semibold text-gray-800 mb-4">实验设置</h3>
          <p class="mb-4">作者在两个GPU平台上评估Jenga：(1) NVIDIA H100 80GB GPU和(2) NVIDIA L4 24GB GPU。使用vLLM v0.8.3作为基线，仅更改内存管理系统。评估了广泛的异构LLM，包括Llama 3.2 vision、Gemma-3、Ministral、Llama 4、Jamba等模型。</p>
          
          <h3 class="text-xl font-semibold text-gray-800 mb-4">端到端吞吐量</h3>
          <p class="mb-4">图14比较了vLLM和Jenga在H100和L4 GPU上的端到端推理吞吐量：</p>
          
          <!-- 图14占位符 -->
          <div class="original-figure-container bg-white p-4 rounded-lg border border-gray-200 shadow-sm mb-6">
            <div class="flex flex-col sm:flex-row sm:items-center justify-between mb-3 gap-2">
              <h5 class="font-semibold text-gray-700 text-base md:text-lg">
                <i class="fas fa-tachometer-alt mr-2 text-blue-500"></i>
                原图 14: 端到端吞吐量对比
              </h5>
              <span class="text-xs bg-blue-100 text-blue-800 px-2 py-1 rounded self-start sm:self-auto">论文原图</span>
            </div>
            
            <div class="image-placeholder bg-gradient-to-br from-gray-100 to-gray-200 p-6 md:p-8 rounded-lg text-center border-2 border-dashed border-gray-300">
              <img src="./images/fig14.png" alt="论文图14: 端到端吞吐量对比" class="max-w-full h-auto rounded-lg">
            </div>
            
            <div class="mt-3 text-sm text-gray-600 bg-gray-50 p-3 rounded border">
              <strong>原图图注:</strong> End-to-end throughput
            </div>
          </div>
          
          <p class="mb-4">Jenga在H100上实现了高达1.73倍加速(平均1.46倍)，在L4上实现了高达2.16倍加速(平均1.65倍)。</p>
          
          <h3 class="text-xl font-semibold text-gray-800 mb-4">端到端延迟</h3>
          <p class="mb-4">图15显示了Llama 3.2 Vision模型在不同请求率下的延迟：</p>
          
          <!-- 图15占位符 -->
          <div class="original-figure-container bg-white p-4 rounded-lg border border-gray-200 shadow-sm mb-6">
            <div class="flex flex-col sm:flex-row sm:items-center justify-between mb-3 gap-2">
              <h5 class="font-semibold text-gray-700 text-base md:text-lg">
                <i class="fas fa-clock mr-2 text-blue-500"></i>
                原图 15: Llama Vision模型的平均延迟
              </h5>
              <span class="text-xs bg-blue-100 text-blue-800 px-2 py-1 rounded self-start sm:self-auto">论文原图</span>
            </div>
            
            <div class="image-placeholder bg-gradient-to-br from-gray-100 to-gray-200 p-6 md:p-8 rounded-lg text-center border-2 border-dashed border-gray-300">
              <img src="./images/fig15.png" alt="论文图15: Llama Vision模型的平均延迟" class="max-w-full h-auto rounded-lg">
            </div>
            
            <div class="mt-3 text-sm text-gray-600 bg-gray-50 p-3 rounded border">
              <strong>原图图注:</strong> Averaged Latency for the Llama Vision Model with changing request rates
            </div>
          </div>
          
          <p class="mb-4">当请求率较低时，vLLM和Jenga的延迟相似。当请求率增长时，Jenga将端到端延迟降低了高达1.90倍，首令牌时间降低了高达23.40倍。</p>
          
          <h3 class="text-xl font-semibold text-gray-800 mb-4">最大上下文长度</h3>
          <p class="mb-4">表3显示了Llama 4的最大支持上下文长度：</p>
          
          <div class="bg-white p-4 rounded-lg border border-gray-200 shadow-sm mb-6">
            <h5 class="font-semibold text-gray-700 mb-3 flex items-center">
              <i class="fas fa-table mr-2 text-blue-500"></i>
              表3: Llama 4 109B模型的最大支持长度
            </h5>
            <div class="overflow-x-auto">
              <table class="min-w-full bg-white border border-gray-300">
                <thead>
                  <tr class="bg-gray-100">
                    <th class="py-2 px-4 border-b">配置</th>
                    <th class="py-2 px-4 border-b">vLLM</th>
                    <th class="py-2 px-4 border-b">Jenga</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="py-2 px-4 border-b">8xH100</td>
                    <td class="py-2 px-4 border-b">1.3M</td>
                    <td class="py-2 px-4 border-b">5.2M</td>
                  </tr>
                  <tr>
                    <td class="py-2 px-4 border-b">8xH200</td>
                    <td class="py-2 px-4 border-b">3.7M</td>
                    <td class="py-2 px-4 border-b">14.7M</td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>
          
          <p class="mb-4">vLLM无法在单个GPU节点内容纳完整上下文。相比之下，Jenga通过释放非活动页面将每请求内存使用减少4倍，支持4倍更长的上下文，并在一个8xH200 GPU节点内支持完整上下文长度。</p>
          
          <h3 class="text-xl font-semibold text-gray-800 mb-4">内存碎片分析</h3>
          <p class="mb-4">图17显示了Ministral模型推理期间每个部分使用的内存：</p>
          
          <!-- 图17占位符 -->
          <div class="original-figure-container bg-white p-4 rounded-lg border border-gray-200 shadow-sm mb-6">
            <div class="flex flex-col sm:flex-row sm:items-center justify-between mb-3 gap-2">
              <h5 class="font-semibold text-gray-700 text-base md:text-lg">
                <i class="fas fa-chart-pie mr-2 text-blue-500"></i>
                原图 17: Ministral模型的内存使用时间线
              </h5>
              <span class="text-xs bg-blue-100 text-blue-800 px-2 py-1 rounded self-start sm:self-auto">论文原图</span>
            </div>
            
            <div class="image-placeholder bg-gradient-to-br from-gray-100 to-gray-200 p-6 md:p-8 rounded-lg text-center border-2 border-dashed border-gray-300">
              <img src="./images/fig17.png" alt="论文图17: Ministral模型的内存使用时间线" class="max-w-full h-auto rounded-lg">
            </div>
            
            <div class="mt-3 text-sm text-gray-600 bg-gray-50 p-3 rounded border">
              <strong>原图图注:</strong> Timeline of memory usage for Ministral model. vLLM shows significant wasted memory due to memory fragmentation (in red), while Jenga minimizes waste.
            </div>
          </div>
          
          <p class="mb-4">对于两个跟踪，vLLM平均浪费38.2%的KV缓存内存，而Jenga仅有0.04%的KV缓存内存浪费。</p>
          
          <h3 class="text-xl font-semibold text-gray-800 mb-4">前缀缓存评估</h3>
          <p class="mb-4">图18通过改变arXiv数据集中的文章数量评估Jenga的前缀缓存系统：</p>
          
          <!-- 图18占位符 -->
          <div class="original-figure-container bg-white p-4 rounded-lg border border-gray-200 shadow-sm mb-6">
            <div class="flex flex-col sm:flex-row sm:items-center justify-between mb-3 gap-2">
              <h5 class="font-semibold text-gray-700 text-base md:text-lg">
                <i class="fas fa-database mr-2 text-blue-500"></i>
                原图 18: 不同文章数量的前缀缓存
              </h5>
              <span class="text-xs bg-blue-100 text-blue-800 px-2 py-1 rounded self-start sm:self-auto">论文原图</span>
            </div>
            
            <div class="image-placeholder bg-gradient-to-br from-gray-100 to-gray-200 p-6 md:p-8 rounded-lg text-center border-2 border-dashed border-gray-300">
              <img src="./images/fig18.png" alt="论文图18: 不同文章数量的前缀缓存" class="max-w-full h-auto rounded-lg">
            </div>
            
            <div class="mt-3 text-sm text-gray-600 bg-gray-50 p-3 rounded border">
              <strong>原图图注:</strong> Prefix caching with different number of articles
            </div>
          </div>
          
          <p class="mb-4">当文章数量较少时，两个系统都可以缓存所有文章并提供相似的吞吐量。当文章数量较多时，Jenga具有高达1.60倍更高的缓存命中率，并提供高达1.77倍的吞吐量改进。</p>
        </div>
      </section>
      
      <!-- 结论 -->
      <section id="conclusion" class="content-section mobile-optimized mb-12 md:mb-16">
        <h2 class="text-2xl md:text-3xl font-bold text-gray-800 mb-6 md:mb-8 flex items-center">
          <i class="fas fa-flag-checkered mr-3 text-blue-500"></i>
          结论
        </h2>
        
        <div class="prose max-w-none text-gray-700">
          <p class="mb-4">作者介绍了Jenga，一个用于管理现代LLM架构中异构嵌入的高效内存分配框架。通过利用层属性，Jenga减少了内存碎片，并为不同类型的嵌入启用了可定制的缓存策略。</p>
          
          <div class="bg-gradient-to-r from-blue-50 to-purple-50 border-l-4 border-blue-500 p-6 rounded-r-lg mb-6">
            <h4 class="font-bold text-blue-700 mb-3 flex items-center">
              <i class="fas fa-check-circle mr-2"></i>主要成果
            </h4>
            <ul class="list-disc list-inside space-y-2 text-gray-700">
              <li>识别了新型LLM架构在嵌入大小和注意力机制方面的异构性增长趋势</li>
              <li>提出了基于LCM的两级内存分配系统，有效支持不同大小的嵌入</li>
              <li>实现了可定制的缓存策略，最大化不同注意力机制的命中率</li>
              <li>开源实现Jenga，相比现有推理引擎提升1.46倍吞吐量</li>
              <li>在各种模型和场景中，Jenga实现了高达83.3%的GPU内存利用率提升和1.39到2.16倍的服务吞吐量增加</li>
            </ul>
          </div>
          
          <p class="mb-4">Jenga为异构LLM服务提供了高效的内存管理解决方案，解决了现代LLM架构中日益增长的异构性带来的挑战。通过层属性感知的设计，Jenga能够适应各种注意力机制和嵌入大小，显著提高了内存利用率和系统吞吐量。</p>
        </div>
      </section>
    </div>
  </div>
  
  <!-- 交互脚本 -->
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      const navItems = document.querySelectorAll('.nav-item');
      const sections = document.querySelectorAll('section');
      
      function highlightNav() {
        let current = '';
        sections.forEach(section => {
          const sectionTop = section.offsetTop;
          const sectionHeight = section.clientHeight;
          if (window.scrollY >= (sectionTop - 100)) {
            current = section.getAttribute('id');
          }
        });

        navItems.forEach(item => {
          item.classList.remove('active');
          if (item.getAttribute('href') === `#${current}`) {
            item.classList.add('active');
          }
        });
      }

      window.addEventListener('scroll', highlightNav);
      
      // 平滑滚动
      navItems.forEach(item => {
        item.addEventListener('click', function(e) {
          e.preventDefault();
          const targetId = this.getAttribute('href');
          const targetSection = document.querySelector(targetId);
          window.scrollTo({
            top: targetSection.offsetTop - 80,
            behavior: 'smooth'
          });
        });
      });
    });
  </script>
</body>
</html>