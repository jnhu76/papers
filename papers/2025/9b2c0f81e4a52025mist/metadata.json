{
  "id": "9b2c0f81e4a52025mist",
  "title": "Mist: Efficient Distributed Training of Large Language Models via Memory-Parallelism Co-Optimization",
  "authors": ["Zhanda Zhu", "Christina Giannoula", "Muralidhar Andoorveedu", "Qidong Su", "Karttikeya Mangalam", "Bojian Zheng", "Gennady Pekhimenko"],
  "year": 2025,
  "conference": "EuroSys",
  "category": "机器学习系统",
  "keywords": ["大语言模型", "分布式训练", "内存优化", "并行优化", "自动化系统", "Pipeline Parallelism", "ZeRO"],
  "abstract": "Various parallelism, such as data, tensor, and pipeline parallelism, along with memory optimizations like activation checkpointing, redundancy elimination, and offloading, have been proposed to accelerate distributed training for Large Language Models. To find the best combination of these techniques, automatic distributed training systems are proposed. However, existing systems only tune a subset of optimizations, due to the lack of overlap awareness, inability to navigate the vast search space, and ignoring the inter-microbatch imbalance, leading to sub-optimal performance. To address these shortcomings, we propose Mist, a memory, overlap, and imbalance-aware automatic distributed training system that comprehensively co-optimizes all memory footprint reduction techniques alongside parallelism. Mist is based on three key ideas: (1) fine-grained overlap-centric scheduling, orchestrating optimizations in an overlapped manner, (2) symbolic-based performance analysis that predicts runtime and memory usage using symbolic expressions for fast tuning, and (3) imbalance-aware hierarchical tuning, decoupling the process into an inter-stage imbalance and overlap aware Mixed Integer Linear Programming problem and an intra-stage Dual-Objective Constrained Optimization problem, and connecting them through Pareto frontier sampling. Our evaluation results show that Mist achieves an average of 1.28× (up to 1.73×) and 1.27× (up to 2.04×) speedup compared to state-of-the-art manual system Megatron-LM and state-of-the-art automatic system Aceso, respectively."
}