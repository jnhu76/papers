<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
  <style>
    body { 
      font-family: 'Inter', sans-serif; 
      scroll-behavior: smooth;
    }
    .mobile-optimized { 
      margin-bottom: 2rem !important; 
    }
    @media (max-width: 768px) {
      .content-section { 
        padding: 1rem; 
        margin-bottom: 1.5rem;
      }
      .technical-details {
        margin: 1rem 0;
      }
    }
    .hide-scrollbar {
      -ms-overflow-style: none;
      scrollbar-width: none;
    }
    .hide-scrollbar::-webkit-scrollbar {
      display: none;
    }
    .nav-item {
      @apply px-4 py-2 rounded-lg transition-colors duration-200 text-gray-600 hover:text-blue-600 hover:bg-blue-50;
    }
    .nav-item.active {
      @apply text-blue-600 bg-blue-100;
    }
  </style>
</head>
<body class="bg-gradient-to-br from-blue-400 via-purple-500 to-pink-400 min-h-screen">
  <!-- 导航系统 -->
  <nav class="nav-scroll bg-white/90 backdrop-blur-sm sticky top-0 z-50 border-b border-gray-200">
    <div class="container mx-auto px-4 py-3">
      <div class="flex overflow-x-auto space-x-6 hide-scrollbar">
        <a href="#abstract" class="nav-item whitespace-nowrap">摘要</a>
        <a href="#background-motivation" class="nav-item whitespace-nowrap">背景与动机</a>
        <a href="#challenges" class="nav-item whitespace-nowrap">问题与挑战</a>
        <a href="#design-implementation" class="nav-item whitespace-nowrap">设计与实现</a>
        <a href="#evaluation" class="nav-item whitespace-nowrap">测试与评估</a>
        <a href="#conclusion" class="nav-item whitespace-nowrap">结论与不足</a>
      </div>
    </div>
  </nav>

  <div class="container mx-auto px-4 py-8">
    <div class="bg-white/95 backdrop-blur-sm rounded-xl shadow-lg p-6">
      <!-- 论文标题和元数据 -->
      <div class="mb-12 md:mb-16">
        <h1 class="text-3xl md:text-4xl font-bold text-gray-800 mb-6 text-center md:text-left">
          FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving
        </h1>
        
        <div class="bg-gradient-to-r from-blue-50 to-purple-50 border-l-4 border-blue-500 p-6 rounded-r-lg">
          <div class="grid grid-cols-1 md:grid-cols-2 gap-4 text-gray-700">
            <div class="space-y-3">
              <div>
                <strong class="text-blue-700 block mb-1">作者信息</strong>
                <div class="text-lg">Zihao Ye, Lequn Chen, Ruihang Lai, Wuwei Lin, Yineng Zhang, Stephanie Wang, Tianqi Chen, Baris Kasikci, Vinod Grover, Arvind Krishnamurthy, Luis Ceze</div>
                <div class="text-sm text-gray-600 mt-1">University of Washington, NVIDIA, Perplexity AI, Carnegie Mellon University</div>
              </div>
              <div>
                <strong class="text-blue-700 block mb-1">发表信息</strong>
                <div>arXiv preprint arXiv:2501.01005v2</div>
              </div>
            </div>
            <div class="space-y-3">
              <div>
                <strong class="text-blue-700 block mb-1">DOI标识</strong>
                <div class="font-mono text-sm bg-white px-3 py-2 rounded border">arXiv:2501.01005v2</div>
              </div>
              <div>
                <strong class="text-blue-800 block mb-1">关键词</strong>
                <div class="flex flex-wrap gap-2 mt-1">
                  <span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm">LLM推理</span>
                  <span class="bg-green-100 text-green-800 px-3 py-1 rounded-full text-sm">注意力机制</span>
                  <span class="bg-purple-100 text-purple-800 px-3 py-1 rounded-full text-sm">KV缓存优化</span>
                  <span class="bg-amber-100 text-amber-800 px-3 py-1 rounded-full text-sm">JIT编译</span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>

      <!-- AI生成内容标识 -->
      <div class="mt-6 mb-8 p-4 bg-gradient-to-r from-amber-50 to-orange-50 dark:from-gray-800 dark:to-gray-900 border-l-4 border-amber-500 dark:border-amber-400 rounded-r-lg shadow-sm">
        <div class="flex items-start gap-3">
          <div class="flex-shrink-0 mt-0.5">
            <svg class="w-6 h-6 text-amber-600 dark:text-amber-400" fill="currentColor" viewBox="0 0 20 20">
              <path fill-rule="evenodd" d="M8.257 3.099c.765-1.36 2.722-1.36 3.486 0l5.58 9.92c.75 1.334-.213 2.98-1.742 2.98H4.42c-1.53 0-2.493-1.646-1.743-2.98l5.58-9.92zM11 13a1 1 0 11-2 0 1 1 0 012 0zm-1-8a1 1 0 00-1 1v3a1 1 0 002 0V6a1 1 0 00-1-1z" clip-rule="evenodd" />
            </svg>
          </div>
          <div class="flex-1">
            <div class="flex flex-wrap items-center gap-2 mb-2">
              <span class="px-3 py-1 bg-amber-100 dark:bg-amber-900/40 text-amber-800 dark:text-amber-300 text-sm font-bold rounded-full border border-amber-200 dark:border-amber-700">
                ⚠️ AI生成内容
              </span>
              <span class="text-xs text-amber-700 dark:text-amber-300 font-medium px-2 py-1 bg-amber-50 dark:bg-amber-900/30 rounded">
                法律要求标识
              </span>
            </div>
            <p class="text-sm text-gray-700 dark:text-gray-300 leading-relaxed">
              根据《人工智能生成合成内容标识办法》要求，本文的<strong class="text-amber-700 dark:text-amber-400">解析、评述及总结内容由人工智能模型生成</strong>。生成内容可能存在不准确、过时或偏差，仅作为学习参考之用。
            </p>
            <div class="mt-3 pt-3 border-t border-amber-200 dark:border-gray-700">
              <p class="text-xs text-gray-600 dark:text-gray-400 flex items-start">
                <svg class="w-4 h-4 mr-2 mt-0.5 text-blue-500 flex-shrink-0" fill="currentColor" viewBox="0 0 20 20">
                  <path fill-rule="evenodd" d="M18 10a8 8 0 11-16 0 8 8 0 0116 0zm-7-4a1 1 0 11-2 0 1 1 0 012 0zM9 9a1 1 0 000 2v3a1 1 0 001 1h1a1 1 0 100-2v-3a1 1 0 00-1-1H9z" clip-rule="evenodd" />
                </svg>
                建议您：1) 核对原始论文；2) 结合专业知识判断；3) 不依赖AI生成内容做出关键学术决策。
              </p>
            </div>
          </div>
        </div>
      </div>

      <!-- 核心贡献 -->
      <div class="bg-gradient-to-r from-green-50 to-blue-50 border-l-4 border-green-500 p-4 rounded-r-lg mb-12">
        <h4 class="font-bold text-green-700 mb-2 flex items-center text-lg">
          <i class="fas fa-trophy mr-2"></i>核心贡献
        </h4>
        <ul class="list-disc list-inside space-y-2 text-gray-700">
          <li><strong>统一的块稀疏格式</strong>：使用块稀疏行(BSR)格式统一管理异构KV缓存存储，优化内存访问并减少冗余。</li>
          <li><strong>可定制的注意力模板</strong>：通过JIT编译技术，允许用户快速实现和优化各种注意力变体。</li>
          <li><strong>动态负载均衡调度</strong>：设计运行时调度器，自动适应输入序列长度的动态变化，提升GPU利用率。</li>
          <li><strong>生产级集成</strong>：已集成到vLLM、SGLang、MLC-Engine等主流LLM推理框架中。</li>
        </ul>
      </div>
      
      <!-- 各技术章节 -->
      <section id="abstract" class="content-section mobile-optimized mb-12 md:mb-16">
        <h2 class="text-2xl md:text-3xl font-bold text-gray-800 mb-6 md:mb-8 flex items-center">
          <i class="fas fa-file-alt mr-3 text-blue-500"></i>
          摘要
        </h2>
        <div class="prose max-w-none">
          <p class="text-gray-700 mb-4">
            作者提出了FlashInfer，一个用于大语言模型(LLM)推理服务的高效且可定制的注意力引擎。核心目标是解决LLM推理服务中注意力计算面临的<strong>KV缓存存储异构性</strong>和<strong>工作负载模式多样性</strong>两大挑战。
          </p>
          <p class="text-gray-700 mb-4">
            主要技术贡献包括：
          </p>
          <ul class="list-disc pl-6 space-y-2 text-gray-700 mb-6">
            <li>采用<strong>块稀疏格式</strong>统一表示不同存储模式（如页表、基数树），提升内存访问效率。</li>
            <li>提供<strong>可定制的注意力模板</strong>，通过JIT编译支持各种注意力变体（如RoPE融合、特殊掩码）。</li>
            <li>设计<strong>动态负载均衡调度框架</strong>，自动适应变化的输入序列长度，保持与CUDA Graph的兼容性。</li>
          </ul>
          <div class="bg-gray-50 p-4 rounded-lg border border-gray-200">
            <p class="text-gray-700 font-medium">
              <i class="fas fa-chart-line text-green-500 mr-2"></i>性能提升：实验表明，与现有最佳方案相比，FlashInfer在标准LLM服务基准测试中实现<strong>29-69%的Token间延迟降低</strong>，在长上下文推理中实现<strong>28-30%的延迟降低</strong>，在并行生成场景中实现<strong>13-17%的加速</strong>。
            </p>
          </div>
        </div>
      </section>
      
      <section id="background-motivation" class="content-section mobile-optimized mb-12 md:mb-16">
        <h2 class="text-2xl md:text-3xl font-bold text-gray-800 mb-6 md:mb-8 flex items-center">
          <i class="fas fa-layer-group mr-3 text-blue-500"></i>
          背景与动机
        </h2>
        <div class="prose max-w-none">
          <h3 class="text-xl font-semibold text-gray-700 mb-4">LLM推理中的注意力计算</h3>
          <p class="text-gray-700 mb-4">
            Transformer架构中的注意力机制是LLM推理的核心组件。在推理过程中，模型需要根据当前查询(Query)和历史键值(KV)缓存计算注意力输出。随着模型规模和上下文长度的增长，<strong>注意力计算效率成为推理服务的瓶颈</strong>。
          </p>
          
          <div class="grid grid-cols-1 md:grid-cols-2 gap-6 my-6">
            <div class="tech-card bg-blue-50 p-4 rounded-lg border border-blue-200">
              <h4 class="font-bold text-blue-700 text-lg mb-2">
                <i class="fa-solid fa-brain mr-2"></i>KV缓存
              </h4>
              <p class="text-sm text-gray-700">存储历史上下文信息，随着生成过程不断增长，可能达到数十万Token。</p>
            </div>
            <div class="tech-card bg-green-50 p-4 rounded-lg border border-green-200">
              <h4 class="font-bold text-green-700 text-lg mb-2">
                <i class="fa-solid fa-bolt mr-2"></i>注意力模式多样性
              </h4>
              <p class="text-sm text-gray-700">包括预填充(prefill)、解码(decode)、推测解码(speculative decoding)等多种计算模式。</p>
            </div>
          </div>
          
          <h3 class="text-xl font-semibold text-gray-700 mb-4 mt-8">现有技术基础</h3>
          <ul class="list-disc pl-6 space-y-3 text-gray-700 mb-6">
            <li><strong>FlashAttention</strong>：通过在线softmax技巧减少中间矩阵存储，但主要针对训练场景优化。</li>
            <li><strong>PagedAttention</strong>：使用页表管理KV缓存，减少内存碎片，但需要专用的注意力内核。</li>
            <li><strong>Grouped Query Attention (GQA)</strong>：多个查询头共享相同的KV头，减少KV缓存大小。</li>
          </ul>
          
          <!-- 原图占位符：Figure 1 - System Overview -->
          <details class="technical-details bg-gray-50 rounded-lg p-4 md:p-6 mb-8 mt-6">
            <summary class="cursor-pointer font-semibold text-lg md:text-xl text-gray-800 hover:text-blue-600 transition-colors py-2">
              <i class="fas fa-sitemap mr-2"></i>技术细节：图1 - FlashInfer系统设计概览
            </summary>
            
            <div class="mt-6 space-y-6">
              <!-- 原图展示部分 -->
              <div class="original-figure-container">
                <div class="flex flex-col sm:flex-row sm:items-center justify-between mb-4 gap-2">
                  <h5 class="font-semibold text-gray-700 text-base md:text-lg">
                    <i class="fas fa-image mr-2 text-blue-500"></i>
                    原图 1: FlashInfer系统设计概览
                  </h5>
                  <span class="text-xs bg-blue-100 text-blue-800 px-2 py-1 rounded self-start sm:self-auto">论文原图</span>
                </div>
                
                <!-- 原图占位符 -->
                <div class="image-placeholder bg-gradient-to-br from-gray-100 to-gray-200 p-6 md:p-8 rounded-lg text-center border-2 border-dashed border-gray-300">
                  <img src="./images/fig1.png" alt="论文图1: FlashInfer系统设计概览图，显示编译时JIT编译和运行时动态调度的分工" class="max-w-full h-auto rounded-lg">
                </div>
                
                <!-- 原图图注 -->
                <div class="mt-4 text-sm text-gray-600 bg-gray-50 p-3 rounded border">
                  <strong>原图图注:</strong> Overview of the FlashInfer system design: Attention variant specifications, task information and KV-cache layout specifics are provided at compile time for JIT compilation, while sequence length information is input at runtime for dynamic scheduling.
                </div>
              </div>
              
              <!-- 技术解释部分 -->
              <div class="bg-blue-50 p-4 rounded-lg border border-blue-200">
                <h5 class="font-semibold text-blue-700 mb-3 flex items-center text-base md:text-lg">
                  <i class="fas fa-info-circle mr-2"></i>技术解释：
                </h5>
                <ul class="list-disc list-inside space-y-2 text-sm md:text-base text-gray-700">
                  <li><strong>编译时优化:</strong> 注意力变体规范、任务信息和KV缓存布局在编译时确定，通过JIT编译生成高度优化的内核代码。</li>
                  <li><strong>运行时调度:</strong> 序列长度信息在运行时输入，动态调度器根据实际工作负载调整计算任务分配。</li>
                  <li><strong>分离设计:</strong> 将静态配置（内核代码）与动态信息（序列长度）分离，保持与CUDA Graph的兼容性。</li>
                </ul>
              </div>
            </div>
          </details>
        </div>
      </section>
      
      <section id="challenges" class="content-section mobile-optimized mb-12 md:mb-16">
        <h2 class="text-2xl md:text-3xl font-bold text-gray-800 mb-6 md:mb-8 flex items-center">
          <i class="fas fa-exclamation-triangle mr-3 text-blue-500"></i>
          问题与挑战
        </h2>
        <div class="prose max-w-none">
          <p class="text-gray-700 mb-6">
            LLM推理服务中的注意力计算面临两个核心挑战，这使得通用的高性能注意力内核开发变得困难：
          </p>
          
          <div class="grid grid-cols-1 md:grid-cols-2 gap-6 mb-8">
            <div class="bg-red-50 border border-red-200 rounded-xl p-5">
              <div class="flex items-center mb-3">
                <i class="fas fa-random text-red-500 text-2xl mr-3"></i>
                <h3 class="text-xl font-bold text-red-700">工作负载多样性</h3>
              </div>
              <ul class="list-disc pl-5 space-y-2 text-gray-700">
                <li><strong>计算模式多样</strong>：预填充、解码、推测解码、树注意力等不同模式。</li>
                <li><strong>输入动态性</strong>：查询长度和KV缓存长度在批次内和随时间变化。</li>
                <li><strong>负载不均衡</strong>：朴素实现可能导致GPU SM（流多处理器）负载不均。</li>
                <li><strong>前缀复用机会</strong>：多请求处理中出现共享前缀，需要特殊优化。</li>
              </ul>
            </div>
            
            <div class="bg-orange-50 border border-orange-200 rounded-xl p-5">
              <div class="flex items-center mb-3">
                <i class="fas fa-microchip text-orange-500 text-2xl mr-3"></i>
                <h3 class="text-xl font-bold text-orange-700">硬件异构性</h3>
              </div>
              <ul class="list-disc pl-5 space-y-2 text-gray-700">
                <li><strong>存储格式多样</strong>：页表、基数树等不同KV缓存存储模式。</li>
                <li><strong>硬件架构差异</strong>：不同GPU架构（Turing/Ampere/Ada/Hopper）需要不同的优化策略。</li>
                <li><strong>注意力变体激增</strong>：分组注意力、滑动窗口注意力、RoPE、特殊掩码等。</li>
                <li><strong>定制化需求</strong>：需要为每种注意力变体开发专用的高性能内核。</li>
              </ul>
            </div>
          </div>
          
          <div class="bg-gray-50 p-5 rounded-lg border border-gray-300">
            <h3 class="text-lg font-bold text-gray-700 mb-3">当前解决方案的局限性</h3>
            <p class="text-gray-700 mb-3">
              现有系统（如vLLM、TensorRT-LLM）通常为特定场景开发专用的注意力内核，导致：
            </p>
            <ul class="list-disc pl-6 space-y-2 text-gray-700">
              <li><strong>高维护成本</strong>：每个系统维护自己的内核实现，重复工作。</li>
              <li><strong>灵活性不足</strong>：难以快速支持新的注意力变体或硬件架构。</li>
              <li><strong>性能次优</strong>：通用内核无法充分利用特定场景的优化机会。</li>
              <li><strong>兼容性问题</strong>：不同存储格式需要不同的内核实现。</li>
            </ul>
          </div>
        </div>
      </section>
      
      <section id="design-implementation" class="content-section mobile-optimized mb-12 md:mb-16">
        <h2 class="text-2xl md:text-3xl font-bold text-gray-800 mb-6 md:mb-8 flex items-center">
          <i class="fas fa-cogs mr-3 text-blue-500"></i>
          设计与实现
        </h2>
        
        <div class="mb-10">
          <h3 class="text-xl font-bold text-gray-700 mb-4">3.1 KV缓存存储优化</h3>
          <p class="text-gray-700 mb-4">
            作者提出了统一的块稀疏行(BSR)格式来表示各种KV缓存存储结构，包括页表和基数树。
          </p>
          
          <!-- 原图占位符：Figure 2 - Page Table in BSR format -->
          <details class="technical-details bg-gray-50 rounded-lg p-4 md:p-6 mb-8">
            <summary class="cursor-pointer font-semibold text-lg md:text-xl text-gray-800 hover:text-blue-600 transition-colors py-2">
              <i class="fas fa-table mr-2"></i>技术细节：图2 - 页表的BSR格式表示
            </summary>
            
            <div class="mt-6 space-y-6">
              <div class="original-figure-container">
                <div class="flex flex-col sm:flex-row sm:items-center justify-between mb-4 gap-2">
                  <h5 class="font-semibold text-gray-700 text-base md:text-lg">
                    <i class="fas fa-image mr-2 text-blue-500"></i>
                    原图 2: 页表在BSR格式中的表示 (B_r=4, B_c=1)
                  </h5>
                  <span class="text-xs bg-blue-100 text-blue-800 px-2 py-1 rounded self-start sm:self-auto">论文原图</span>
                </div>
                
                <div class="image-placeholder bg-gradient-to-br from-gray-100 to-gray-200 p-6 md:p-8 rounded-lg text-center border-2 border-dashed border-gray-300">
                  <img src="./images/fig2.png" alt="论文图2: 展示PageAttention的页表如何表示为块稀疏矩阵，非零块表示被查询访问的KV缓存页" class="max-w-full h-auto rounded-lg">
                </div>
                
                <div class="mt-4 text-sm text-gray-600 bg-gray-50 p-3 rounded border">
                  <strong>原图图注:</strong> Representation of Page Table in BSR (B_r=4,B_c=1) format. The number of column blocks in the block sparse matrix corresponds to the total number of blocks allocated for the Page Table. Non-zero blocks represent KV-Cache pages accessed by queries.
                </div>
              </div>
              
              <div class="bg-blue-50 p-4 rounded-lg border border-blue-200">
                <h5 class="font-semibold text-blue-700 mb-3 flex items-center text-base md:text-lg">
                  <i class="fas fa-info-circle mr-2"></i>技术解释：
                </h5>
                <ul class="list-disc list-inside space-y-2 text-sm md:text-base text-gray-700">
                  <li><strong>统一表示:</strong> 将页表中的块映射到BSR矩阵的非零块，实现不同存储格式的统一处理。</li>
                  <li><strong>灵活性:</strong> 支持任意块大小(B_r, B_c)，适应不同粒度的稀疏模式。</li>
                  <li><strong>向量级稀疏:</strong> 当B_c=1时，支持向量级稀疏，适用于细粒度KV缓存修剪。</li>
                  <li><strong>内存效率:</strong> 只存储非零块，减少内存占用和带宽消耗。</li>
                </ul>
              </div>
            </div>
          </details>
          
          <div class="bg-green-50 p-4 rounded-lg border border-green-200 mb-6">
            <h4 class="font-bold text-green-700 mb-2">可组合格式 (Composable Formats)</h4>
            <p class="text-gray-700 mb-3">
              针对共享前缀场景，作者提出了可组合格式：将KV缓存稀疏矩阵分解为多个子矩阵，每个使用不同的块大小。
            </p>
            <ul class="list-disc pl-5 space-y-1 text-gray-700">
              <li><strong>共享前缀</strong>：使用较大的B_r存储，多个查询可共享高速共享内存中的KV缓存。</li>
              <li><strong>独特后缀</strong>：使用较小的B_r(如1,1)存储，每个查询独立访问自己的KV缓存。</li>
              <li><strong>无需数据移动</strong>：仅计算稀疏子矩阵的索引和指针数组。</li>
            </ul>
          </div>
        </div>
        
        <div class="mb-10">
          <h3 class="text-xl font-bold text-gray-700 mb-4">3.2 计算抽象与JIT编译</h3>
          
          <!-- 原图占位符：Figure 4 - Data transfer from global to shared memory -->
          <details class="technical-details bg-gray-50 rounded-lg p-4 md:p-6 mb-8">
            <summary class="cursor-pointer font-semibold text-lg md:text-xl text-gray-800 hover:text-blue-600 transition-colors py-2">
              <i class="fas fa-exchange-alt mr-2"></i>技术细节：图4 - 全局内存到共享内存的数据传输
            </summary>
            
            <div class="mt-6 space-y-6">
              <div class="original-figure-container">
                <div class="flex flex-col sm:flex-row sm:items-center justify-between mb-4 gap-2">
                  <h5 class="font-semibold text-gray-700 text-base md:text-lg">
                    <i class="fas fa-image mr-2 text-blue-500"></i>
                    原图 4: FlashInfer中稀疏/稠密KV缓存的数据传输
                  </h5>
                  <span class="text-xs bg-blue-100 text-blue-800 px-2 py-1 rounded self-start sm:self-auto">论文原图</span>
                </div>
                
                <div class="image-placeholder bg-gradient-to-br from-gray-100 to-gray-200 p-6 md:p-8 rounded-lg text-center border-2 border-dashed border-gray-300">
                  <img src="./images/fig4.png" alt="论文图4: 展示如何从稀疏和稠密KV缓存加载数据到共享内存，左侧为稀疏KV缓存(b_c=2)，右侧为稠密KV缓存" class="max-w-full h-auto rounded-lg">
                </div>
                
                <div class="mt-4 text-sm text-gray-600 bg-gray-50 p-3 rounded border">
                  <strong>原图图注:</strong> Data transfer from global to shared memory for sparse/-dense KV-Cache in FlashInfer. Left: Sparse KV-Cache with b_c=2; Right: Dense KV-Cache. Head dimension d.
                </div>
              </div>
              
              <div class="bg-blue-50 p-4 rounded-lg border border-blue-200">
                <h5 class="font-semibold text-blue-700 mb-3 flex items-center text-base md:text-lg">
                  <i class="fas fa-info-circle mr-2"></i>技术解释：
                </h5>
                <ul class="list-disc list-inside space-y-2 text-sm md:text-base text-gray-700">
                  <li><strong>稀疏数据加载:</strong> 使用BSR矩阵的索引数组计算稀疏KV缓存的地址，将分散的全局内存数据收集到连续的共享内存中。</li>
                  <li><strong>稠密数据加载:</strong> 使用行索引的仿射变换计算稠密KV缓存的地址。</li>
                  <li><strong>内存访问优化:</strong> KV缓存的最后一个维度（头维度，通常128或256）保持连续，符合GPU缓存行大小。</li>
                  <li><strong>异步拷贝:</strong> 使用128B宽度的LDGSTS异步拷贝指令最大化内存带宽。</li>
                  <li><strong>TMA支持:</strong> 在Hopper架构上，为连续KV缓存使用Tensor Memory Accelerator(TMA)指令加速数据移动。</li>
                </ul>
              </div>
            </div>
          </details>
          
          <div class="mb-6">
            <h4 class="font-bold text-gray-700 mb-3">可定制的注意力模板</h4>
            <p class="text-gray-700 mb-4">
              FlashInfer设计了可定制的CUDA模板和JIT编译器，支持各种注意力变体：
            </p>
            
            <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-6">
              <div class="bg-purple-50 p-4 rounded-lg border border-purple-200">
                <h5 class="font-bold text-purple-700 mb-2">变换函数 (Functors)</h5>
                <ul class="list-disc pl-5 space-y-1 text-sm text-gray-700">
                  <li>QueryTransform/KeyTransform/ValueTransform</li>
                  <li>OutputTransform</li>
                  <li>LogitsTransform/LogitsMask</li>
                </ul>
              </div>
              
              <div class="bg-amber-50 p-4 rounded-lg border border-amber-200">
                <h5 class="font-bold text-amber-700 mb-2">支持的特性</h5>
                <ul class="list-disc pl-5 space-y-1 text-sm text-gray-700">
                  <li>RoPE融合</li>
                  <li>自定义掩码</li>
                  <li>Logits软上限</li>
                  <li>滑动窗口注意力</li>
                  <li>FlashSigmoid</li>
                </ul>
              </div>
            </div>
            
            <div class="bg-gray-50 p-4 rounded-lg border border-gray-300">
              <p class="text-gray-700 text-sm">
                <strong>JIT编译流程:</strong> 用户提供CUDA代码定义变体函数 → JIT编译器将变体类插入模板 → 生成CUDA代码 → 通过PyTorch JIT编译器编译 → 注册为自定义算子。
              </p>
            </div>
          </div>
        </div>
        
        <div class="mb-10">
          <h3 class="text-xl font-bold text-gray-700 mb-4">3.3 动态感知运行时</h3>
          
          <!-- 原图占位符：Figure 6 - Load-balanced runtime scheduler -->
          <details class="technical-details bg-gray-50 rounded-lg p-4 md:p-6 mb-8">
            <summary class="cursor-pointer font-semibold text-lg md:text-xl text-gray-800 hover:text-blue-600 transition-colors py-2">
              <i class="fas fa-tasks mr-2"></i>技术细节：图6 - 负载均衡运行时调度器
            </summary>
            
            <div class="mt-6 space-y-6">
              <div class="original-figure-container">
                <div class="flex flex-col sm:flex-row sm:items-center justify-between mb-4 gap-2">
                  <h5 class="font-semibold text-gray-700 text-base md:text-lg">
                    <i class="fas fa-image mr-2 text-blue-500"></i>
                    原图 6: FlashInfer的负载均衡运行时调度器
                  </h5>
                  <span class="text-xs bg-blue-100 text-blue-800 px-2 py-1 rounded self-start sm:self-auto">论文原图</span>
                </div>
                
                <div class="image-placeholder bg-gradient-to-br from-gray-100 to-gray-200 p-6 md:p-8 rounded-lg text-center border-2 border-dashed border-gray-300">
                  <img src="./images/fig6.png" alt="论文图6: FlashInfer运行时调度器的工作流程，展示序列长度信息如何用于计算计划信息" class="max-w-full h-auto rounded-lg">
                </div>
                
                <div class="mt-4 text-sm text-gray-600 bg-gray-50 p-3 rounded border">
                  <strong>原图图注:</strong> FlashInfer's load-balanced runtime scheduler, sequence length information (both on query/output and key/value dimension) are provided to the scheduler to compute the plan information: (1) Work queue of each CTA (2) Index mapping between partial and final outputs. These plan information are cached at GPU-side and used as inputs for persistent attention/contraction kernels.
                </div>
              </div>
              
              <div class="bg-blue-50 p-4 rounded-lg border border-blue-200">
                <h5 class="font-semibold text-blue-700 mb-3 flex items-center text-base md:text-lg">
                  <i class="fas fa-info-circle mr-2"></i>技术解释：
                </h5>
                <ul class="list-disc list-inside space-y-2 text-sm md:text-base text-gray-700">
                  <li><strong>负载均衡调度:</strong> 受Stream-K启发，但为了确定性输出，不使用原子聚合。</li>
                  <li><strong>KV分块:</strong> 长KV序列被分割为多个块，最终输出是所有块部分输出的组合。</li>
                  <li><strong>计划信息:</strong> 包括每个CTA的工作队列、部分输出和最终输出之间的索引映射。</li>
                  <li><strong>CUDA Graph兼容:</strong> 注意力和收缩阶段都使用持久内核，网格大小编译后固定，满足CUDA Graph要求。</li>
                  <li><strong>两级调度:</strong> CPU端运行调度器生成计划信息，GPU端使用计划信息执行计算。</li>
                </ul>
              </div>
              
              <div class="bg-green-50 p-4 rounded-lg border border-green-200">
                <h5 class="font-semibold text-green-700 mb-3 flex items-center text-base md:text-lg">
                  <i class="fas fa-cogs mr-2"></i>设计实现：
                </h5>
                <div class="text-sm md:text-base text-gray-700 space-y-3">
                  <div>
                    <strong class="text-green-700">调度算法:</strong>
                    <p>算法1（论文中）详细描述了负载均衡调度过程。输入为序列长度信息，输出为CTA工作队列和输出映射。调度器在CPU上运行，开销可在多个层上摊销，因为相同的计划信息可在所有层重用。</p>
                  </div>
                  <div>
                    <strong class="text-green-700">CUDA Graph集成:</strong>
                    <p>通过固定工作空间缓冲区中每个部分的偏移，确保传递给内核的指针在每个生成步骤中相同，满足CUDA Graph对静态配置的要求。</p>
                  </div>
                  <div>
                    <strong class="text-green-700">内存管理:</strong>
                    <p>使用页锁定的主机缓冲区和设备工作空间缓冲区存储调度器元数据和部分输出。根据调度器元数据和部分输出的上限估计分配最大所需容量。</p>
                  </div>
                </div>
              </div>
            </div>
          </details>
        </div>
      </section>
      
      <section id="evaluation" class="content-section mobile-optimized mb-12 md:mb-16">
        <h2 class="text-2xl md:text-3xl font-bold text-gray-800 mb-6 md:mb-8 flex items-center">
          <i class="fas fa-chart-line mr-3 text-blue-500"></i>
          测试与评估
        </h2>
        
        <div class="mb-10">
          <h3 class="text-xl font-bold text-gray-700 mb-4">4.1 端到端LLM服务性能</h3>
          <p class="text-gray-700 mb-4">
            在SGLang v0.3.4中集成FlashInfer，与Triton后端进行对比。测试使用Llama 3.1 8B（1×H100）和70B（4×H100）模型，数据集包括ShareGPT和合成变长工作负载。
          </p>
          
          <!-- 原图占位符：Figure 7 - ITL and TTFT performance -->
          <details class="technical-details bg-gray-50 rounded-lg p-4 md:p-6 mb-8">
            <summary class="cursor-pointer font-semibold text-lg md:text-xl text-gray-800 hover:text-blue-600 transition-colors py-2">
              <i class="fas fa-chart-bar mr-2"></i>技术细节：图7 - 中位数Token间延迟和首Token时间
            </summary>
            
            <div class="mt-6 space-y-6">
              <div class="original-figure-container">
                <div class="flex flex-col sm:flex-row sm:items-center justify-between mb-4 gap-2">
                  <h5 class="font-semibold text-gray-700 text-base md:text-lg">
                    <i class="fas fa-image mr-2 text-blue-500"></i>
                    原图 7: 集成FlashInfer和Triton的SGLang的中位数ITL和TTFT
                  </h5>
                  <span class="text-xs bg-blue-100 text-blue-800 px-2 py-1 rounded self-start sm:self-auto">论文原图</span>
                </div>
                
                <div class="image-placeholder bg-gradient-to-br from-gray-100 to-gray-200 p-6 md:p-8 rounded-lg text-center border-2 border-dashed border-gray-300">
                  <img src="./images/fig7.png" alt="论文图7: 展示SGLang集成FlashInfer和Triton后端时的中位数Token间延迟(ITL)和中位数首Token时间(TTFT)对比" class="max-w-full h-auto rounded-lg">
                </div>
                
                <div class="mt-4 text-sm text-gray-600 bg-gray-50 p-3 rounded border">
                  <strong>原图图注:</strong> Medium Inter-Token-Latency (ITL) and medium Time-To-First-Token (TTFT) of SGLang integrated with FlashInfer and Triton.
                </div>
              </div>
              
              <div class="bg-blue-50 p-4 rounded-lg border border-blue-200">
                <h5 class="font-semibold text-blue-700 mb-3 flex items-center text-base md:text-lg">
                  <i class="fas fa-info-circle mr-2"></i>性能分析：
                </h5>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-4 text-sm md:text-base">
                  <div>
                    <strong class="text-blue-700">测试配置:</strong>
                    <ul class="list-disc list-inside mt-1 text-gray-700">
                      <li>硬件环境: NVIDIA A100 40GB SXM / H100 80GB SXM</li>
                      <li>软件栈: CUDA 12.4, PyTorch 2.4.0, f16精度</li>
                      <li>工作负载: ShareGPT数据集 + 变长合成工作负载</li>
                      <li>度量指标: ITL, TTFT, P99 TTFT < 200ms</li>
                    </ul>
                  </div>
                  <div>
                    <strong class="text-blue-700">关键结果:</strong>
                    <ul class="list-disc list-inside mt-1 text-gray-700">
                      <li><strong>性能提升:</strong> FlashInfer在所有设置中均比Triton后端有稳定加速</li>
                      <li><strong>延迟降低:</strong> 相比Triton后端，实现29-69%的ITL降低</li>
                      <li><strong>模型规模扩展:</strong> 在8B和70B模型上均表现优异</li>
                    </ul>
                  </div>
                </div>
              </div>
            </div>
          </details>
        </div>
        
        <div class="mb-10">
          <h3 class="text-xl font-bold text-gray-700 mb-4">4.2 内核性能对比</h3>
          <p class="text-gray-700 mb-4">
            与开源FlashAttention库（最新main分支，包含FA2和FA3内核）进行对比。批大小固定为16，测试三种序列长度分布：恒定(1024)、均匀(512-1024)、偏斜(Zipf分布，平均长度1024)。
          </p>
          
          <!-- 原图占位符：Figure 8 - Kernel performance comparison -->
          <details class="technical-details bg-gray-50 rounded-lg p-4 md:p-6 mb-8">
            <summary class="cursor-pointer font-semibold text-lg md:text-xl text-gray-800 hover:text-blue-600 transition-colors py-2">
              <i class="fas fa-microchip mr-2"></i>技术细节：图8 - 解码和预填充内核的带宽和FLOPs利用率
            </summary>
            
            <div class="mt-6 space-y-6">
              <div class="original-figure-container">
                <div class="flex flex-col sm:flex-row sm:items-center justify-between mb-4 gap-2">
                  <h5 class="font-semibold text-gray-700 text-base md:text-lg">
                    <i class="fas fa-image mr-2 text-blue-500"></i>
                    原图 8: 解码（上）和预填充（下）内核实现的带宽和FLOPs利用率
                  </h5>
                  <span class="text-xs bg-blue-100 text-blue-800 px-2 py-1 rounded self-start sm:self-auto">论文原图</span>
                </div>
                
                <div class="image-placeholder bg-gradient-to-br from-gray-100 to-gray-200 p-6 md:p-8 rounded-lg text-center border-2 border-dashed border-gray-300">
                  <img src="./images/fig8.png" alt="论文图8: 解码和预填充内核在不同序列长度分布下的带宽和FLOPs利用率对比，数值越高越好" class="max-w-full h-auto rounded-lg">
                </div>
                
                <div class="mt-4 text-sm text-gray-600 bg-gray-50 p-3 rounded border">
                  <strong>原图图注:</strong> Achieved bandwidth and FLOPs utilizations (the higher the better) for decode (top) and prefill (down) kernels.
                </div>
              </div>
              
              <div class="bg-blue-50 p-4 rounded-lg border border-blue-200">
                <h5 class="font-semibold text-blue-700 mb-3 flex items-center text-base md:text-lg">
                  <i class="fas fa-info-circle mr-2"></i>性能分析：
                </h5>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-4 text-sm md:text-base">
                  <div>
                    <strong class="text-blue-700">解码内核优势:</strong>
                    <ul class="list-disc list-inside mt-1 text-gray-700">
                      <li><strong>多样化分块大小:</strong> 提供(1,16,32,64,128)×(32,64,128)多种分块大小选择</li>
                      <li><strong>自适应选择:</strong> 基于硬件资源和工作负载强度启发式选择最优分块大小</li>
                      <li><strong>优于FA:</strong> FlashAttention对解码使用次优分块大小</li>
                    </ul>
                  </div>
                  <div>
                    <strong class="text-blue-700">预填充内核优势:</strong>
                    <ul class="list-disc list-inside mt-1 text-gray-700">
                      <li><strong>负载均衡调度:</strong> 在均匀和偏斜分布下显著优于FlashAttention</li>
                      <li><strong>动态适应:</strong> 自动适应变化的序列长度分布</li>
                      <li><strong>因果掩码:</strong> 支持LLM服务中常见的因果掩码设置</li>
                    </ul>
                  </div>
                </div>
              </div>
            </div>
          </details>
        </div>
        
        <div class="mb-10">
          <h3 class="text-xl font-bold text-gray-700 mb-4">4.3 长上下文推理定制化</h3>
          <p class="text-gray-700 mb-4">
            测试Streaming-LLM场景，该算法支持百万Token推理且GPU内存使用恒定。FlashInfer仅用20行额外代码即可生成融合RoPE的注意力内核。
          </p>
          
          <!-- 原图占位符：Figure 9 - Streaming-LLM performance -->
          <details class="technical-details bg-gray-50 rounded-lg p-4 md:p-6 mb-8">
            <summary class="cursor-pointer font-semibold text-lg md:text-xl text-gray-800 hover:text-blue-600 transition-colors py-2">
              <i class="fas fa-stream mr-2"></i>技术细节：图9 - Streaming-LLM端到端延迟和带宽利用率
            </summary>
            
            <div class="mt-6 space-y-6">
              <div class="original-figure-container">
                <div class="flex flex-col sm:flex-row sm:items-center justify-between mb-4 gap-2">
                  <h5 class="font-semibold text-gray-700 text-base md:text-lg">
                    <i class="fas fa-image mr-2 text-blue-500"></i>
                    原图 9: Streaming-LLM的性能对比
                  </h5>
                  <span class="text-xs bg-blue-100 text-blue-800 px-2 py-1 rounded self-start sm:self-auto">论文原图</span>
                </div>
                
                <div class="image-placeholder bg-gradient-to-br from-gray-100 to-gray-200 p-6 md:p-8 rounded-lg text-center border-2 border-dashed border-gray-300">
                  <img src="./images/fig9.png" alt="论文图9: 上图展示Streaming-LLM使用FlashInfer融合内核和FlashAttention未融合内核的端到端延迟，下图展示FlashInfer融合RoPE内核与FlashAttention未融合内核的带宽利用率对比" class="max-w-full h-auto rounded-lg">
                </div>
                
                <div class="mt-4 text-sm text-gray-600 bg-gray-50 p-3 rounded border">
                  <strong>原图图注:</strong> Top: End-to-end latency of Streaming-LLM with FlashInfer fused and FlashAttention's unfused kernels, original implementation is included. Down: bandwidth utilization of FlashInfer fused RoPE kernel compared to FlashAttention's unfused kernel.
                </div>
              </div>
              
              <div class="bg-blue-50 p-4 rounded-lg border border-blue-200">
                <h5 class="font-semibold text-blue-700 mb-3 flex items-center text-base md:text-lg">
                  <i class="fas fa-info-circle mr-2"></i>性能分析：
                </h5>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-4 text-sm md:text-base">
                  <div>
                    <strong class="text-blue-700">端到端延迟:</strong>
                    <ul class="list-disc list-inside mt-1 text-gray-700">
                      <li><strong>测试配置:</strong> Vicuna-13B模型，MT-Bench数据集</li>
                      <li><strong>延迟降低:</strong> 在不同设置下实现28-30%的延迟降低</li>
                      <li><strong>优化实现:</strong> 优化原始实现，消除不必要的开销</li>
                    </ul>
                  </div>
                  <div>
                    <strong class="text-blue-700">内核级性能:</strong>
                    <ul class="list-disc list-inside mt-1 text-gray-700">
                      <li><strong>带宽利用率:</strong> FlashInfer融合内核比未融合内核高1.6-3.7倍</li>
                      <li><strong>融合优势:</strong> 将RoPE计算与注意力计算融合，减少内存访问</li>
                      <li><strong>定制化重要性:</strong> 证明了注意力内核定制化的必要性</li>
                    </ul>
                  </div>
                </div>
              </div>
            </div>
          </details>
        </div>
        
        <div class="mb-10">
          <h3 class="text-xl font-bold text-gray-700 mb-4">4.4 并行生成性能</h3>
          <p class="text-gray-700 mb-4">
            测试可组合格式在并行解码中的优势。并行生成(n参数)是LLM服务中的重要任务，尤其在LLM智能体中。共享前缀的存在使得前缀缓存能显著提升并行生成效率。
          </p>
          
          <!-- 原图占位符：Figure 10 - Parallel generation performance -->
          <details class="technical-details bg-gray-50 rounded-lg p-4 md:p-6 mb-8">
            <summary class="cursor-pointer font-semibold text-lg md:text-xl text-gray-800 hover:text-blue-600 transition-colors py-2">
              <i class="fas fa-parachute-box mr-2"></i>技术细节：图10 - 并行生成期间的ITL和TTFT性能
            </summary>
            
            <div class="mt-6 space-y-6">
              <div class="original-figure-container">
                <div class="flex flex-col sm:flex-row sm:items-center justify-between mb-4 gap-2">
                  <h5 class="font-semibold text-gray-700 text-base md:text-lg">
                    <i class="fas fa-image mr-2 text-blue-500"></i>
                    原图 10: MLC-Engine在并行生成期间使用和不使用可组合格式的ITL和TTFT
                  </h5>
                  <span class="text-xs bg-blue-100 text-blue-800 px-2 py-1 rounded self-start sm:self-auto">论文原图</span>
                </div>
                
                <div class="image-placeholder bg-gradient-to-br from-gray-100 to-gray-200 p-6 md:p-8 rounded-lg text-center border-2 border-dashed border-gray-300">
                  <img src="./images/fig10.png" alt="论文图10: 展示MLC-Engine在并行生成期间使用可组合格式和单格式的性能对比，x轴为可组合格式性能，y轴为单格式性能，点在对角线上方表示可组合格式更优" class="max-w-full h-auto rounded-lg">
                </div>
                
                <div class="mt-4 text-sm text-gray-600 bg-gray-50 p-3 rounded border">
                  <strong>原图图注:</strong> ITL and TTFT of MLC-Engine with and without composable formats during parallel generation, the x-axis refers to composable formats performance, and the y-axis refers to single format performance, if a point is above the diagonal line, it means composable formats outperform single format. Different parallel generation n are shown in different colors.
                </div>
              </div>
              
              <div class="bg-blue-50 p-4 rounded-lg border border-blue-200">
                <h5 class="font-bold text-blue-700 mb-3 flex items-center text-base md:text-lg">
                  <i class="fas fa-info-circle mr-2"></i>性能分析：
                </h5>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-4 text-sm md:text-base">
                  <div>
                    <strong class="text-blue-700">测试配置:</strong>
                    <ul class="list-disc list-inside mt-1 text-gray-700">
                      <li>模型: Llama 3.1 8B和70B</li>
                      <li>数据集: ShareGPT</li>
                      <li>请求率: 固定为16</li>
                      <li>并行Token数: 1,2,4,8,16,32,64</li>
                    </ul>
                  </div>
                  <div>
                    <strong class="text-blue-700">关键结果:</strong>
                    <ul class="list-disc list-inside mt-1 text-gray-700">
                      <li><strong>中等并行度(4≤n≤32):</strong> 可组合格式对ITL和TTFT均有稳定加速</li>
                      <li><strong>峰值加速:</strong> n=4时，8B模型ITL降低13.73%，TTFT降低16.41%；70B模型ITL降低17.42%，TTFT降低22.86%</li>
                      <li><strong>小n值:</strong> n值过小，块大小增加不足，无法从可组合格式受益</li>
                      <li><strong>大n值:</strong> 计算不再由注意力主导（尤其是短序列的ShareGPT），优势趋于平稳</li>
                    </ul>
                  </div>
                </div>
              </div>
            </div>
          </details>
        </div>
      </section>
      
      <section id="conclusion" class="content-section mobile-optimized mb-12 md:mb-16">
        <h2 class="text-2xl md:text-3xl font-bold text-gray-800 mb-6 md:mb-8 flex items-center">
          <i class="fas fa-flag-checkered mr-3 text-blue-500"></i>
          结论与不足
        </h2>
        
        <div class="prose max-w-none">
          <h3 class="text-xl font-bold text-gray-700 mb-4">主要结论</h3>
          <div class="bg-green-50 p-5 rounded-lg border border-green-300 mb-6">
            <ul class="list-disc pl-6 space-y-3 text-gray-700">
              <li><strong>统一的KV缓存表示:</strong> 块稀疏格式成功统一了页表、基数树等多种KV缓存存储模式，提供灵活高效的内存管理。</li>
              <li><strong>高度可定制:</strong> JIT编译框架使FlashInfer能够快速适应各种注意力变体，满足不断发展的LLM需求。</li>
              <li><strong>动态负载均衡:</strong> 运行时调度器有效处理输入序列长度的动态变化，提升GPU利用率。</li>
              <li><strong>生产就绪:</strong> 已集成到vLLM、SGLang、MLC-Engine等主流框架，证明其工业实用性。</li>
              <li><strong>显著性能提升:</strong> 在各种推理场景（标准服务、长上下文、并行生成）中均实现显著延迟降低。</li>
            </ul>
          </div>
          
          <h3 class="text-xl font-bold text-gray-700 mb-4">局限性分析</h3>
          <div class="bg-amber-50 p-5 rounded-lg border border-amber-300 mb-6">
            <ul class="list-disc pl-6 space-y-3 text-gray-700">
              <li><strong>仅支持前向传播:</strong> 目前仅支持注意力计算的前向传播，不支持训练所需的反向传播。论文作者计划在未来工作中探索可定制的反向注意力内核模板。</li>
              <li><strong>硬件平台限制:</strong> 目前主要针对NVIDIA GPU优化（Turing到Hopper架构）。虽然调度设计是后端无关的，但需要额外工作来支持其他硬件平台。</li>
              <li><strong>TMA指令限制:</strong> Hopper架构的Tensor Memory Accelerator(TMA)不支持非仿射内存访问模式，因此稀疏收集在FA3模板上无法使用TMA，导致约10%的性能差距。</li>
              <li><strong>Python开销:</strong> 在某些集成中（如vLLM），主机端的Python数组操作开销可能导致轻微性能回归。</li>
              <li><strong>块大小权衡:</strong> 增大块列大小(B_c)可使用TMA优化稀疏收集，但会降低块稀疏格式的灵活性，可能不适用于所有用例。</li>
            </ul>
          </div>
          
          <h3 class="text-xl font-bold text-gray-700 mb-4">未来工作方向</h3>
          <div class="bg-blue-50 p-5 rounded-lg border border-blue-300">
            <ul class="list-disc pl-6 space-y-3 text-gray-700">
              <li><strong>扩展至训练:</strong> 开发可定制的反向注意力内核，支持训练场景。</li>
              <li><strong>高级DSL支持:</strong> 探索将高级领域特定语言(DSL)编译为FlashInfer的注意力规范。</li>
              <li><strong>多后端支持:</strong> 扩展到其他后端，如Triton、ThunderKittens等。</li>
              <li><strong>FP8优化:</strong> 进一步完善FP8-FP16混合精度注意力内核，支持更多LLM模型。</li>
              <li><strong>更细粒度稀疏:</strong> 优化对细粒度KV缓存修剪算法的支持。</li>
            </ul>
          </div>
          
          <div class="mt-8 p-4 bg-gradient-to-r from-gray-50 to-blue-50 rounded-lg border border-gray-300">
            <p class="text-gray-700">
              <strong>项目开源:</strong> FlashInfer项目已在GitHub开源：<a href="https://github.com/flashinfer-ai/flashinfer" class="text-blue-600 hover:underline" target="_blank">https://github.com/flashinfer-ai/flashinfer</a>，并已在生产级系统中大规模部署。
            </p>
          </div>
        </div>
      </section>
    </div>
  </div>
  
  <!-- 交互脚本 -->
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      const navItems = document.querySelectorAll('.nav-item');
      const sections = document.querySelectorAll('section');
      
      function highlightNav() {
        let current = '';
        sections.forEach(section => {
          const sectionTop = section.offsetTop;
          const sectionHeight = section.clientHeight;
          if (window.scrollY >= (sectionTop - 100)) {
            current = section.getAttribute('id');
          }
        });

        navItems.forEach(item => {
          item.classList.remove('active');
          if (item.getAttribute('href') === `#${current}`) {
            item.classList.add('active');
          }
        });
      }

      window.addEventListener('scroll', highlightNav);
      
      navItems.forEach(item => {
        item.addEventListener('click', function(e) {
          e.preventDefault();
          const targetId = this.getAttribute('href');
          const targetSection = document.querySelector(targetId);
          window.scrollTo({
            top: targetSection.offsetTop - 80,
            behavior: 'smooth'
          });
        });
      });
      
      highlightNav();
    });
  </script>
  
  <!-- AI生成内容标识 -->
  <div id="ai-badge" style="position: fixed; bottom: 20px; right: 20px; z-index: 9999; cursor: pointer;">
    <div style="background: linear-gradient(135deg, #6366f1, #8b5cf6); color: white; padding: 8px 16px; border-radius: 20px; font-size: 14px; font-weight: 600; box-shadow: 0 4px 12px rgba(99, 102, 241, 0.3); display: flex; align-items: center; gap: 6px; transition: all 0.3s ease;">
      <span style="font-size: 16px;">🤖</span>
      <span>AI生成</span>
    </div>
  </div>
  
  <script>
    (function(){
      const badge = document.getElementById('ai-badge');
      let expanded = false;
      
      badge.addEventListener('click', function() {
        if(!expanded) {
          const details = document.createElement('div');
          details.id = 'ai-details';
          details.style.cssText = "position:absolute;bottom:50px;right:0;background:white;color:#333;padding:12px;border-radius:8px;box-shadow:0 4px 12px rgba(0,0,0,0.15);width:200px;font-size:12px;line-height:1.5;border:1px solid #e5e7eb;";
          details.innerHTML = '<div style="font-weight:600;margin-bottom:8px;color:#6366f1">人工智能生成内容</div><div style="color:#666">本页面内容通过AI技术自动生成，仅供参考。生成时间：' + new Date().toLocaleDateString('zh-CN') + '</div>';
          badge.appendChild(details);
          expanded = true;
        } else {
          const details = document.getElementById('ai-details');
          if(details) details.remove();
          expanded = false;
        }
      });
    })();
  </script>
</body>
</html>