{
  "id": "e0c3c6d8e9c0",
  "title": "An Expressive, Efficient Attention Architecture",
  "authors": ["Yu Zhang", "Junjie Yan", "Zongyu Lin", "Zhejun Jiang", "Xingcheng Yao", "Weixiao Huang", "Jiaxi Hu", "Bohong Yin", "Fanqing Meng", "Jiacheng You", "Chengyin Liu", "Chu Wei", "Xin Men", "Zhengtao Wang", "Songlin Yang", "Chao Hong", "Zhiyuan Li", "Yutian Chen", "Wentao Li", "Guanduo Chen", "Enzhe Lu", "Yucheng Wang", "Weizhou Liu", "Huabin Zheng", "Yanru Chen", "Feng Wang", "Weixin Xu", "Yibo Liu", "Longhui Yu", "Mengnan Dong", "Yejie Wang", "Zheng Zhang", "Yu Fan", "Siyuan Pan", "Longguang Zhong", "Wenhao Wu", "Enming Yuan", "Yuhao Wu", "Dehao Zhang", "Longyu Guan", "Yizhi Zhang", "Jiawen Tao", "T.Y. Liu", "Guohong Fu", "Haiming Wang", "Xinran Xu", "Shengjun Fang", "Yuzhi Wang", "Weiran He", "Guokun Lai", "Shaowei Liu", "Yuxin Wu", "Yiwei Li", "Xinyu Zhou", "Jianlin Su", "Zhilin Yang", "Jiezhong Qiu", "Yulun Du", "Bo Pang"],
  "year": 2025,
  "conference": "arXiv",
  "category": "自然语言处理",
  "keywords": ["线性注意力", "Kimi Delta Attention", "混合架构", "长上下文", "高效推理", "Delta Rule", "硬件效率"],
  "abstract": "We introduce Kimi Linear, a hybrid linear attention architecture that, for the first time, outperforms full attention under fair comparisons across various scenarios--including short-context, long-context, and reinforcement learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an expressive linear attention module that extends Gated DeltaNet with a finer-grained gating mechanism, enabling more effective use of limited finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware efficiency through a specialized variant of the _Diagonal-Plus-Low-Rank_ (DPLR) transition matrices, which substantially reduces computation compared to the general DPLR formulation while remaining more consistent with the classical delta rule.\nWe pretrain a Kimi Linear model with 3B activated parameters and 48B total parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention (MLA). Our experiments show that with an identical training recipe, Kimi Linear outperforms full MLA with a sizeable margin across all evaluated tasks, while reducing KV cache usage by up to 75% and achieving up to 6x decoding throughput for a 1M context. These results demonstrate that Kimi Linear can be a drop-in replacement for full attention architectures with superior performance and efficiency, including tasks with longer input and output lengths.\nTo support further research, we open-source the KDA kernel and vLLM implementations 1, and release the pre-trained and instruction-tuned model checkpoints. 2"
}