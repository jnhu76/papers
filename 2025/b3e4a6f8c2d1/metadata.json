{
  "id": "b3e4a6f8c2d1",
  "title": "JENGA: Effective Memory Management for Serving LLM with Heterogeneity",
  "authors": ["Chen Zhang", "Kuntai Du", "Shu Liu", "Woosuk Kwon", "Xiangxi Mo", "Yufeng Wang", "Xiaoxuan Liu", "Kaichao You", "Zhuohan Li", "Mingsheng Long", "Jidong Zhai", "Joseph Gonzalez", "Ion Stoica"],
  "year": 2025,
  "conference": "SOSP",
  "category": "操作系统",
  "keywords": ["LLM Serving", "Memory Management", "KV Cache", "PagedAttention", "异构模型", "前缀缓存", "内存碎片"],
  "abstract": "Large language models are widely used but expensive to run. To reduce costs, it is crucial to maximize request batch size through efficient GPU memory management. Existing approaches, such as PagedAttention, struggle with modern LLMs because of the growing heterogeneity in the sizes of models’ internal embeddings and attention mechanisms.\n\nIn this paper, we present Jenga, a memory allocation framework for these heterogeneous LLMs. Jenga tackles two key challenges: (1) memory fragmentation caused by embeddings of different sizes, and (2) unpredictable memory usage from varying attention mechanisms across layers. Jenga employs an attention-property-aware allocator, leveraging the least common multiple (LCM) of embedding sizes to optimize memory usage and performing cache eviction based on attention patterns to enhance memory reuse. We implement Jenga in vLLM, and evaluate it with diverse LLMs, datasets, and GPUs. Evaluations show that Jenga improves GPU memory utilization by up to 83% and serving throughput by up to 2.16x (1.46x on average)."
}