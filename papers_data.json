{
  "time": 1769567971253,
  "papers": [
    {
      "id": "1a2b3c4d5e6f20253VibETen",
      "title": "3VibETensor: System Software for Deep Learning, Fully Generated by AI Agents",
      "authors": [
        "Bing Xu",
        "Terry Chen",
        "Fengzhe Zhou",
        "Tianqi Chen",
        "Yangqing Jia",
        "Vinod Grover",
        "Haicheng Wu",
        "Wei Liu",
        "Craig Wittenbrink",
        "Wen- mei Hwu",
        "Roger Bringmann",
        "Ming- Yu Liu",
        "Luis Ceze",
        "Michael Lightstone",
        "Humphrey Shi"
      ],
      "year": 2026,
      "conference": "arXiv",
      "category": "机器学习系统",
      "keywords": [
        "AI辅助软件工程",
        "深度学习系统软件",
        "大语言模型",
        "编程智能体",
        "系统生成",
        "VIBETENSOR",
        "GPU运行时"
      ],
      "abstract": "VIBETensor is an open- source research system software stack for deep learning, generated by LLM- powered coding agents under high- level human guidance. In this paper, \"fully generated\" refers to code provenance: implementation changes were produced and applied as agent- proposed diffs; validation relied on builds, tests, and differential checks executed by the agent workflow, without per- change manual diff review. It implements a PyTorch- style eager tensor library with a \\(\\mathrm{C + + 20}\\) core (CPU+CUDA), a torch- like Python overlay via nanobind [1], and an experimental Node.js/TypeScript interface. Unlike thin bindings, VIBETensor includes its own tensor/storage system, schema- litc dispatcher, reverse- mode autograd engine, CUDA runtime (streams/events/graphs [2]), a stream- ordered caching allocator with diagnostics, and a stable C ABI for dynamically loaded operator plugins. We view this open- source release as a milestone for AI- assisted software engineering: it demonstrates that coding agents can generate a coherent deep learning runtime spanning language bindings down to CUDA memory management, with validation constrained by builds and tests. We describe the architecture, summarize the workflow used to produce and validate the system, and evaluate the resulting artifact. We report repository scale and test- suite composition, and summarize reproducible microbenchmarks from an accompanying AI- generated kernel suite, including fused attention measured against PyTorch SDPA/FlashAttention [3]. We also report end- to- end training sanity checks on three small workloads (sequence reversal, CIFAR- 10 ViT, and a miniGPT- style model) on NVIDIA H100 (Hopper, SM90) and Blackwell- class GPUs; multi- GPU results are Blackwell- only and rely on an optional CUTLASS- based ring- allreduce plugin gated on CUDA \\(^{13 + }\\) and sm103a toolchain support. Finally, we discuss failure modes that arise in generated system software—in particular a \"Frankenstein\" composition effect where locally correct subsystems can interact to produce globally suboptimal performance. We open- source the resulting system software and evaluation artifacts at https://github.com/NVlabs/vibetensor.",
      "htmlFile": "2026/1a2b3c4d5e6f20253VibETen/index.html"
    },
    {
      "id": "a12b3c4d5e6f2026manifold",
      "title": "mHC: Manifold-Constrained Hyper-Connections",
      "authors": [
        "Zhenda Xie",
        "Yixuan Wei",
        "Huangi Cao",
        "Chenggang Zhao",
        "Chengqi Deng",
        "Jiashi Li",
        "Damai Dai",
        "Huazuo Gao",
        "Jiang Chang",
        "Liang Zhao",
        "Shangyan Zhou",
        "Zhean Xu",
        "Zhengyan Zhang",
        "Wangding Zeng",
        "Shengding Hu",
        "Yuqing Wang",
        "Jingyang Yuan",
        "Lean Wang",
        "Wenfeng Liang"
      ],
      "year": 2026,
      "conference": "arXiv",
      "category": "机器学习",
      "keywords": [
        "超连接",
        "Hyper-Connections",
        "流形约束",
        "Manifold Constraint",
        "残差连接",
        "Residual Connections",
        "训练稳定性",
        "Sinkhorn-Knopp算法",
        "大模型训练"
      ],
      "abstract": "Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.",
      "htmlFile": "2026/a12b3c4d5e6f2026manifold/index.html"
    },
    {
      "id": "a1b2c3d4e5f62026Humanity",
      "title": "Humanity's Last Hallucination: A Forensic Audit of the Scientific Insolvency in GPQA and HLE",
      "authors": [
        "Sammy Zeng"
      ],
      "year": 2026,
      "conference": "arXiv",
      "category": "自然语言处理",
      "keywords": [
        "大语言模型",
        "基准测试",
        "科学破产",
        "GPQA",
        "HLE",
        "Humanity's Last Exam",
        "幻觉"
      ],
      "abstract": "This study conducts a forensic audit of GPQA and HLE, which are widely regarded as the \"gold standards\" for measuring the limits of machine intelligence. Through a four-phase progressive verification process, we reveal severe scientific insolvency issues in both datasets: the inherent error rate lower bounds for GPQA and HLE are as high as 26.8% and 58.0% (37.2% overall), respectively. These systematic fallacies stem from factual errors, missing parameters, and transcription mistakes. Our research shows that GPQA is essentially an \"advanced intellectual booby trap from the old era,\" while HLE is a \"scientific ruin dismembered by filtering mechanisms.\" These two benchmarks have transformed from rulers for measuring intelligence into noise generators measuring how well models fit logical fallacies, causing the ruler we use to measure AI evolution to become severely distorted.",
      "htmlFile": "2026/a1b2c3d4e5f62026Humanity/index.html"
    },
    {
      "id": "a3f8c9d4b5e2",
      "title": "The Reality Gap in Robotics: Challenges, Solutions, and Best Practices",
      "authors": [
        "Elie Aljalbout",
        "Jiaxu Xing",
        "Angel Romero",
        "Iretiayo Akinola",
        "Caelan Reed Garrett",
        "Eric Heiden",
        "Abhishek Gupta",
        "Tucker Hermans",
        "Yashraj Narang",
        "Dieter Fox",
        "Davide Scaramuzza",
        "Fabio Ramos"
      ],
      "year": 2026,
      "conference": "Arxiv",
      "category": "机器人学",
      "keywords": [
        "仿真",
        "机器人学习",
        "sim-to-real transfer",
        "reality gap",
        "sim-to-real",
        "domain randomization",
        "强化学习"
      ],
      "abstract": "Machine learning has facilitated significant advancements across various robotics domains, including navigation, locomotion, and manipulation. Many such achievements have been driven by the extensive use of simulation as a critical tool for training and testing robotic systems prior to their deployment in real-world environments. However, simulations consist of abstractions and approximations that inevitably introduce discrepancies between simulated and real environments, known as the reality gap. These discrepancies significantly hinder the successful transfer of systems from simulation to the real world. Closing this gap remains one of the most pressing challenges in robotics. Recent advances in sim-to-real transfer have demonstrated promising results across various platforms, including locomotion, navigation, and manipulation. By leveraging techniques such as domain randomization, real-to-sim transfer, state and action abstractions, and sim-real co-training, many works have overcome the reality gap. However, challenges persist, and a deeper understanding of the reality gap’s root causes and solutions is necessary. In this survey, we present a comprehensive overview of the sim-to-real landscape, highlighting the causes, solutions, and evaluation metrics for the reality gap and sim-to-real transfer.",
      "htmlFile": "2026/a3f8c9d4b5e2/index.html"
    },
    {
      "id": "f2a7c9e1b3d820245Condition",
      "title": "Conditional Memory as a Complementary Sparsity Axis for Efficient Language Modeling",
      "authors": [
        "Xin Cheng",
        "Wangding Zeng",
        "Damai Dai",
        "Qinyu Chen",
        "Bingxuan Wang",
        "Zhenda Xie",
        "Kezhao Huang",
        "Xingkai Yu",
        "Zhewen Hao",
        "Yukun Li",
        "Han Zhang",
        "Huishuai Zhang",
        "Dongyan Zhao",
        "Wenfeng Liang"
      ],
      "year": 2026,
      "conference": "arXiv",
      "category": "自然语言处理",
      "keywords": [
        "Conditional Memory",
        "Mixture of Experts",
        "N-gram",
        "Sparsity Allocation",
        "Engram",
        "Knowledge Retrieval",
        "Large Language Models"
      ],
      "abstract": "While Mixture-of-Experts (MoE) scales capacity via conditional computation, Transformers lack a native primitive for knowledge lookup, forcing them to inefficiently simulate retrieval through computation. To address this, we introduce conditional memory as a complementary sparsity axis, instantiated via Engram, a module that modernizes classic N-gram embedding for O(1) lookup. By formulating the Sparsity Allocation problem, we uncover a U-shaped scaling law that optimizes the trade-off between neural computation (MoE) and static memory (Engram). Guided by this law, we scale Engram to 27B parameters, achieving superior performance over a strictly iso-parameter and iso-FLOPs MoE baseline. Most notably, while the memory module is expected to aid knowledge retrieval (e.g., MMLU +3.4; CMMLU +4.0), we observe even larger gains in general reasoning (e.g., BBH +5.0; ARC-Challenge +3.7) and code/math domains (HumanEval +3.0; MATH +2.4). Mechanistic analyses reveal that Engram relieves the backbone's early layers from static reconstruction, effectively deepening the network for complex reasoning. Furthermore, by delegating local dependencies to lookups, it frees up attention capacity for global context, substantially boosting long-context retrieval (e.g., MultiQuery NIAH: 84.2 → 97.0). Finally, Engram establishes infrastructure-aware efficiency: its deterministic addressing enables runtime prefetching from host memory, incurring negligible overhead. We envision conditional memory as an indispensable modeling primitive for next-generation sparse models. Code available at: https://github.com/deepseek-ai/Engram",
      "htmlFile": "2026/f2a7c9e1b3d820245Condition/index.html"
    },
    {
      "id": "3a1b5c8d9e0f2025conditio",
      "title": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
      "authors": [
        "Xin Cheng",
        "Wangding Zeng",
        "Damai Dai",
        "Qinyu Chen",
        "Bingxuan Wang",
        "Zhenda Xie",
        "Kezhao Huang",
        "Xingkai Yu",
        "Zhewen Hao",
        "Yukun Li",
        "Han Zhang",
        "Huishuai Zhang",
        "Dongyan Zhao",
        "Wenfeng Liang"
      ],
      "year": 2025,
      "conference": "ICLR",
      "category": "自然语言处理",
      "keywords": [
        "条件记忆",
        "Conditional Memory",
        "稀疏性",
        "Sparsity",
        "大语言模型",
        "Large Language Models",
        "Engram"
      ],
      "abstract": "While Mixture-of-Experts (MoE) scales capacity via conditional computation, Transformers lack a native primitive for knowledge lookup, forcing them to inefficiently simulate retrieval through computation. To address this, we introduce conditional memory as a complementary sparsity axis, instantiated via **Engram**, a module that modernizes classic $N$-gram embedding for $O(1)$ lookup. By formulating the _Sparsity Allocation_ problem, we uncover a U-shaped scaling law that optimizes the trade-off between neural computation (MoE) and static memory (Engram). Guided by this law, we scale Engram to 27B parameters, achieving superior performance over a strictly iso-parameter and iso-FLOPs MoE baseline. Most notably, while the memory module is expected to aid knowledge retrieval (e.g., MMLU +3.4; CMMLU +4.0), we observe even larger gains in general reasoning (e.g., BBH +5.0; ARC-Challenge +3.7) and code/math domains (HumanEval +3.0; MATH +2.4). Mechanistic analyses reveal that Engram relieves the backbone's early layers from static reconstruction, effectively deepening the network for complex reasoning. Furthermore, by delegating local dependencies to lookups, it frees up attention capacity for global context, substantially boosting long-context retrieval (e.g., Multi-Query NIAH: 84.2 → 97.0). Finally, Engram establishes infrastructure-aware efficiency: its deterministic addressing enables runtime prefetching from host memory, incurring negligible overhead. We envision conditional memory as an indispensable modeling primitive for next-generation sparse models.",
      "htmlFile": "2025/3a1b5c8d9e0f2025conditio/index.html"
    },
    {
      "id": "6a1e9f9c8c4b",
      "title": "AEOLIA: A Fast and Secure Userspace Interrupt-Based Storage Stack",
      "authors": [
        "Chuandong Li",
        "Ran Yi",
        "Zonghao Zhang",
        "Jing Liu",
        "Changwoo Min",
        "Jie Zhang",
        "Yingwei Luo",
        "Xiaolin Wang",
        "Zhenlin Wang",
        "Diyu Zhou"
      ],
      "year": 2025,
      "conference": "SOSP",
      "category": "操作系统与存储系统",
      "keywords": [
        "用户中断",
        "用户空间文件系统",
        "用户空间NVMe驱动",
        "高性能存储",
        "调度"
      ],
      "abstract": "Polling-based userspace storage stacks achieve great I/O performance. However, they cannot efficiently and securely share disks and CPUs among multiple tasks. In contrast, interrupt-based kernel stacks inherently suffer from subpar I/O performance but achieve advantages in resource sharing.\n\nWe present AEOLIA, a novel storage stack that achieves great I/O performance while offering efficient and secure resource sharing. AEOLIA is an interrupt-based userspace storage stack, representing a new point in the design space previously considered unfeasible. Our main observation is that, contrary to conventional wisdom, polling offers only marginal disk performance improvements over interrupts. AEOLIA exploits user interrupt, an emerging hardware feature commonly used for userspace IPIs, in a novel way to deliver storage interrupts directly to userspace, thereby achieving high I/O performance with direct access. AEOLIA leverages the hardware intra-process isolation features and sched_ext, an eBPF-based userspace scheduling framework, to efficiently and securely share CPUs and disks among multiple tasks, challenging the common belief that these are inherent disadvantages of userspace storage stacks. The above design enables AEOLIA to realize AsofS, a high-performance library file system that securely and directly accesses disks. Our evaluation shows that AEOLIA outperforms Linux by 2x and AsofS outperforms ext4 by up to 19.1x, respectively.",
      "htmlFile": "2025/6a1e9f9c8c4b/index.html"
    },
    {
      "id": "6e7d8c9a1b2f2025Insights",
      "title": "Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures",
      "authors": [
        "Chenggang Zhao",
        "Chengqi Deng",
        "Chong Ruan",
        "Damai Dai",
        "Huazuo Gao",
        "Jiashi Li",
        "Liyue Zhang",
        "Panpan Huang",
        "Shangyan Zhou",
        "Shirong Ma",
        "Wenfeng Liang",
        "Ying He",
        "Yuqing Wang",
        "Yuxuan Liu",
        "Y.X. Wei"
      ],
      "year": 2025,
      "conference": "ISCA",
      "category": "高性能计算",
      "keywords": [
        "大语言模型",
        "MoE",
        "FP8",
        "硬件软件协同设计",
        "网络拓扑",
        "推理效率",
        "低精度计算"
      ],
      "abstract": "The rapid scaling of large language models (LLMs) has unveiled critical limitations in current hardware architectures, including constraints in memory capacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3, trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model co-design can effectively address these challenges, enabling cost-efficient training and inference at scale. This paper presents an in-depth analysis of the DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting key innovations such as Multi-head Latent Attention (MLA) for enhanced memory efficiency, Mixture of Experts (MoE) architectures for optimized computation-communication trade-offs, FP8 mixed-precision training to unlock the full potential of hardware capabilities, and a Multi-Plane Network Topology to minimize cluster-level network overhead. Building on the hardware bottlenecks encountered during DeepSeek-V3's development, we engage in a broader discussion with academic and industry peers on potential future hardware directions, including precise low-precision computation units, scale-up and scale-out convergence, and innovations in low-latency communication fabrics. These insights underscore the critical role of hardware and model co-design in meeting the escalating demands of AI workloads, offering a practical blueprint for innovation in next-generation AI systems.",
      "htmlFile": "2025/6e7d8c9a1b2f2025Insights/index.html"
    },
    {
      "id": "7b4a9c1e2f862025The",
      "title": "The Streaming Batch Model for Efficient and Fault-Tolerant Heterogeneous Execution",
      "authors": [
        "Frank Sifei Luan",
        "Ron Yifeng Wang",
        "Yile Gu",
        "Ziming Mao",
        "Charlotte Lin",
        "Amog Kamsetty",
        "Hao Chen",
        "Cheng Su",
        "Balaji Veeramani",
        "Scott Lee",
        "SangBin Cho",
        "Clark Zinzow",
        "Eric Liang",
        "Ion Stoica",
        "Stephanie Wang"
      ],
      "year": 2025,
      "conference": "arXiv",
      "category": "分布式系统与机器学习系统",
      "keywords": [
        "流式批处理模型",
        "异构计算",
        "分布式数据处理",
        "机器学习系统",
        "Ray Data",
        "资源弹性",
        "容错性"
      ],
      "abstract": "While ML model training and inference are both GPU-intensive, CPU-based data processing is often the bottleneck. Distributed data processing systems based on the batch or stream processing models assume homogeneous resource requirements. They excel at CPU-based computation but either under-utilize heterogeneous resources or impose high overheads on failure and reconfiguration.\n\nWe introduce the _streaming batch_ model, a hybrid of batch and streaming that enables efficient and fault-tolerant heterogeneous execution. The key idea is to use _partitions_ as the unit of execution to achieve elasticity, but to allow partitions to be dynamically created and streamed between heterogeneous operators for memory-efficient pipelining. We present Ray Data, a streaming batch system that improves throughput on heterogeneous batch inference pipelines by 2.5-12× compared to traditional batch and stream processing systems. By leveraging heterogeneous clusters, Ray Data improves training throughput for multimodal models such as Stable Diffusion by 31% compared to single-node ML data loaders.",
      "htmlFile": "2025/7b4a9c1e2f862025The/index.html"
    },
    {
      "id": "8e54f19a2c7b2025d2fsdevic",
      "title": "D2FS: Device-Driven Filesystem Garbage Collection",
      "authors": [
        "Juwon Kim",
        "Seungjae Lee",
        "Joontaek Oh",
        "Dongkun Shin",
        "Youjip Won"
      ],
      "year": 2025,
      "conference": "FAST",
      "category": "文件系统与存储技术",
      "keywords": [
        "设备驱动垃圾回收 (Device-Driven Garbage Collection)",
        "日志结构文件系统 (Log-structured Filesystem)",
        "FTL (Flash Translation Layer)",
        "迁移回调 (Migration Upcall)",
        "虚拟过度配置 (Virtual Overprovisioning)",
        "D2FS"
      ],
      "abstract": "In this work, we propose a mechanism to free the log-structured filesystem from running the garbage collection. We exploit the garbage collection functionality of the underlying flash storage to reclaim the invalid sections in the filesystem partition. We call it a Log-structured Filesystem with Device-Driven Garbage Collection, D2FS. D2FS consists of three key ingredients: Coupled Garbage Collection, Migration Upcall, and Virtual Overprovisioning. Coupled Garbage Collection consolidates the valid flash pages at the storage device and remaps the migrated flash pages to new filesystem locations so that the valid pages are clustered not only physically but also logically. Migration Upcall asynchronously notifies the host about the file mappings updated by the Coupled Garbage Collection, minimizing interference with the foreground filesystem operations. Virtual Overprovisioning separates the size of the filesystem partition from the physical capacity of the associated storage partition and sets the size of the filesystem partition larger than the physical storage partition. Virtual overprovisioning ensures that FTL runs the device-level garbage collection on time so that the filesystem partition never runs out of free sections. By integrating these techniques, we save the log-structured filesystem from the garbage collection overhead, a primary obstacle hindering its widespread adoption in production environments. D2FS outperforms F2FS by 3× (FIO), zoned F2FS by 1.7× (FIO), and IPLFS by 1.5× (MySQL YCSB-F).",
      "htmlFile": "2025/8e54f19a2c7b2025d2fsdevic/index.html"
    },
    {
      "id": "8f3a7c1b9e2d",
      "title": "Don’t Maintain Twice, It’s Alright: Merged Metadata Management in Deduplication File System with GOGETAFS",
      "authors": [
        "Yanqi Pan",
        "Wen Xia",
        "Erci Xu",
        "Hao Huang",
        "Xiangyu Zou",
        "Shiyi Li"
      ],
      "year": 2025,
      "conference": "FAST",
      "category": "存储系统",
      "keywords": [
        "去重文件系统",
        "元数据管理",
        "LFP映射",
        "崩溃一致性",
        "非加密哈希",
        "持久内存",
        "GogetaFS"
      ],
      "abstract": "Emerging storage technologies, such as persistent memory and ultra-low latency SSD, enable the deduplication file system (DedupFS) to use non-cryptographic hash for fast fingerprinting. However, we find that the accelerated computation exposes another major performance penalty: the seemingly innocuous in-storage deduplication metadata maintenance incurs up to 38% overhead in the I/O path.\nWe find the root cause is that DedupFSes maintain dedup-specific _fingerprint-to-physical_ mappings, which incurs additional crash consistency overheads. However, this overhead is unnecessary. Our insight is that the deduplication mapping can be merged with the file system _logical-to-physical_ mapping, forming a _logical-fingerprint-physical_ (LFP) mapping. Thus, we can persist deduplication metadata alongside file system metadata in a single I/O. We propose GogetaFS to realize the efficiency of LFP. Using a series of techniques to manage data and metadata atop LFP, GogetaFS achieves compatible, effective, and memory-efficient deduplication within the file system. Experiments across a range of workloads show that GogetaFS consistently outperforms existing DedupFSes and can minimize metadata maintenance overheads.",
      "htmlFile": "2025/8f3a7c1b9e2d/index.html"
    },
    {
      "id": "8f3a7c2b1d9e",
      "title": "Densing law of LLMs",
      "authors": [
        "Chaojun Xiao",
        "Jie Cai",
        "Weilin Zhao",
        "Biyuan Lin",
        "Guoyang Zeng",
        "Jie Zhou",
        "Zhi Zheng",
        "Xu Han",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "year": 2025,
      "conference": "Nature Mach. Intell.",
      "category": "自然语言处理",
      "keywords": [
        "大语言模型",
        "能力密度",
        "缩放定律",
        "模型效率",
        "推理成本",
        "Densing law",
        "参数效率"
      ],
      "abstract": "Large language models (LLMs) have emerged as a milestone in artificial intelligence. The scaling law indicates that the performance of LLMs can continually improve as the model size increases, which poses challenges for training and deployment. Despite numerous efforts to improve LLM efficiency, there is no general consensus on development trends and evaluation metrics for efficiency of LLMs with different scales. To address this tension between model performance and efficiency, we introduce the concept of capability density as a metric to evaluate the quality of the LLMs and describe the trend of LLMs in terms of both effectiveness and efficiency. Intuitively, capability density can be understood as the capability contained within each unit of model parameters. Capability density provides a unified framework for assessing both model performance and efficiency. Here we show an empirical observation, called the 'densing law', that the capability density of LLMs grows exponentially over time. More specifically, using widely used benchmarks for evaluation, the maximum capability density of open-source LLMs doubles approximately every 3.5 months. This reveals that both parameter requirements and inference costs of LLMs for achieving equivalent performance decrease exponentially, offering insights for efficient LLM development strategies.",
      "htmlFile": "2025/8f3a7c2b1d9e/index.html"
    },
    {
      "id": "8f3a9c1b2e4d",
      "title": "PolyStore: Exploiting Combined Capabilities of Heterogeneous Storage",
      "authors": [
        "Yujie Ren",
        "David Domingo",
        "Jian Zhang",
        "Paul John",
        "Rekha Pitchumani",
        "Sanidhya Kashyap",
        "Sudarsun Kannan"
      ],
      "year": 2025,
      "conference": "FAST",
      "category": "存储系统",
      "keywords": [
        "异构存储",
        "水平架构",
        "累积带宽",
        "元数据层",
        "DRAM缓存",
        "持久性",
        "PolyStore"
      ],
      "abstract": "With the \"non-hierarchical\" trend in emerging storage media, the philosophy of hierarchy inevitably falls short in fully leveraging the combined bandwidth of multiple devices. In this paper, we propose a horizontally structured storage architecture that leverages the combined capabilities of heterogeneous devices. We introduce PolyStore, a meta layer atop storage medium-optimized file systems that spans userspace and the OS, allowing applications to access multiple storage devices concurrently with transparent, fine-grained data placement. PolyStore maximizes cumulative storage bandwidth and reduces hardware and software bottlenecks without compromising important properties such as sharing and security. Our evaluations show that PolyStore achieves 1.11x - 9.38x performance gains for micro-benchmarks and 1.52x - 2.02x for real-world applications across various device configurations.",
      "htmlFile": "2025/8f3a9c1b2e4d/index.html"
    },
    {
      "id": "9a9e5f1a8c1c",
      "title": "cache_ext: Customizing the Page Cache with eBPF",
      "authors": [
        "Tal Zussman",
        "Andrew Cheng",
        "Ioannis Zarkadas",
        "Jeremy Carin",
        "Hubertus Franke",
        "Jonas Pfefferle",
        "Asaf Cidon"
      ],
      "year": 2025,
      "conference": "SOSP",
      "category": "操作系统",
      "keywords": [
        "操作系统",
        "eBPF",
        "页缓存",
        "缓存策略",
        "内存管理",
        "Linux内核",
        "驱逐算法"
      ],
      "abstract": "The OS page cache is central to the performance of many applications, by reducing excessive accesses to storage. However, its one-size-fits-all eviction policy performs poorly in many workloads. While the systems community has experimented with a plethora of new and adaptive eviction policies in non-OS settings (e.g., key-value stores, CDNs), it is very difficult to implement such policies in the page cache, due to the complexity of modifying kernel code. To address these shortcomings, we design a flexible eBPF-based framework for the Linux page cache, called cache_ext, that allows developers to customize the page cache without modifying the kernel. cache_ext enables applications to customize the page cache policy for their specific needs, while also ensuring that different applications' policies do not interfere with each other and preserving the page cache's ability to share memory across different processes. We demonstrate the flexibility of cache_ext's interface by using it to implement eight different policies, including sophisticated eviction algorithms. Our evaluation shows that it is indeed beneficial for applications to customize the page cache to match their workloads' unique properties, and that they can achieve up to 70% higher throughput and 58% lower tail latency.",
      "htmlFile": "2025/9a9e5f1a8c1c/index.html"
    },
    {
      "id": "9b2c0f81e4a52025mist",
      "title": "Mist: Efficient Distributed Training of Large Language Models via Memory-Parallelism Co-Optimization",
      "authors": [
        "Zhanda Zhu",
        "Christina Giannoula",
        "Muralidhar Andoorveedu",
        "Qidong Su",
        "Karttikeya Mangalam",
        "Bojian Zheng",
        "Gennady Pekhimenko"
      ],
      "year": 2025,
      "conference": "EuroSys",
      "category": "机器学习系统",
      "keywords": [
        "大语言模型",
        "分布式训练",
        "内存优化",
        "并行优化",
        "自动化系统",
        "Pipeline Parallelism",
        "ZeRO"
      ],
      "abstract": "Various parallelism, such as data, tensor, and pipeline parallelism, along with memory optimizations like activation checkpointing, redundancy elimination, and offloading, have been proposed to accelerate distributed training for Large Language Models. To find the best combination of these techniques, automatic distributed training systems are proposed. However, existing systems only tune a subset of optimizations, due to the lack of overlap awareness, inability to navigate the vast search space, and ignoring the inter-microbatch imbalance, leading to sub-optimal performance. To address these shortcomings, we propose Mist, a memory, overlap, and imbalance-aware automatic distributed training system that comprehensively co-optimizes all memory footprint reduction techniques alongside parallelism. Mist is based on three key ideas: (1) fine-grained overlap-centric scheduling, orchestrating optimizations in an overlapped manner, (2) symbolic-based performance analysis that predicts runtime and memory usage using symbolic expressions for fast tuning, and (3) imbalance-aware hierarchical tuning, decoupling the process into an inter-stage imbalance and overlap aware Mixed Integer Linear Programming problem and an intra-stage Dual-Objective Constrained Optimization problem, and connecting them through Pareto frontier sampling. Our evaluation results show that Mist achieves an average of 1.28× (up to 1.73×) and 1.27× (up to 2.04×) speedup compared to state-of-the-art manual system Megatron-LM and state-of-the-art automatic system Aceso, respectively.",
      "htmlFile": "2025/9b2c0f81e4a52025mist/index.html"
    },
    {
      "id": "9b5b6d5f3a8b",
      "title": "Unlocking the Potential of CXL for Disaggregated Memory in Cloud-Native Databases",
      "authors": [
        "Xinjun Yang",
        "Gerry Fan",
        "Yuhui Wang",
        "Yingqiang Zhang",
        "Hao Chen*",
        "Bo Wang",
        "Weupu Hu",
        "Feifei Li",
        "Jing Fang",
        "Jim Kao",
        "Yang Kong",
        "Tao Huang",
        "Jianping Jiang"
      ],
      "year": 2025,
      "conference": "SIGMOD",
      "category": "数据库系统",
      "keywords": [
        "Compute Express Link (CXL)",
        "云原生数据库",
        "内存解耦",
        "内存池化",
        "缓存一致性",
        "即时恢复",
        "数据共享"
      ],
      "abstract": "Memory disaggregation has become a major trend in cloud-native databases. However, most existing memory disaggregation solutions suffer from read/write amplification, limited bandwidth, inefficient recovery, and challenges in data sharing. Fortunately, the emerging CXL technology introduces new opportunities for memory disaggregation design in cloud-native databases.\nTo overcome these challenges, we leverage the CXL switch to design _PolarCXLMem_, a CXL-switch-based disaggregated memory system for cloud-native databases. To the best of our knowledge, _PolarCXLMem_ is the first CXL-switch-based disaggregated memory system. Building on _PolarCXLMem_, we propose a novel instant recovery scheme, _PolarRecv_, which enables instant recovery and fast buffer pool warm-up after a crash. To further support _PolarCXLMem_ in multi-primary databases, we design a new cache coherency protocol that facilitates data sharing between database nodes based on _PolarCXLMem_. Finally, we evaluate _PolarCXLMem_ with PolarDB, a widely deployed cloud-native database, under various workloads. This is the first study, to our knowledge, that investigates the performance of CXL-based disaggregated memory in a commercially deployed cloud-native database. Our evaluation shows that _PolarCXLMem_ can improve throughput by up to 2.1x in pooling scenarios and 1.55x in sharing scenarios compared to RDMA-based systems.",
      "htmlFile": "2025/9b5b6d5f3a8b/index.html"
    },
    {
      "id": "9b5f5c6c8c2f",
      "title": "Scalable Address Spaces using Concurrent Interval Skiplist",
      "authors": [
        "Kim, Tae Woo",
        "Kwon, Youngjin",
        "Kang, Jeehoon"
      ],
      "year": 2025,
      "conference": "SOSP",
      "category": "操作系统",
      "keywords": [
        "操作系统",
        "地址空间",
        "可扩展性",
        "并发数据结构",
        "内存管理",
        "并行操作",
        "区间跳表"
      ],
      "abstract": "A kernel's address space design can significantly bottleneck multi-threaded applications, as address space operations such as mmap() and mumap() are serialized by coarse-grained locks like Linux's mmap_lock. Such locks have long been known as one of the most intractable contention points in memory management. While prior works have attempted to address this issue, they either fail to sufficiently parallelize operations or are impractical for real-world kernels. We present the first scalable and practical address space design that parallelizes critical operations. We identify key scalability bottlenecks—many of which extend beyond address spaces—and address them with targeted solutions. At its core is the concurrent interval skiplist, a new data structure that integrates mapping and locking for parallel interval operations. We implement our design on Linux 6.8 and evaluate it on a dual-socket 48-core machine. Our results show a significant throughput improvement of 13.1× for an mmap() microbenchmark, 4.49× for LevelDB, 3.19× for the Apache web server, 1.47× for Metis MapReduce, and 1.27× for Psearchy text indexing.",
      "htmlFile": "2025/9b5f5c6c8c2f/index.html"
    },
    {
      "id": "9d9c7f9c9c9b",
      "title": "Why Language Models Hallucinate",
      "authors": [
        "Adam Tauman Kalai",
        "Ohr Nachum",
        "Santosh S. Vempala",
        "Edwin Zhang"
      ],
      "year": 2025,
      "conference": "arXiv",
      "category": "自然语言处理",
      "keywords": [
        "语言模型",
        "幻觉",
        "预训练",
        "后训练",
        "评估基准",
        "不确定性",
        "错误分析"
      ],
      "abstract": "Like students facing hard exam questions, large language models sometimes guess when uncertain, producing plausible yet incorrect statements instead of admitting uncertainty. Such \"hallucinations\" persist even in state-of-the-art systems and undermine trust. We argue that language models hallucinate because the training and evaluation procedures reward guessing over acknowledging uncertainty, and we analyze the statistical causes of hallucinations in the modern training pipeline. Hallucinations need not be mysterious--they originate simply as errors in binary classification. If incorrect statements cannot be distinguished from facts, then hallucinations in pretrained language models will arise through natural statistical pressures. We then argue that hallucinations persist due to the way most evaluations are graded--language models are optimized to be good test-takers, and guessing when uncertain improves test performance. This \"epidemic\" of penalizing uncertain responses can only be addressed through a socio-technical mitigation: modifying the scoring of existing benchmarks that are misaligned but dominate leaderboards, rather than introducing additional hallucination evaluations. This change may steer the field toward more trustworthy AI systems.",
      "htmlFile": "2025/9d9c7f9c9c9b/index.html"
    },
    {
      "id": "a0f5b5c7c7c9",
      "title": "Pruning in Snowflake: Working Smarter, Not Harder",
      "authors": [
        "Andreas Zimmerer",
        "Damien Dam",
        "Jan Kossmann",
        "Juliane Waack",
        "Ismail Oukid",
        "Andreas Kipf"
      ],
      "year": 2025,
      "conference": "SIGMOD",
      "category": "数据库系统",
      "keywords": [
        "数据仓库",
        "数据跳过",
        "分区剪枝",
        "top-k检索",
        "LIMIT剪枝",
        "连接剪枝",
        "分析查询处理"
      ],
      "abstract": "Modern cloud-based data analytics systems must efficiently process petabytes of data residing on cloud storage. A key optimization technique in state-of-the-art systems like Snowflake is partition pruning--skipping chunks of data that do not contain relevant information for computing query results.\n\nWhile partition pruning based on query predicates is a well-established technique, we present new pruning techniques that extend the scope of partition pruning to LIMT, top-k, and JOIN operations, significantly expanding the opportunities for pruning across diverse query types. We detail the implementation of each method and examine their impact on real-world workloads.\n\nOur analysis of Snowflake's production workloads reveals that real-world analytical queries exhibit much higher selectivity than commonly assumed, yielding effective partition pruning and highlighting the need for more realistic benchmarks. We show that we can harness high selectivity by utilizing min/max metadata available in modern data analytics systems and data lake formats like Apache Iceberg, reducing the number of processed micro-partitions by 99.4% across the Snowflake data platform.",
      "htmlFile": "2025/a0f5b5c7c7c9/index.html"
    },
    {
      "id": "a12b34c56d782025Does",
      "title": "Does AI-Assisted Coding Deliver? A Difference-in-Differences Study of Cursor's Impact on Software Projects",
      "authors": [
        "Hao He",
        "Courtney Miller",
        "Shyam Agarwal",
        "Christian Kästner",
        "Bogdan Vasilescu"
      ],
      "year": 2025,
      "conference": "arXiv",
      "category": "软件工程",
      "keywords": [
        "AI辅助编程",
        "大语言模型",
        "软件开发速度",
        "软件质量",
        "双重差分法",
        "技术债",
        "Cursor"
      ],
      "abstract": "Large language models (LLMs) have demonstrated the promise to revolutionize the field of software engineering. Among other things, LLM agents are rapidly gaining momentum in their application to software development, with practitioners claiming a multifold productivity increase after adoption. Yet, empirical evidence is lacking around these claims. In this paper, we estimate the _causal_ effect of adopting a widely popular LLM agent assistant, namely Cursor, on _development velocity_ and _software quality_. The estimation is enabled by a state-of-the-art difference-in-differences design comparing Cursor-adopting GitHub projects with a matched control group of similar GitHub projects that do not use Cursor. We find that the adoption of Cursor leads to a significant, large, but transient increase in project-level development velocity, along with a significant and persistent increase in static analysis warnings and code complexity. Further panel generalized method of moments estimation reveals that the increase in static analysis warnings and code complexity acts as a major factor causing long-term velocity slowdown. Our study carries implications for software engineering practitioners, LLM agent assistant designers, and researchers.",
      "htmlFile": "2025/a12b34c56d782025Does/index.html"
    },
    {
      "id": "a1b2c3d4e5f62025Beyond",
      "title": "Beyond Smoothed Analysis: Analyzing the Simplex Method by the Book",
      "authors": [
        "Eleon Bach",
        "Alexander E. Black",
        "Sophie Huiberts",
        "Sean Kafer"
      ],
      "year": 2025,
      "conference": "STOC",
      "category": "算法分析",
      "keywords": [
        "算法分析框架",
        "单纯形法",
        "平滑分析",
        "实际性能",
        "线性规划",
        "多项式时间",
        "by the book analysis"
      ],
      "abstract": "Narrowing the gap between theory and practice is a longstanding goal of the algorithm analysis community. To further progress our understanding of how algorithms work in practice, we propose a new algorithm analysis framework that we call by the book analysis. In contrast to earlier frameworks, by the book analysis not only models an algorithm's input data, but also the algorithm itself. Results from by the book analysis are meant to correspond well with established knowledge of an algorithm's practical behavior, as they are meant to be grounded in observations from implementations, input modeling best practices, and measurements on practical benchmark instances. We apply our framework to the simplex method, an algorithm which is beloved for its excellent performance in practice and notorious for its high running time under worst-case analysis. The simplex method similarly showcased the state of the art framework smoothed analysis (Spielman and Teng, STOC'01). We explain how our framework overcomes several weaknesses of smoothed analysis and we prove that under input scaling assumptions, feasibility tolerances and other design principles used by simplex method implementations, the simplex method indeed attains a polynomial running time.",
      "htmlFile": "2025/a1b2c3d4e5f62025Beyond /index.html"
    },
    {
      "id": "a1b2c3d4e5f62025VULPOCon",
      "title": "VULPO: Context-Aware Vulnerability Detection via On-Policy LLM Optimization",
      "authors": [
        "Youpeng Li",
        "Fuxun Yu",
        "Xinda Wang"
      ],
      "year": 2025,
      "conference": "Arxiv",
      "category": "软件安全与代码分析",
      "keywords": [
        "Vulnerability Detection",
        "Context-Aware",
        "On-Policy Reinforcement Learning",
        "Large Language Model",
        "Code Property Graph"
      ],
      "abstract": "The widespread reliance on open-source software dramatically increases the risk of vulnerability exploitation, underscoring the need for effective and scalable vulnerability detection (VD). Existing VD techniques, whether traditional machine learning-based or LLM-based approaches like prompt engineering, supervised fine-tuning, or off-policy preference optimization, remain fundamentally limited in their ability to perform context-aware analysis: They depend on fixed inputs or static preference datasets, cannot adaptively explore repository-level dependencies, and are constrained by function-level benchmarks that overlook critical vulnerability context.\nThis paper introduces Vulnerability-Adaptive Policy Optimization (VULPO), an on-policy LLM reinforcement learning framework for context-aware VD. To support training and evaluation, we first construct ContextVul, a new dataset that augments high-quality function-level samples with lightweight method to extract repository-level context information. We then design multi-dimensional reward structuring that jointly captures prediction correctness, vulnerability localization accuracy, and the semantic relevance of vulnerability analysis, thereby guiding the model toward comprehensive contextual reasoning. To address the asymmetric difficulty of different vulnerability cases and mitigate reward hacking, VULPO incorporates label-level and sample-level difficulty-adaptive reward scaling, encouraging the model to explore challenging cases while maintaining balanced reward distribution.\nExtensive experiments demonstrate the superiority of our VULPO framework in context-aware VD: Our VULPO-4B substantially outperforms existing VD baselines based on prompt engineering and off-policy optimization, improving F1 by 85% over Qwen3-4B and achieving performance comparable to a 150x larger-scale model, DeepSeek-R1-0528.",
      "htmlFile": "2025/a1b2c3d4e5f62025VULPOCon/index.html"
    },
    {
      "id": "a3b5c7d9e1f2",
      "title": "Analyzing Modern NVIDIA GPU cores",
      "authors": [
        "Rodrigo Huerta",
        "Joselorenzo Cruz",
        "Mojtaba Abaie Shoushtary",
        "Antonio Gonzalez"
      ],
      "year": 2025,
      "conference": "MICRO",
      "category": "计算机体系结构",
      "keywords": [
        "GPU",
        "微架构",
        "逆向工程",
        "NVIDIA",
        "模拟器",
        "编译器指导",
        "依赖管理"
      ],
      "abstract": "GPUs are the most popular platform for accelerating HPC workloads, such as artificial intelligence and science simulations. However, most microarchitectural research in academia relies on GPU core pipeline designs based on architectures that are more than 15 years old.\nThis paper reverse engineers modern NVIDIA GPU cores, unveiling many key aspects of its design and explaining how GPUs leverage hardware-compiler techniques where the compiler guides hardware during execution. In particular, it reveals how the issue logic works including the policy of the issue scheduler, the structure of the register file and its associated cache, and multiple features of the memory pipeline. Moreover, it analyses how a simple instruction prefetcher based on a stream buffer fits well with modern NVIDIA GPUs and is likely to be used. Furthermore, we investigate the impact of the register file cache and the number of register file read ports on both simulation accuracy and performance.\nBy modeling all these new discovered microarchitectural details, we achieve 18.24% lower mean absolute percentage error (MAPE) in execution cycles than previous state-of-the-art simulators, resulting in an average of 13.98% MAPE with respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that this new model stands for other NVIDIA architectures, such as Turing.\nFinally, we show that the software-based dependence management mechanism included in modern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in terms of performance and area.",
      "htmlFile": "2025/a3b5c7d9e1f2/index.html"
    },
    {
      "id": "a3f9b8c7d1e2",
      "title": "AegonKV: A High Bandwidth, Low Tail Latency, and Low Storage Cost KV-Separated LSM Store with SmartSSD-based GC Offloading",
      "authors": [
        "Zhuohui Duan",
        "Hao Feng",
        "Haikun Liu",
        "Xiaofei Liao",
        "Hai Jin",
        "Bangyu Li"
      ],
      "year": 2025,
      "conference": "FAST",
      "category": "存储系统",
      "keywords": [
        "键值存储",
        "LSM树",
        "垃圾回收",
        "SmartSSD",
        "近数据处理",
        "KV分离",
        "GC卸载"
      ],
      "abstract": "The key-value separation is renowned for its significant mitigation of the write amplification inherent in traditional LSM trees. However, KV separation potentially increases performance overhead in the management of Value region, especially for _garbage collection_ (GC) operation that is used to reduce the redundant space occupation. In response, many efforts have been made to optimize the GC mechanism for KV separation. However, our analysis indicates that such solution based on trade-offs between CPU and I/O overheads cannot simultaneously satisfy the three requirements of KV separated systems in terms of throughput, tail latency, and space usage. This limitation hinders their real-world application.\n\nIn this paper, we introduce AegonKV, a \"three-birds-one-stone\" solution that comprehensively enhances the throughput, tail latency, and space usage of KV separated systems. AegonKV first proposes a SmartSSD-based GC offloading mechanism to enable asynchronous GC operations without competing with LSM read/write for bandwidth or CPU. AegonKV leverages offload-friendly data structures and hardware/software execution logic to address the challenges of GC offloading. Experiments demonstrate that AegonKV achieves the largest throughput improvement of 1.28-3.3 times, a significant reduction of 37%-66% in tail latency, and 15%-85% in space overhead compared to existing KV separated systems.",
      "htmlFile": "2025/a3f9b8c7d1e2/index.html"
    },
    {
      "id": "a4c9b7d2e5f1",
      "title": "ChatBug: A Common Vulnerability of Aligned LLMs Induced by Chat Templates",
      "authors": [
        "Fengqing Jiang",
        "Zhangchen Xu",
        "Luyao Niu",
        "Bill Yuchen Lin",
        "Radha Poovendran"
      ],
      "year": 2025,
      "conference": "arXiv",
      "category": "自然语言处理",
      "keywords": [
        "大语言模型",
        "ChatBug",
        "安全对齐",
        "聊天模板",
        "漏洞攻击",
        "对抗训练",
        "指令调优"
      ],
      "abstract": "Large language models (LLMs) are expected to follow instructions from users and engage in conversations. Techniques to enhance LLMs’ instruction-following capabilities typically fine-tune them using data structured according to a predefined chat template. Although chat templates are shown to be effective in optimizing LLM performance, their impact on safety alignment of LLMs has been less understood, which is crucial for deploying LLMs safely at scale.\nIn this paper, we investigate how chat templates affect safety alignment of LLMs. We identify a common vulnerability, named ChatBug, that is introduced by chat templates. Our key insight to identify ChatBug is that the chat templates provide a rigid format that need to be followed by LLMs, but not by users. Hence, a malicious user may not necessarily follow the chat template when prompting LLMs. Instead, malicious users could leverage their knowledge of the chat template and accordingly craft their prompts to bypass safety alignments of LLMs. We study two attacks to exploit the ChatBug vulnerability. Additionally, we demonstrate that the success of multiple existing attacks can be attributed to the ChatBug vulnerability. We show that a malicious user can exploit the ChatBug vulnerability of eight state-of-the-art (SOTA) LLMs and effectively elicit unintended responses from these models. Moreover, we show that ChatBug can be exploited by existing jailbreak attacks to enhance their attack success rates. We investigate potential countermeasures to ChatBug. Our results show that while adversarial training effectively mitigates the ChatBug vulnerability, the victim model incurs significant performance degradation. These results highlight the trade-off between safety alignment and helpfulness. Developing new methods for instruction tuning to balance this trade-off is an open and critical direction for future research.",
      "htmlFile": "2025/a4c9b7d2e5f1/index.html"
    },
    {
      "id": "a7c3e9b12f84",
      "title": "Nested Learning: The Illusion of Deep Learning Architectures",
      "authors": [
        "Ali Behrouz",
        "Meisam Razaviyayn",
        "Peiling Zhong",
        "Vahab Mirrokni"
      ],
      "year": 2025,
      "conference": "NeurIPS",
      "category": "机器学习",
      "keywords": [
        "嵌套学习",
        "关联记忆",
        "上下文学习",
        "深度优化器",
        "自修改模型",
        "连续记忆系统",
        "Hope架构"
      ],
      "abstract": "Over the last decades, developing more powerful neural architectures and simultaneously designing optimization algorithms to effectively train them have been the core of research efforts to enhance the capability of machine learning models. Despite the recent progresses, particularly in developing Language Models (LMs), there are fundamental challenges and unanswered questions about how such models can _continually learn/memorize_, _self-improved_, _and find \"effective solutions_\". In this paper, we present a new learning paradigm, called Nested Learning (NL), that coherently represents a model with a set of nested, multi-level, and/or parallel optimization problems, each of which with its own \"_context flow_\". NL reveals that existing deep learning methods learns from data through _compressing_ their own context flow, and explain how _in-context learning_ emerges in large models. NL suggests a path (a new dimension to deep learning) to design more expressive learning algorithms with more \"_levels_\", resulting in higher-order in-context learning abilities. In addition to its neuroscientifically plausible and mathematically white-box nature, we advocate for its importance by presenting three core contributions: (1) Deep Optimizers: Based on NL, we show that well-known gradient-based optimizers (e.g., Adam, SGD with Momentum, etc.) are in fact associative memory modules that aim to compress the gradients with gradient descent. Building on this insight, we present a set of more expressive optimizers with deep memory and/or more powerful learning rules; (2) Self-Modifying Titans: Taking advantage of NL's insights on learning algorithms, we present a novel sequence model that learns how to modify itself by learning its own update algorithm; and (3) Continuum Memory System: We present a new formulation for memory system that generalizes the traditional viewpoint of \"long-term/short-term memory\". Combining our self-modifying sequence model with the continuum memory system, we present a learning module, called Hope, showing promising results in language modeling, continual learning, and long-context reasoning tasks.",
      "htmlFile": "2025/a7c3e9b12f84/index.html"
    },
    {
      "id": "a7c3e9b1f204",
      "title": "On Scalable Integrity Checking for Secure Cloud Disks",
      "authors": [
        "Quinn Burke",
        "Ryan Sheatsley",
        "Rachel King",
        "Owen Hines",
        "Michael Swift",
        "Patrick McDaniel"
      ],
      "year": 2025,
      "conference": "FAST",
      "category": "存储系统安全",
      "keywords": [
        "完整性验证",
        "Merkle哈希树",
        "云存储",
        "动态Merkle树",
        "性能优化",
        "安全磁盘",
        "工作负载感知"
      ],
      "abstract": "Merkle hash trees are the standard method to protect the integrity and freshness of stored data. However, hash trees introduce additional compute and I/O costs on the I/O critical path, and prior efforts have not fully characterized these costs. In this paper, we quantify performance overheads of storage-level hash trees in realistic settings. We then design an optimized tree structure called _Dynamic Merkle Trees (DMTs)_ based on an analysis of root causes of overheads. DMTs exploit patterns in workloads to deliver up to a 2.2× throughput and latency improvement over the state of the art. Our novel approach provides a promising new direction to achieve integrity guarantees in storage efficiently and at scale.",
      "htmlFile": "2025/a7c3e9b1f204/index.html"
    },
    {
      "id": "a7c3e9b1f42d",
      "title": "Oasis: An Out-of-core Approximate Graph System via All-Distances Sketches",
      "authors": [
        "Tsun-Yu Yang",
        "Yi Li",
        "Yizou Chen",
        "Bingzhe Li",
        "Ming-Chang Yang"
      ],
      "year": 2025,
      "conference": "FAST",
      "category": "图计算系统",
      "keywords": [
        "图计算",
        "近似计算",
        "All-Distances Sketch",
        "外存系统",
        "图分析",
        "内存优化",
        "存储I/O优化"
      ],
      "abstract": "The All-Distances Sketch (ADS) is a powerful and theoretically-sound sketching scheme that captures neighborhood information in graphs for approximate processing. It enables high-accuracy estimation of many useful applications with a guarantee of accuracy and can significantly accelerate the execution times by orders of magnitude. However, ADS requires a substantial amount of space that is multiple times larger than the graph data. More seriously, existing studies mainly focus on managing ADSs in memory, posing an increasing challenge for users who aim to leverage ADS for large-scale graph processing, particularly in light of the exponential growth of real-world graphs nowadays.\n\nTo this end, this paper introduces Oasis, an Out-of-core Approximate graph SYSTEM that brings the ADS technique into practical use by leveraging storage effectively. Specifically, Oasis offers a holistic framework that facilitates both ADS construction and estimation. For ADS construction, it allows users to adjust the memory usage based on the machine's available memory and enable an efficient construction process. For ADS estimation, Oasis provides a user-friendly interface to easily execute the estimators while mitigating the impact of slow storage I/O. Evaluation results show that Oasis provides a practical graph processing solution with exceptional execution time and low memory usage, at the cost of a slight decrease in accuracy.",
      "htmlFile": "2025/a7c3e9b1f42d/index.html"
    },
    {
      "id": "a7c3e9b2f1d8",
      "title": "The FastLanes File Format",
      "authors": [
        "Azim Afroozeh",
        "Peter Boncz"
      ],
      "year": 2025,
      "conference": "PVLDB",
      "category": "数据库系统",
      "keywords": [
        "文件格式",
        "轻量级压缩",
        "数据并行",
        "多列压缩",
        "向量化解码",
        "FastLanes",
        "Expression Encoding"
      ],
      "abstract": "This paper introduces a new open-source big data file format, called FastLanes. It is designed for modern data-parallel execution (SIMD or GPU), and evolves the features of previous data formats such as Parquet, which are the foundation of data lakes, and which increasingly are used in AI pipelines. It does so by avoiding generic compression methods (e.g. Snappy) in favor of lightweight encodings, that are fully data-parallel. To enhance compression ratio, it cascades encodings using a flexible _expression encoding_ mechanism. This mechanism also enables multi-column compression (MCC), enhancing compression by exploiting correlations between columns, a long-time weakness of columnar storage. We contribute a 2-phase algorithm to find encodings expressions during compression.\n\nFastLanes also innovates in its API, providing flexible support for _partial_ decompression, facilitating engines to execute queries on compressed data. FastLanes is designed for fine-grained access, at the level of small batches rather than rowgroups; in order to limit the decompression memory footprint to fit CPU and GPU caches.\n\nWe contribute an open-source implementation of FastLanes in portable (auto-vectorizing) C++. Our evaluation on a corpus of real-world data shows that FastLanes improves compression ratio over Parquet, while strongly accelerating decompression, making it a win-win over the state-of-the-art.",
      "htmlFile": "2025/a7c3e9b2f1d8/index.html"
    },
    {
      "id": "a7c3e9b8d4f2a",
      "title": "Selective On-Device Execution of Data-Dependent Read I/Os",
      "authors": [
        "Chanyoung Park",
        "Minu Chung",
        "Hyungon Moon"
      ],
      "year": 2025,
      "conference": "FAST",
      "category": "存储系统",
      "keywords": [
        "存储计算",
        "数据依赖读I/O",
        "设备内执行",
        "eBPF",
        "键值存储",
        "NVMe",
        "性能优化"
      ],
      "abstract": "Recent studies have demonstrated the benefits of employing on-device and in-kernel storage functions. On-device functions are primarily used to preprocess data within storage devices, effectively reducing the amount of I/O. In contrast, in-kernel functions are proposed to expedite sequences of data-dependent read I/O requests, particularly useful for applications traversing on-disk data structures. In this work, we investigate the unexplored potential of using on-device functions for data-dependent read I/O requests on read-only on-disk data structures. The results are promising: on-device I/O functions enable applications to issue I/O requests more rapidly and integrate seamlessly with in-kernel functions to efficiently manage high volumes of requests. We developed a prototype of this on-device function atop NVMeVirt, a state-of-the-art storage emulator. We demonstrate that on-device function enhances performance through experiments utilizing a simple B+-tree key-value store and WiredTiger, a widely used log-structured merge tree-based key-value store. Use of the on-device function improves the throughput of the B+-tree key-value store by up to 41%, and reduces WiredTiger's 99-percentile tail latency on YCSB C by up to 3.85%, compared to the host-only in-kernel storage function.",
      "htmlFile": "2025/a7c3e9b8d4f2a/index.html"
    },
    {
      "id": "a7c3f8e1b92d",
      "title": "GPHash: An Efficient Hash Index for GPU with Byte-Granularity Persistent Memory",
      "authors": [
        "Menglei Chen",
        "Yu Hua",
        "Zhangyu Chen",
        "Ming Zhang",
        "Gen Dong"
      ],
      "year": 2025,
      "conference": "FAST",
      "category": "存储系统",
      "keywords": [
        "GPU",
        "持久内存",
        "哈希索引",
        "并行计算",
        "缓存管理",
        "一致性保证",
        "GPM系统"
      ],
      "abstract": "GPU with persistent memory (GPM) enables GPU-powered applications to directly manage the data in persistent memory at the byte granularity. Hash indexes have been widely used to achieve efficient data management. However, conventional hash indexes become inefficient for GPM systems due to warp-agnostic execution manner, high-overhead consistency guarantee, and significant bandwidth gap between PM and GPU. In this paper, we propose GPHash, an efficient hash index for GPM systems with high performance and consistency guarantee. To fully exploit the parallelism of GPU, GPHash executes all index operations in a lock-free and warp-cooperative manner. Moreover, by using CAS primitive and slot states, GPHash ensures consistency guarantee with low overhead. To further bridge the bandwidth gap between PM and GPU, GPHash caches hot items in GPU memory while minimizing the overhead for cache management. Extensive evaluations on YCSB and real-world workloads show that GPHash outperforms state-of-the-art CPU-assisted data management approaches and GPM hash indexes by up to 27.62\\\\(\\\\times\\\\).",
      "htmlFile": "2025/a7c3f8e1b92d/index.html"
    },
    {
      "id": "a7f3e12b9c8d2025OpenOneR",
      "title": "OpenOneRec Technical Report: An Open Foundation Model and Benchmark to Accelerate Generative Recommendation",
      "authors": [
        "OneRec Team"
      ],
      "year": 2025,
      "conference": "arXiv",
      "category": "推荐系统",
      "keywords": [
        "生成式推荐",
        "基础模型",
        "基准评测",
        "指令跟随",
        "跨域迁移",
        "OpenOneRec",
        "RecIF-Bench"
      ],
      "abstract": "While the OneRec series has successfully unified the fragmented recommendation pipeline into an end-to-end generative framework, a significant gap remains between recommendation systems and general intelligence. Constrained by isolated data, existing models struggle to leverage the massive data scaling that drives the emergent capabilities of Large Language Models (LLMs). As a result, they operate as domain specialists—proficient in pattern matching but lacking world knowledge, reasoning capabilities, and instruction following. This limitation is further compounded by the lack of a holistic benchmark to evaluate such integrated capabilities. To address this, our contributions are: 1) RecIF-Bench & Open Data: We propose RecIF-Bench, a holistic benchmark covering 8 diverse tasks that thoroughly evaluate capabilities from fundamental prediction to complex reasoning. Concurrently, we release a massive training dataset comprising 96 million interactions from 160,000 users to facilitate reproducible research. 2) Framework & Scaling: To ensure full reproducibility, we open-source our comprehensive training pipeline, encompassing data processing, co-pretraining, and post-training. Leveraging this framework, we demonstrate that recommendation capabilities can scale predictably while mitigating catastrophic forgetting of general knowledge. 3) OneRec-Foundation: We release OneRec-Foundation (1.7B and 8B), a family of models establishing new state-of-the-art (SOTA) results across all tasks in RecIF-Bench. Furthermore, when transferred to the Amazon benchmark, our models surpass the strongest baselines with an average 26.8% improvement in Recall@10 across 10 diverse datasets (Figure 1). This work marks a step towards building truly intelligent recommender systems. Nonetheless, realizing this vision presents significant technical and theoretical challenges, highlighting the need for broader research engagement in this promising direction.",
      "htmlFile": "2025/a7f3e12b9c8d2025OpenOneR/index.html"
    },
    {
      "id": "a7f8c9e2d3b1",
      "title": "LLMs Encode How Difficult Problems Are",
      "authors": [
        "William Lugoloobi",
        "Chris Russell"
      ],
      "year": 2025,
      "conference": "arXiv",
      "category": "自然语言处理",
      "keywords": [
        "大语言模型",
        "问题难度",
        "线性探针",
        "强化学习",
        "数学推理",
        "人类判断",
        "GRPO"
      ],
      "abstract": "Large language models exhibit a puzzling inconsistency:they solve complex problems yet frequently fail on seemingly simpler ones.We investigate whether LLMs internally encode problem difficulty in a way that aligns with human judgment,and whether this representation tracks generalization during reinforcement learning post-training.We train linear probes across layers and token positions on 60models,evaluating on mathematical and coding subsets of Easy2HardBench.We find that humanlabeled difficulty is strongly linearly decodable (AMC:ρ ≈0.88)and exhibits clear modelsize scaling,whereas LLM-derived difficulty is substantially weaker and scales poorly.Steering along the difficulty direction reveals that pushing models toward \"easier\"representations reduces hallucination and improves accuracy.During GRPO training on Qwen2.5-Math-1.5B,the human-difficulty probe strengthens and positively correlates with test accuracy across training steps,while the LLM-difficulty probe degrades and negatively correlates with $\\mathrm{p e r f o r}$-mance.These results suggest that human annotations provide a stable difficulty signal that RL amplifies,while automated difficulty estimates derived from model performance become misaligned precisely as models improve.We release probe code and evaluation scripts to facilitate replication.",
      "htmlFile": "2025/a7f8c9e2d3b1/index.html"
    },
    {
      "id": "a8d6f8c4f2e3",
      "title": "Tiered Memory Management Beyond Hotness",
      "authors": [
        "Jinshu Liu",
        "Hamid Hadian",
        "Hanchen Xu",
        "Huaicheng Li"
      ],
      "year": 2025,
      "conference": "OSDI",
      "category": "操作系统与内存管理",
      "keywords": [
        "分层内存",
        "内存管理",
        "Amortized Offcore Latency (AOL)",
        "Memory-Level Parallelism (MLP)",
        "CXL内存",
        "页面迁移",
        "性能优化"
      ],
      "abstract": "Tiered memory systems often rely on access frequency (\"hotness\") to guide data placement. However, hot data is not always performance-critical, limiting the effectiveness of hotness-based policies. We introduce amortized offcore latency (AOL), a novel metric that precisely captures the true performance impact of memory accesses by accounting for memory access latency and memory-level parallelism (MLP). Leveraging AOL, we present two powerful tiering mechanisms: Soar, a profile-guided allocation policy that places objects based on their performance contribution, and Atro, a lightweight page migration regulation policy to eliminate unnecessary migrations. Soar and Atro outperform four state-of-the-art tiering designs across a diverse set of workloads by up to 12.4×, while underperforming in a few cases by no more than 3%.",
      "htmlFile": "2025/a8d6f8c4f2e3/index.html"
    },
    {
      "id": "a8f3b1c2d5e9_2025_HouseFuz",
      "title": "HouseFuzz: Service-Aware Grey-Box Fuzzing for Vulnerability Detection in Linux-Based Firmware",
      "authors": [
        "Haoyu Xiao",
        "Ziqi Wei",
        "Jiarun Dai",
        "Bowen Li",
        "Yuan Zhang",
        "Min Yang"
      ],
      "year": 2025,
      "conference": "IEEE S&P",
      "category": "系统安全",
      "keywords": [
        "固件模糊测试",
        "灰盒模糊测试",
        "服务感知",
        "Linux固件",
        "漏洞检测",
        "多进程框架",
        "服务协议"
      ],
      "abstract": "To date, grey-box fuzzing has become an essential technique to detect vulnerabilities implied in Linux-based firmware. However, existing fuzzing approaches commonly encounter three overlooked obstacles stemming from firmware service characteristics, which largely hinder the effectiveness and efficiency of vulnerability identification. Firstly, the multi-process nature of firmware services is oversimplified during both the emulation and the fuzzing procedures, limiting the scope of firmware testing. Furthermore, firmware services usually incorporate customized service protocols, which feature rich and stringent semantic constraints, causing unique challenges for input generation. To address these obstacles, this paper proposes a service-aware grey-box fuzzing tool HouseFuzz. During the firmware emulation, HouseFuzz carefully traverses the system initialization procedure for identifying those network-facing and daemon processes overlooked by existing approaches. After that, during the fuzzing procedure, HouseFuzz features a multi-process fuzzing framework, enabling the comprehensive inspection of firmware services activated via multiple processes. Furthermore, HouseFuzz leverages both offline and online firmware service analysis to capture the token-level semantic constraints of customized service protocols, based on which HouseFuzz can effectively generate high-quality test cases. In evaluation, compared to SoTA grey-box firmware fuzzing approaches, HouseFuzz identified 76% more network services, achieved 24.8% more code coverage, and detected 175% more 0-day vulnerabilities on the same firmware dataset.",
      "htmlFile": "2025/a8f3b1c2d5e9_2025_HouseFuz/index.html"
    },
    {
      "id": "ac38f7b2d4e620252025Memory",
      "title": "Memory in the Age of AI Agents: A Survey",
      "authors": [
        "Yuyang Hu",
        "Shichun Liu",
        "Yanwei Yue",
        "Guibin Zhang",
        "Boyang Liu",
        "Fangyi Zhu",
        "Jiahang Lin",
        "Honglin Guo",
        "Shihan Dou",
        "Zhiheng Xi",
        "Senjie Jin",
        "Jiejun Tan",
        "Yanbin Yin",
        "Jiongnan Liu",
        "Zeyu Zhang",
        "Zhongxiang Sun",
        "Yutao Zhu",
        "Hao Sun",
        "Boci Peng",
        "Zhenrong Cheng",
        "Xuanbo Fan",
        "Jiaxin Guo",
        "Xinlei Yu",
        "Zhenhong Zhou",
        "Zewen Hu",
        "Jiahao Huo",
        "Junhao Wang",
        "Yuwei Niu",
        "Yu Wang",
        "Zhenfei Yin",
        "Xiaobin Hu",
        "Yue Liao",
        "Qiankun Li",
        "Kun Wang",
        "Wangchunshu Zhou",
        "Yixin Liu",
        "Dawei Cheng",
        "Qi Zhang",
        "Tao Gui",
        "Shirui Pan",
        "Yan Zhang",
        "Philip Torr",
        "Zhicheng Dou",
        "Ji-Rong Wen",
        "Xuanjing Huang",
        "Yu-Gang Jiang",
        "Shuicheng Yan"
      ],
      "year": 2025,
      "conference": "arXiv",
      "category": "自然语言处理",
      "keywords": [
        "AI Agent",
        "Memory",
        "LLM Agent",
        "Survey",
        "Memory Forms Functions and Dynamics"
      ],
      "abstract": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. It underpins long-horizon reasoning, continual adaptation, and effective interaction with complex environments. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, assumptions, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity and dynamics of contemporary agent memory systems. This survey aims to provide an up-to-date and comprehensive landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we move beyond coarse temporal categorizations and propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time as agents interact with their environments. To support empirical research and practical development, we compile a comprehensive summary of representative benchmarks and open source memory frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including automation-oriented memory design, the deep integration of reinforcement learning with memory systems, multimodal memory, shared memory for multi-agent systems, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.",
      "htmlFile": "2025/ac38f7b2d4e620252025Memory/index.html"
    },
    {
      "id": "b0d9c8a7e1f2",
      "title": "LLMs Get Lost In Multi-Turn Conversation",
      "authors": [
        "Philippe Laban",
        "Hiroaki Hayashi",
        "Yingbo Zhou",
        "Jennifer Neville"
      ],
      "year": 2025,
      "conference": "arXiv",
      "category": "自然语言处理",
      "keywords": [
        "大语言模型",
        "多轮对话",
        "underspecification",
        "可靠性",
        "评估",
        "对话系统",
        "性能退化"
      ],
      "abstract": "Large Language Models (LLMs) are conversational interfaces. As such, LLMs have the potential to assist their users not only when they can fully specify the task at hand, but also to help them define, explore, and refine what they need through multi-turn conversational exchange. Although analysis of LLM conversation logs has confirmed that underspecification occurs frequently in user instructions, LLM evaluation has predominantly focused on the single-turn, fully-specified instruction setting. In this work, we perform large-scale simulation experiments to compare LLM performance in single- and multi-turn settings. Our experiments confirm that all the top open- and closed-weight LLMs we test exhibit significantly lower performance in multi-turn conversations than single-turn, with an average drop of 39% across six generation tasks. Analysis of 200,000+ simulated conversations decomposes the performance degradation into two components: a minor loss in aptitude and a significant increase in unreliability. We find that LLMs often make assumptions in early turns and prematurely attempt to generate final solutions, on which they overly rely. In simpler terms, we discover that when LLMs take a wrong turn in a conversation, they get lost and do not recover.",
      "htmlFile": "2025/b0d9c8a7e1f2/index.html"
    },
    {
      "id": "b2d3d9d6e9b9",
      "title": "Rethinking The Compaction Policies in LSM-trees",
      "authors": [
        "Hengrui Wang",
        "Jiansheng Qiu",
        "Fangzhou Yuan",
        "Huanchen Zhang"
      ],
      "year": 2025,
      "conference": "SIGMOD",
      "category": "数据库系统",
      "keywords": [
        "LSM-tree",
        "Compaction Policy",
        "Query Throughput",
        "Dynamic Programming",
        "EcoTune",
        "Key-Value Store",
        "SSD"
      ],
      "abstract": "Log-structured merge-trees (LSM-trees) are widely used to construct key-value stores. They periodically compact overlapping sorted runs to reduce the read amplification. Prior research on compaction policies has focused on the trade-off between write amplification (WA) and read amplification (RA). In this paper, we propose to treat the compaction operation in LSM-trees as a computational and I/O-bandwidth investment for improving the system’s future query throughput, and thus rethink the compaction policy designs. A typical LSM-tree application handles a steady but moderate write stream and prioritizes resources for top-level flushes of small sorted runs to avoid data loss due to write stalls. The goal of the compaction policy, therefore, is to maintain an optimal number of sorted runs to maximize average query throughput. Because compaction and read operations compete for the CPU and I/O resources from the same pool, we must perform a joint optimization to determine the appropriate timing and aggressiveness of the compaction. We introduce a three-level model of an LSM-tree and propose EcoTune, an algorithm based on dynamic programming to find the optimal compaction policy according to workload characterizations. Our evaluation on RocksDB shows that EcoTune improves the average query throughput by 1.5x to 3x over the leveling policy and by up to 1.8x over the lazy-leveling policy on workloads with range/point query ratios.",
      "htmlFile": "2025/b2d3d9d6e9b9/index.html"
    },
    {
      "id": "b3a5b9a8d3f9",
      "title": "eBPF Misbehavior Detection: Fuzzing with a Specification-Based Oracle",
      "authors": [
        "Tao Lyu",
        "Kumar Kartikeya Dwivedi",
        "Thomas Bourgeat",
        "Mathias Payer",
        "Meng Xu",
        "Sanidhya Kashyap"
      ],
      "year": 2025,
      "conference": "SOSP",
      "category": "操作系统与系统安全",
      "keywords": [
        "eBPF",
        "Fuzzing",
        "Specification-based Oracle",
        "Linux Kernel",
        "Verifier Bugs",
        "SMT Solver",
        "安全验证",
        "模糊测试"
      ],
      "abstract": "Bugs in the Linux eBPF verifier may cause it to mistakenly accept unsafe eBPF programs or reject safe ones, causing either security or usability issues. While prior works on fuzzing the eBPF verifier have been effective, their bug oracles only hint at the existence of bugs indirectly (e.g., when a memory error occurs in downstream execution) instead of showing the root cause, confining them to uncover a narrow range of security bugs only with no detection of usability issues.\nIn this paper, we propose SpecCheck, a specification-based oracle integrated with our fuzzer Vertras, to detect a wide range of bugs in the eBPF verifier. SpecCheck encodes eBPF instruction semantics and safety properties as a specification and turns the claim of whether a concrete eBPF program is safe into checking the satisfiability of the corresponding safety constraints, which can be reasoned automatically without abstraction. The output from the oracle will be cross-checked with the eBPF verifier for any discrepancies. Using SpecCheck, Vertras uncovered 13 bugs in the Linux eBPF verifier, including severe bugs that can cause privilege escalation or information leakage, as well as bugs that cause frustration in even experienced kernel developers.",
      "htmlFile": "2025/b3a5b9a8d3f9/index.html"
    },
    {
      "id": "b3c5d7f9a1e22025tilelang",
      "title": "TileLang: A Composable Tiled Programming Model for AI Systems",
      "authors": [
        "Lei Wang",
        "Yu Cheng",
        "Yinling Shi",
        "Zhengiu Tang",
        "Zhiwen Mo",
        "Wenhao Xie",
        "Lingxiao Ma",
        "Yuqing Xia",
        "Jilong Xue",
        "Fan Yang",
        "Zhi Yang"
      ],
      "year": 2025,
      "conference": "arXiv",
      "category": "高性能计算与编译器",
      "keywords": [
        "TileLang",
        "编程模型",
        "AI系统",
        "高性能内核",
        "编译器",
        "调度空间",
        "数据流"
      ],
      "abstract": "Modern AI workloads rely heavily on optimized computing kernels for both training and inference. These AI kernels follow well-defined data-flow patterns, such as moving tiles between DRAM and SRAM and performing a sequence of computations on those tiles. However, writing high-performance kernels remains complex despite the clarity of these patterns. Achieving peak performance requires careful, hardware-centric optimizations to fully leverage modern accelerators. While domain-specific compilers attempt to reduce the burden of writing high-performance kernels, they often struggle with usability and expressiveness gaps.\n\nIn this paper, we present TileLang, a generalized tiled programming model for more efficient AI Kernel programming. **TileLang decouples scheduling space (thread binding, layout, tensorize and pipeline) from dataflow, and encapsulated them as a set of customization annotations and primitives.** This approach allows users to focus on the kernel's data-flow itself, while leaving most other optimizations to compilers. We conduct comprehensive experiments on commonly-used devices, across numerous experiments, our evaluation shows that TileLang can achieve state-of-the-art performance in key kernels, demonstrating that its unified block-and-thread paradigm and transparent scheduling capabilities deliver both the power and flexibility demanded by modern AI system development.",
      "htmlFile": "2025/b3c5d7f9a1e22025tilelang/index.html"
    },
    {
      "id": "b3e4a6f8c2d1",
      "title": "JENGA: Effective Memory Management for Serving LLM with Heterogeneity",
      "authors": [
        "Chen Zhang",
        "Kuntai Du",
        "Shu Liu",
        "Woosuk Kwon",
        "Xiangxi Mo",
        "Yufeng Wang",
        "Xiaoxuan Liu",
        "Kaichao You",
        "Zhuohan Li",
        "Mingsheng Long",
        "Jidong Zhai",
        "Joseph Gonzalez",
        "Ion Stoica"
      ],
      "year": 2025,
      "conference": "SOSP",
      "category": "操作系统",
      "keywords": [
        "LLM Serving",
        "Memory Management",
        "KV Cache",
        "PagedAttention",
        "异构模型",
        "前缀缓存",
        "内存碎片"
      ],
      "abstract": "Large language models are widely used but expensive to run. To reduce costs, it is crucial to maximize request batch size through efficient GPU memory management. Existing approaches, such as PagedAttention, struggle with modern LLMs because of the growing heterogeneity in the sizes of models’ internal embeddings and attention mechanisms.\n\nIn this paper, we present Jenga, a memory allocation framework for these heterogeneous LLMs. Jenga tackles two key challenges: (1) memory fragmentation caused by embeddings of different sizes, and (2) unpredictable memory usage from varying attention mechanisms across layers. Jenga employs an attention-property-aware allocator, leveraging the least common multiple (LCM) of embedding sizes to optimize memory usage and performing cache eviction based on attention patterns to enhance memory reuse. We implement Jenga in vLLM, and evaluate it with diverse LLMs, datasets, and GPUs. Evaluations show that Jenga improves GPU memory utilization by up to 83% and serving throughput by up to 2.16x (1.46x on average).",
      "htmlFile": "2025/b3e4a6f8c2d1/index.html"
    },
    {
      "id": "b4a1c8b4a1c8",
      "title": "How to Copy Memory? Coordinated Asynchronous Copy as a First-Class OS Service",
      "authors": [
        "Jingkai He",
        "Yunpeng Dong",
        "Dong Du",
        "Mo Zou",
        "Zhitai Yu",
        "Yuxin Ren",
        "Ning Jia",
        "Yubin Xia",
        "Haibo Chen"
      ],
      "year": 2025,
      "conference": "SOSP",
      "category": "操作系统",
      "keywords": [
        "内存复制",
        "异步复制",
        "操作系统服务",
        "操作系统",
        "内存管理",
        "性能优化",
        "硬件加速"
      ],
      "abstract": "In modern systems, memory copy remains a critical performance bottleneck across various scenarios, playing a pervasive role in system-wide execution such as syscalls, IPC, and user-mode applications. Numerous efforts have aimed at optimizing copy performance, including zero-copy with page remapping and hardware-accelerated copy. However, they typically target specific use cases, such as Linux zero-copy send() for messages of ≥10KB. This paper argues for copy as a first-class OS service, offering three key benefits: (1) with the asynchronous copy abstraction provided by the service, applications can overlap their execution with copy; (2) the service can effectively utilize hardware capabilities to enhance copy performance; (3) the service's global view of copies further enables holistic optimization. To this end, we introduce Copier, a new OS service of coordinated asynchronous copy, to serve both user-mode applications and OS services. We build Copier-Linux to demonstrate Copier's ability to improve performance for diverse use cases, including Redis, Protobuf, network stack, proxy, etc. Evaluations show that Copier achieves up to a 1.8× speedup for real-world applications like Redis and a 1.6× improvement over zIO, the state-of-the-art in optimizing copy efficiency. To further facilitate adoption, we develop a toolchain to ease the use of Copier. We also integrate Copier into a commercial smartphone OS (HarmonyOS 5.0), achieving promising results.",
      "htmlFile": "2025/b4a1c8b4a1c8/index.html"
    },
    {
      "id": "b5c8a1d4f7e2",
      "title": "Prove It to the Kernel: Precise Extension Analysis via Proof-Guided Abstraction Refinement",
      "authors": [
        "Sun, Hao",
        "Su, Zhendong"
      ],
      "year": 2025,
      "conference": "SOSP",
      "category": "操作系统与系统软件",
      "keywords": [
        "eBPF验证",
        "抽象解释",
        "证明引导的抽象精化",
        "内核扩展",
        "静态分析",
        "形式化证明",
        "SMT求解"
      ],
      "abstract": "Modern OS kernels, such as Linux, employ the eBPF subsystem to enable user space to extend kernel functionality. To ensure safety, an in-kernel verifier statically analyzes these extensions; however, its imprecise analysis frequently results in the erroneous rejection of safe extensions, exposing a critical tension between the precision and computational complexity of the verifier that limits kernel extensibility.\n\nWe propose a proof-guided abstraction refinement technique that significantly enhances the verifier's precision while preserving low kernel space complexity. Rather than incorporating sophisticated analysis (e.g., via new abstract domains) directly into the verifier, our key insight is to decouple the complex reasoning to user space while bridging the gap through formal proofs. Upon encountering uncertainties, the verifier initiates an abstraction refinement procedure rather than rejecting the extension. As the refinement involves nontrivial reasoning, the verifier simply delineates the task and delegates it to user space. A formal proof is produced externally, which the verifier subsequently checks in linear time before adopting the refined abstraction. Consequently, our approach achieves high precision via user space reasoning while confining kernel space operations to an efficient proof check. Evaluation results show that our technique enables the verifier to accept 403 out of 512 real-world eBPF programs that were previously rejected erroneously, paving the way for more reliable and flexible kernel extensions.",
      "htmlFile": "2025/b5c8a1d4f7e2/index.html"
    },
    {
      "id": "b5c8a3d9f1e2",
      "title": "FlacIO: Flat and Collective I/O for Container Image Service",
      "authors": [
        "Yubo Liu",
        "Hongbo Li",
        "Mingrui Liu",
        "Rui Jing",
        "Jian Guo",
        "Bo Zhang",
        "Hanjun Guo",
        "Yuxin Ren",
        "Ning Jia"
      ],
      "year": 2025,
      "conference": "FAST",
      "category": "存储系统",
      "keywords": [
        "容器镜像服务",
        "冷启动加速",
        "运行时镜像",
        "运行时页缓存",
        "I/O优化",
        "扁平化I/O",
        "聚合I/O"
      ],
      "abstract": "This paper examines the I/O bottlenecks in the container image service. With a comprehensive analysis of existing solutions, we reveal that they suffer from high I/O amplification and excessive network traffic. Furthermore, we identify that the root cause of these problems lies in the storage-oriented and global-oriented container image abstraction. This work proposes a memory-oriented and service-oriented image abstraction, called runtime image, which represents the memory state of the root file system of the container service. The runtime image enables efficient network transfer and fast root file system construction. We design and implement FlacIO, an I/O accelerator based on the runtime image for container image service. FlacIO introduces an efficient runtime image structure that works in conjunction with a runtime page cache on a host node to achieve efficient image service. Our evaluation shows that FlacIO reduces the container cold startup latency by up to 23 and 4.6 times compared to existing full image and lazy loading solutions, respectively. In real-world applications, FlacIO achieves up to 2.25 and 1.7 times performance speedup over other systems in the object storage and machine learning training scenarios, respectively.",
      "htmlFile": "2025/b5c8a3d9f1e2/index.html"
    },
    {
      "id": "b5c9a5d6d6a1",
      "title": "AlphaEvolve: A coding agent for scientific and algorithmic discovery",
      "authors": [
        "Alexander Novikov",
        "Ngan Vu",
        "Marvin Eisenberger",
        "Emilien Dupont",
        "Po-Sen Huang",
        "Adam Zsolt Wagner",
        "Sergey Shirobokov",
        "Borislav Kozlovskii",
        "Francisco J. R. Ruiz",
        "Abbas Mehrabian",
        "M. Pawan Kumar",
        "Abigail See",
        "Swarat Chaudhuri",
        "George Holland",
        "Alex Davies",
        "Sebastian Nowozin",
        "Pushmeet Kohli",
        "Matej Balog"
      ],
      "year": 2025,
      "conference": "arXiv",
      "category": "人工智能",
      "keywords": [
        "代码智能体",
        "演化算法",
        "大语言模型",
        "算法发现",
        "科学发现",
        "矩阵乘法",
        "自动评估"
      ],
      "abstract": "In this white paper, we present _AlphaEvolve_, an evolutionary coding agent that substantially enhances capabilities of state-of-the-art LLMs on highly challenging tasks such as tackling open scientific problems or optimizing critical pieces of computational infrastructure. _AlphaEvolve_ orchestrates an autonomous pipeline of LLMs, whose task is to improve an algorithm by making direct changes to the code. Using an evolutionary approach, continuously receiving feedback from one or more evaluators, _AlphaEvolve_ iteratively improves the algorithm, potentially leading to new scientific and practical discoveries. We demonstrate the broad applicability of this approach by applying it to a number of important computational problems. When applied to optimizing critical components of large-scale computational stacks at Google, _AlphaEvolve_ developed a more efficient scheduling algorithm for data centers, found a functionally equivalent simplification in the circuit design of hardware accelerators, and accelerated the training of the LLM underpinning _AlphaEvolve_ itself. Furthermore, _AlphaEvolve_ discovered novel, provably correct algorithms that surpass state-of-the-art solutions on a spectrum of problems in mathematics and computer science, significantly expanding the scope of prior automated discovery methods (Romera-Paredes et al., 2023). Notably, _AlphaEvolve_ developed a search algorithm that found a procedure to multiply two 4 × 4 complex-valued matrices using 48 scalar multiplications; offering the first improvement, after 56 years, over Strassen's algorithm in this setting. We believe _AlphaEvolve_ and coding agents like it can have a significant impact in improving solutions of problems across many areas of science and computation.",
      "htmlFile": "2025/b5c9a5d6d6a1/index.html"
    },
    {
      "id": "b6d2a9e2b8b1",
      "title": "Principles and Methodologies for Serial Performance Optimization",
      "authors": [
        "Sujin Park",
        "Mingyu Guan",
        "Xiang Cheng",
        "Taesoo Kim"
      ],
      "year": 2025,
      "conference": "OSDI",
      "category": "计算机系统",
      "keywords": [
        "性能优化",
        "串行执行",
        "系统优化",
        "方法论",
        "SysGPT",
        "缓存",
        "批处理"
      ],
      "abstract": "Throughout the history of computer science, optimizing existing systems to achieve higher performance has been a longstanding aspiration. While the primary emphasis of this endeavor lies in reducing latency and increasing throughput, these two are closely intertwined, and answering the *how* question has remained a challenge, often relying on intuition and experience.\n\nThis paper introduces a systematic approach to optimizing sequential tasks, which are fundamental for overall performance. We define three principles—task removal, replacement, and reordering—and distill them into eight actionable methodologies: batching, caching, precomputing, deferring, relaxation, contextualization, hardware specialization, and layering. Our review of OSDI and SOSP papers over the past decade shows that these techniques, when taken together, comprehensively account for the observed sequential optimization strategies.\n\nTo illustrate the framework’s practical value, we present two case studies: one on file and storage systems, and another analyzing kernel synchronization to uncover missed optimization opportunities. Furthermore, we introduce SysGPT, a fine-tuned GPT model trained on curated literature analysis, which offer context-aware performance suggestions. SysGPT’s outputs are more specific and feasible than GPT-4’s, aligning with core strategies from recent research without direct exposure, demonstrating its utility as an optimization assistant.",
      "htmlFile": "2025/b6d2a9e2b8b1/index.html"
    },
    {
      "id": "bf5a3d8e1c232025rvismith",
      "title": "RVISmith: Fuzzing Compilers for RVV Intrinsics",
      "authors": [
        "Yibo He",
        "Cunjian Huang",
        "Xianmiao Qu",
        "Hongdeng Chen",
        "Wei Yang",
        "Tao Xie*"
      ],
      "year": 2025,
      "conference": "CCS",
      "category": "编译器测试",
      "keywords": [
        "编译器测试",
        "Fuzzing",
        "RISC-V向量扩展",
        "SIMD Intrinsics",
        "RVV Intrinsics",
        "模糊测试"
      ],
      "abstract": "Modern processors are equipped with single instruction multiple data (SIMD) instructions for fine-grained data parallelism. Compiler auto-vectorization techniques that target SIMD instructions face performance limitations due to insufficient information available at compile time, requiring programmers to manually manipulate SIMD instructions. SIMD intrinsics, a type of built-in function provided by modern compilers, enable programmers to manipulate SIMD instructions within high-level programming languages. Bugs in compilers for SIMD intrinsics can introduce potential threats to software security, producing unintended calculation results, data loss, program crashes, etc.\n\nTo detect bugs in compilers for SIMD intrinsics, we propose RVISmith, a randomized fuzzer that generates well-defined C programs that include various invocation sequences of RVV (RISC-V Vector Extension) intrinsics. We design RVISmith to achieve the following objectives: (i) achieving high intrinsic coverage, (ii) improving sequence variety, and (iii) without known undefined behaviors. We implement RVISmith based on the ratified RVV intrinsic specification and evaluate our approach with three modern compilers: GCC, LLVM, and XuanTie. Experimental results show that RVISmith achieves 11.5 times higher intrinsic coverage than the state-of-the-art fuzzer for RVV intrinsics. By differential testing that compares results across different compilers, optimizations, and equivalent programs, we detect and report 13 previously unknown bugs of the three compilers under test to date. Of these bugs, 10 are confirmed and another 3 are fixed by the compiler developers.",
      "htmlFile": "2025/bf5a3d8e1c232025rvismith/index.html"
    },
    {
      "id": "c0b7f8b8c1b4",
      "title": "Memory OS of AI Agent",
      "authors": [
        "Jiazheng Kang",
        "Mingming Ji",
        "Zhe Zhao",
        "Ting Bai"
      ],
      "year": 2025,
      "conference": "arXiv",
      "category": "自然语言处理",
      "keywords": [
        "AI Agent",
        "Memory Management",
        "Long-term Memory",
        "Hierarchical Storage",
        "MemoryOS",
        "LLM",
        "Personalization"
      ],
      "abstract": "Large Language Models (LLMs) face a crucial challenge from fixed context windows and inadequate memory management, leading to a severe shortage of long-term memory capabilities and limited personalization in the interactive experience with AI agents. To overcome this challenge, we innovatively propose a **Memory Operating System**, i.e., **MemoryOS**, to achieve comprehensive and efficient memory management for AI agents. Inspired by the memory management principles in operating systems, MemoryOS designs a hierarchical storage architecture and consists of four key modules: Memory Storage, Updating, Retrieval, and Generation. Specifically, the architecture comprises three levels of storage units: short-term memory, mid-term memory, and long-term personal memory. Key operations within MemoryOS include dynamic updates between storage units: short-term to mid-term updates follow a dialogue-chain-based FIFO principle, while mid-term to long-term updates use a segmented page organization strategy. Our pioneering MemoryOS enables hierarchical memory integration and dynamic updating. Extensive experiments on the LoCoMo benchmark show an average improvement of 49.11% on F1 and 46.18% on BLEU-1 over the baselines on GPT-4o-mini, showing contextual coherence and personalized memory retention in long conversations. The implementation code is open-sourced at https://github.com/BAI-LAB/MemoryOS.",
      "htmlFile": "2025/c0b7f8b8c1b4/index.html"
    },
    {
      "id": "c2c5f4d5e6f7",
      "title": "A Survey of Vibe Coding with Large Language Models",
      "authors": [
        "Yuyao Ge",
        "Lingrui Mei",
        "Zenghao Duan",
        "Tianhao Li",
        "Yujia Zheng",
        "Yiwei Wang",
        "Lexin Wang",
        "Jiayu Yao",
        "Tianyu Liu",
        "Yujun Cai",
        "Baolong Bi",
        "Fangda Guo",
        "Jiafeng Guo",
        "Shenghua Liu",
        "Xueqi Cheng"
      ],
      "year": 2025,
      "conference": "arXiv",
      "category": "软件工程",
      "keywords": [
        "Vibe Coding",
        "Coding Agent",
        "Large Language Models",
        "AI辅助开发",
        "软件工程",
        "LLM",
        "智能体系统"
      ],
      "abstract": "The advancement of large language models (LLMs) has catalyzed a paradigm shift from code generation assistance to autonomous coding agents, enabling a novel development methodology termed \"Vibe Coding\" where developers validate AI-generated implementations through outcome observation rather than line-by-line code comprehension. Despite its transformative potential, the effectiveness of this emergent paradigm remains under-explored, with empirical evidence revealing unexpected productivity losses and fundamental challenges in human-AI collaboration. To address this gap, this survey provides the first comprehensive and systematic review of Vibe Coding with large language models, establishing both theoretical foundations and practical frameworks for this transformative development approach. Drawing from systematic analysis of over 1000 research papers, we survey the entire vibe coding ecosystem, examining critical infrastructure components including LLMs for coding, LLM-based coding agent, development environment of coding agent, and feedback mechanisms. We first introduce Vibe Coding as a formal discipline by formalizing it through a Constrained Markov Decision Process that captures the dynamic triadic relationship among human developers, software projects, and coding agents. Building upon this theoretical foundation, we then synthesize existing practices into five distinct development models: Unconstrained Automation, Iterative Conversational Collaboration, Planning-Driven, Test-Driven, and Context-Enhanced Models, thus providing the first comprehensive taxonomy in this domain. Critically, our analysis reveals that successful Vibe Coding depends not merely on agent capabilities but on systematic context engineering, well-established development environments, and human-agent collaborative development models. Based on these findings, we identify key challenges spanning technical infrastructure optimization, security mechanisms, and human-centered design considerations. Ultimately, this survey serves as both a conceptual foundation for AI-augmented software engineering and a technical roadmap for researchers and practitioners navigating this rapidly evolving field.",
      "htmlFile": "2025/c2c5f4d5e6f7/index.html"
    },
    {
      "id": "c5b0a8d4b8b4",
      "title": "CORTENMM: Efficient Memory Management with Strong Correctness Guarantees",
      "authors": [
        "Junyang Zhang",
        "Xiangcan Xu",
        "Yonghao Zou",
        "Zhe Tang",
        "Xinyi Wan",
        "Kang Hu",
        "Siyuan Wang",
        "Wenbo Xu",
        "Di Wang",
        "Hao Chen",
        "Lin Huang",
        "Shoumeng Yan",
        "Yuval Tamir",
        "Yingwei Luo",
        "Xiaolin Wang",
        "Huashan Yu",
        "Zhenlin Wang",
        "Hongliang Tian",
        "Diyu Zhou"
      ],
      "year": 2025,
      "conference": "SOSP",
      "category": "操作系统",
      "keywords": [
        "Memory Management",
        "Virtual Memory",
        "Concurrency",
        "Formal Verification",
        "Scalability"
      ],
      "abstract": "Modern memory management systems suffer from poor performance and subtle concurrency bugs, slowing down applications while introducing security vulnerabilities. We observe that both issues stem from the conventional design of memory management systems with two levels of abstraction: a software-level abstraction (e.g., VMA trees in Linux) and a hardware-level abstraction (typically, page tables). This design increases portability but requires correctly and efficiently synchronizing two drastically different and complex data structures, which is generally challenging.\nWe present CORTENMM, a memory management system with a clean-slate design to achieve both high performance and synchronization correctness. Our key insight is that most OSes no longer need the software-level abstraction, since mainstream ISAs use nearly identical hardware MMU formats. Therefore, departing from prior designs, CORTENMM eliminates the software-level abstraction to achieve sweeping simplicity. Exploiting this simplicity, CORTENMM proposes a transactional interface with scalable locking protocols to program the MMU, achieving high performance by avoiding the extra contention in the software-level abstraction. The one-level design further enables us to formally verify the correctness of concurrent code operating on the MMU (correctness of basic operations and locking protocols), thereby offering strong correctness guarantees. Our evaluation shows that the formally verified CORTENMM outperforms Linux by 1.2\\(\\times\\) to 26\\(\\times\\) on real-world applications.",
      "htmlFile": "2025/c5b0a8d4b8b4/index.html"
    },
    {
      "id": "d3a2e7b4f5c1",
      "title": "DeepAgent: A General Reasoning Agent with Scalable Toolsets",
      "authors": [
        "Xiaoxi Li",
        "Wenxiang Jiao",
        "Jiarui Jin",
        "Guanting Dong",
        "Jiajie Jin",
        "Yinuo Wang",
        "Hao Wang",
        "Yutao Zhu",
        "Ji-Rong Wen",
        "Yuan Lu",
        "Zhicheng Dou"
      ],
      "year": 2025,
      "conference": "arXiv",
      "category": "人工智能",
      "keywords": [
        "推理智能体",
        "工具使用",
        "记忆机制",
        "强化学习",
        "大规模工具集",
        "自主思考",
        "ToolPO"
      ],
      "abstract": "Large reasoning models have demonstrated strong problem-solving abilities, yet real-world tasks often require external tools and long-horizon interactions. Existing agent frameworks typically follow predefined workflows, which limit autonomous and global task completion. In this paper, we introduce **DeepAgent**, an end-to-end deep reasoning agent that performs autonomous thinking, tool discovery, and action execution within a single, coherent reasoning process. To address the challenges of long-horizon interactions, particularly the context length explosion from multiple tool calls and the accumulation of interaction history, we introduce an autonomous memory folding mechanism that compresses past interactions into structured episodic, working, and tool memories, reducing error accumulation while preserving critical information. To teach general-purpose tool use efficiently and stably, we develop an end-to-end reinforcement learning strategy, namely ToolPO, that leverages LLM-simulated APIs and applies tool-call advantage attribution to assign fine-grained credit to the tool invocation tokens. Extensive experiments on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank, TMDB, Spotify, Tool-Hop) and downstream applications (ALFWorld, WebShop, GAIA, HLE), demonstrate that DeepAgent consistently outperforms baselines across both labeled-tool and open-set tool retrieval scenarios. This work takes a step toward more general and capable agents for real-world applications. The code and demo are available at https://github.com/RUC-NLPIR/DeepAgent.",
      "htmlFile": "2025/d3a2e7b4f5c1/index.html"
    },
    {
      "id": "d3a8f2b1c5e72025JustRL",
      "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe",
      "authors": [
        "Bingxiang He",
        "Zekai Qu",
        "Zeyuan Liu",
        "Yinghao Chen",
        "Yuxin Zuo",
        "Cheng Qian",
        "Kaiyan Zhang",
        "Weize Chen",
        "Chaojun Xiao",
        "Ganqu Cui",
        "Ning Ding",
        "Zhiyuan Liu"
      ],
      "year": 2025,
      "conference": "JMLR",
      "category": "强化学习",
      "keywords": [
        "JustRL",
        "简单强化学习",
        "大语言模型",
        "数学推理",
        "单阶段训练"
      ],
      "abstract": "Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: **Is this complexity necessary?** We present **JustRL**, a minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9% and 64.3% average accuracy across nine mathematical benchmarks) while using 2× less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding \"standard tricks\" like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline. We release our models and code to establish a simple, validated baseline for the community.",
      "htmlFile": "2025/d3a8f2b1c5e72025JustRL/index.html"
    },
    {
      "id": "d6c7a1b3e9f2",
      "title": "DeepSeek-OCR: Contexts Optical Compression",
      "authors": [
        "Haoran Wei",
        "Yaofeng Sun",
        "Yukun Li"
      ],
      "year": 2025,
      "conference": "arXiv",
      "category": "计算机视觉",
      "keywords": [
        "OCR",
        "视觉语言模型",
        "上下文压缩",
        "DeepEncoder",
        "多模态学习"
      ],
      "abstract": "We present DeepSeek-OCR as an initial investigation into the feasibility of compressing long contexts via optical 2D mapping. DeepSeek-OCR consists of two components: DeepEncoder and DeepSeek3B-MoE-A570M as the decoder. Specifically, DeepEncoder serves as the core engine, designed to maintain low activations under high-resolution input while achieving high compression ratios to ensure an optimal and manageable number of vision tokens. Experiments show that when the number of text tokens is within 10 times that of vision tokens(i.e., a compression ratio< 10×), the model can achieve decoding(OCR) precision of 97%. Even at a compression ratio of 20×, the OCR accuracy still remains at about 60%. This shows considerable promise for research areas such as historical long-context compression and memory forgetting mechanisms in LLMs. Beyond this, DeepSeek-OCR also demonstrates high practical value. On OmniDocBench, it surpasses GOT-OCR2.0(256 tokens/page) using only 100 vision tokens, and outperforms MinerU2.0(6000+ tokens per page on average) while utilizing fewer than 800 vision tokens. In production, DeepSeek-OCR can generate training data for LLMs/VLMs at a scale of 200k+ pages per day(a single A100-40G). Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR.",
      "htmlFile": "2025/d6c7a1b3e9f2/index.html"
    },
    {
      "id": "d6d3c8e8f2d2",
      "title": "Mantle: Efficient Hierarchical Metadata Management for Cloud Object Storage Services",
      "authors": [
        "Li, Jiahao",
        "Cao, Biao",
        "Jian, Jielong",
        "Li, Cheng",
        "Han, Sen",
        "Wang, Yiduo",
        "Wu, Yufei",
        "Chen, Kang",
        "Yin, Zhihui",
        "Chen, Qiushi",
        "Xiong, Jiwei",
        "Zhao, Jie",
        "Liu, Fengyuan",
        "Xing, Yan",
        "Duan, Liguo",
        "Yu, Miao",
        "Zheng, Ran",
        "Wu, Feng",
        "Meng, Xianjun"
      ],
      "year": 2025,
      "conference": "SOSP",
      "category": "分布式系统",
      "keywords": [
        "元数据管理",
        "云对象存储",
        "层次结构",
        "路径解析",
        "目录更新",
        "可扩展性",
        "性能优化"
      ],
      "abstract": "Cloud Object Storage Services (COSSs) are the primary storage backend in the cloud, supporting large-scale analytics and ML workloads that frequently access deep object paths and update metadata concurrently. However, current COSS architectures incur costly multi-round lookups and high directory contention, delaying job execution. Prior optimizations, largely designed for distributed file systems (with least adoption in clouds), do not apply due to COSS-specific constraints like stateless proxies and limited APIs. Mantle is a new COSS metadata service for modern cloud workloads. It adopts a two-layer architecture: a scalable, sharded database (TafDB) shared across namespaces and a per-namespace, single-server IndexMode consolidating lightweight directory metadata. With a fine-grained division of metadata and responsibility, Mantle supports up to 10 billion objects or directories in a single namespace and achieves 1.8 million lookups per second through scalable execution of single-RPC lookups on IndexMode. It also delivers up to 58K directory updates per second under high contention by integrating out-of-place delta updates in TafDB and offloading loop detection for cross-directory renames to IndexMode, both effectively eliminating coordination bottlenecks. Compared to the metadata services of Teetonic, InfiniFS and LocoFS, Mantle reduces metadata latency by 6.6-99.1% and improves throughput by 0.07-115.00%. With data access enabled, it shortens job completion times by 63.3-93.3% for interactive Spark analytics and 38.5-47.7% for AI-driven audio preprocessing tasks. Mantle has been deployed on Baidu Object Storage (BOS) for over 2 years, a service offered by Baidu Canghai Storage.",
      "htmlFile": "2025/d6d3c8e8f2d2/index.html"
    },
    {
      "id": "d7a3c9f8e12b2025ASE",
      "title": "A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code",
      "authors": [
        "Keke Lian",
        "Bing Wang",
        "Lei Zhang",
        "Libo Chen",
        "Junjie Wang",
        "Ziming Zhao",
        "Yujiu Yang",
        "Miaoqian Lin",
        "Haotong Duan",
        "Haoran Zhao",
        "Shuang Liao",
        "Mingda Guo",
        "Jiazheng Quan",
        "Yilu Zhong",
        "Chenhao He",
        "Zichuan Chen",
        "Jie Wu",
        "Haoling Li",
        "Zhaoxuan Li",
        "Jiongchi Yu",
        "Hui Li",
        "Dong Zhang"
      ],
      "year": 2025,
      "conference": "arXiv",
      "category": "软件工程与人工智能安全",
      "keywords": [
        "AI-Generated Code Security",
        "Repository-Level Benchmark",
        "Static Application Security Testing (SAST)",
        "Large Language Models (LLMs)",
        "Secure Code Generation",
        "Web Vulnerabilities",
        "CWE"
      ],
      "abstract": "The increasing adoption of large language models (LLMs) in software engineering necessitates rigorous security evaluation of their generated code. However, existing benchmarks often lack relevance to real-world AI-assisted programming scenarios, making them inadequate for assessing the practical security risks associated with AI-generated code in production environments. To address this gap, we introduce A.S.E (AI Code Generation Security Evaluation), a repository-level evaluation benchmark designed to closely mirror real-world AI programming tasks, offering a comprehensive and reliable framework for assessing the security of AI-generated code.\n\nOur evaluation of leading LLMs on A.S.E reveals several key findings. In particular, current LLMs still struggle with secure coding. The complexity in repository-level scenarios presents challenges for LLMs that typically perform well on snippet-level tasks. Moreover, a larger reasoning budget does not necessarily lead to better code generation. These observations offer valuable insights into the current state of AI code generation and help developers identify the most suitable models for practical tasks. They also lay the groundwork for refining LLMs to generate secure and efficient code in real-world applications.",
      "htmlFile": "2025/d7a3c9f8e12b2025ASE/index.html"
    },
    {
      "id": "d9c4a2e3b5f1",
      "title": "Moirai: Optimizing Placement of Data and Compute in Hybrid Clouds",
      "authors": [
        "Qiu, Ziyue",
        "Park, Hojin",
        "Zhao, Jing",
        "Wang, Yukai",
        "Balyan, Arnav",
        "Singh, Gurmeet",
        "Zhang, Yangjun",
        "Song, Suqiang (Jack)",
        "Ganger, Gregory R.",
        "Amvrosiadis, George"
      ],
      "year": 2025,
      "conference": "SOSP",
      "category": "分布式系统与云计算",
      "keywords": [
        "混合云",
        "成本优化",
        "数据布局",
        "作业调度",
        "网络开销",
        "数据复制",
        "作业路由"
      ],
      "abstract": "The deployment of large-scale data analytics between on-premise and cloud sites, i.e., hybrid clouds, requires careful partitioning of both data and computation to avoid massive networking costs. We present Moirai, a cost-optimization framework that analyzes job accesses and data dependencies and optimizes the placement of both in hybrid clouds. Moirai informs the job scheduler of data location and access predictions, so it can determine where jobs should be executed to minimize data transfer costs. Our optimizer achieves scalability and cost efficiency by exploiting recurring jobs to identify data dependencies and job access characteristics and reduces the search space by excluding data not accessed recently. We validate Moirai using 4-month traces that span 66.7M queries accessing 13.3EB from Presto and Spark clusters deployed at Uber, a multi-national transportation company leveraging large-scale data analytics for its operations. Moirai reduces hybrid cloud deployment costs by over 97% relative to the state-of-the-art partitioning approach from Alibaba and other public approaches. The savings come from 95–99.5% reduction in cloud egress, up to 99% reduction in replication, and 89–98% reduction in on-premises network infrastructure requirements. We also describe concrete steps being taken towards deploying Moirai in production.",
      "htmlFile": "2025/d9c4a2e3b5f1/index.html"
    },
    {
      "id": "db8a1f374b5e2025Efficient",
      "title": "Efficient Attention Methods: Hardware-efficient, Sparse, Compact, and Linear Attention",
      "authors": [
        "Jintao Zhang",
        "Rundong Su",
        "Chunyu Liu",
        "Jia Wei",
        "Ziteng Wang",
        "Haoxu Wang",
        "Pengle Zhang",
        "Huiqiang Jiang",
        "Haofeng Huang",
        "Chendong Xiang",
        "Haocheng Xi",
        "Shuo Yang",
        "Xingyang Li",
        "Yilong Zhao",
        "Yuezhou Hu",
        "Tianyu Fu",
        "Tianchen Zhao",
        "Yicheng Zhang",
        "Boqun Cao",
        "Youhe Jiang",
        "Kai Jiang",
        "Huayu Chen",
        "Min Zhao",
        "Xiaoming Xu",
        "Yi Wu",
        "Fan Bao",
        "Ion Stoica",
        "Joseph E. Gonzalez",
        "Jianfei Chen",
        "Jun Zhu"
      ],
      "year": 2025,
      "conference": "IEEE",
      "category": "机器学习",
      "keywords": [
        "高效注意力",
        "Transformer",
        "稀疏注意力",
        "紧凑注意力",
        "线性注意力",
        "Efficient Attention",
        "Hardware-efficient Attention"
      ],
      "abstract": "In modern transformer architectures, the attention operation is the only component with a time complexity of O(N^2), whereas all other operations scale linearly as O(N), where N is the sequence length. Recently, a plethora of methods have been proposed to improve the computational efficiency of the attention operation, which are very critical for developing large-scale models. In this paper, we present a unified taxonomy and a comprehensive survey of efficient attention methods. We categorize existing approaches into four classes: (1) Hardware-efficient attention: Optimizes attention computation efficiency by leveraging hardware characteristics; (2) Sparse attention: Selectively performs a subset of computations in attention while omitting others; (3) Compact attention: Compresses the KV cache (e.g., via weight sharing or low-rank decomposition) without changing the computation cost of using a full-sized KV cache; and (4) Linear attention: Reformulate the attention computation to achieve O(N) time complexity. For each category, we also develop a unified analysis framework and offer perspectives on open challenges and promising directions for future research on efficient attention. To the best of our knowledge, this survey is the most comprehensive survey on efficient attention. Our taxonomy of hardware-efficient, compact, sparse, and linear attention covers nearly all existing efficient attention methods, which prior surveys do not.",
      "htmlFile": "2025/db8a1f374b5e2025Efficient/index.html"
    },
    {
      "id": "e0c3c6d8e9c0",
      "title": "An Expressive, Efficient Attention Architecture",
      "authors": [
        "Yu Zhang",
        "Junjie Yan",
        "Zongyu Lin",
        "Zhejun Jiang",
        "Xingcheng Yao",
        "Weixiao Huang",
        "Jiaxi Hu",
        "Bohong Yin",
        "Fanqing Meng",
        "Jiacheng You",
        "Chengyin Liu",
        "Chu Wei",
        "Xin Men",
        "Zhengtao Wang",
        "Songlin Yang",
        "Chao Hong",
        "Zhiyuan Li",
        "Yutian Chen",
        "Wentao Li",
        "Guanduo Chen",
        "Enzhe Lu",
        "Yucheng Wang",
        "Weizhou Liu",
        "Huabin Zheng",
        "Yanru Chen",
        "Feng Wang",
        "Weixin Xu",
        "Yibo Liu",
        "Longhui Yu",
        "Mengnan Dong",
        "Yejie Wang",
        "Zheng Zhang",
        "Yu Fan",
        "Siyuan Pan",
        "Longguang Zhong",
        "Wenhao Wu",
        "Enming Yuan",
        "Yuhao Wu",
        "Dehao Zhang",
        "Longyu Guan",
        "Yizhi Zhang",
        "Jiawen Tao",
        "T.Y. Liu",
        "Guohong Fu",
        "Haiming Wang",
        "Xinran Xu",
        "Shengjun Fang",
        "Yuzhi Wang",
        "Weiran He",
        "Guokun Lai",
        "Shaowei Liu",
        "Yuxin Wu",
        "Yiwei Li",
        "Xinyu Zhou",
        "Jianlin Su",
        "Zhilin Yang",
        "Jiezhong Qiu",
        "Yulun Du",
        "Bo Pang"
      ],
      "year": 2025,
      "conference": "arXiv",
      "category": "自然语言处理",
      "keywords": [
        "线性注意力",
        "Kimi Delta Attention",
        "混合架构",
        "长上下文",
        "高效推理",
        "Delta Rule",
        "硬件效率"
      ],
      "abstract": "We introduce Kimi Linear, a hybrid linear attention architecture that, for the first time, outperforms full attention under fair comparisons across various scenarios--including short-context, long-context, and reinforcement learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an expressive linear attention module that extends Gated DeltaNet with a finer-grained gating mechanism, enabling more effective use of limited finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware efficiency through a specialized variant of the _Diagonal-Plus-Low-Rank_ (DPLR) transition matrices, which substantially reduces computation compared to the general DPLR formulation while remaining more consistent with the classical delta rule.\nWe pretrain a Kimi Linear model with 3B activated parameters and 48B total parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention (MLA). Our experiments show that with an identical training recipe, Kimi Linear outperforms full MLA with a sizeable margin across all evaluated tasks, while reducing KV cache usage by up to 75% and achieving up to 6x decoding throughput for a 1M context. These results demonstrate that Kimi Linear can be a drop-in replacement for full attention architectures with superior performance and efficiency, including tasks with longer input and output lengths.\nTo support further research, we open-source the KDA kernel and vLLM implementations 1, and release the pre-trained and instruction-tuned model checkpoints. 2",
      "htmlFile": "2025/e0c3c6d8e9c0/index.html"
    },
    {
      "id": "e4c218e5b0c3",
      "title": "KIMI K2: Open Agentic Intelligence",
      "authors": [
        "Yifan Bai",
        "Yiping Bao",
        "Guanduo Chen",
        "Jiahao Chen",
        "Ningxin Chen",
        "Ruijue Chen",
        "Yanru Chen",
        "Yuankun Chen",
        "Yutian Chen",
        "Zhuofu Chen*",
        "Jialei Cui",
        "Hao Ding",
        "Mengnan Dong",
        "Ang'ang Du",
        "Chenzhuang Du",
        "Dikang Du",
        "Yulun Du",
        "Yu Fan",
        "Yichen Feng",
        "Kelin Fu",
        "Bofei Gao",
        "Hongcheng Gao",
        "Peizhong Gao",
        "Tong Gao",
        "Xinran Gu",
        "Longyu Guan",
        "Haiqing Guo*",
        "Jianhang Guo",
        "Hao Hu",
        "Xiaoru Hao",
        "Tianhong He",
        "Weiran He",
        "Wenyang He",
        "Chao Hong",
        "Yangyang Hu",
        "Zhenxing Hu",
        "Weixiao Huang",
        "Zhiqi Huang",
        "Zihao Huang",
        "Tao Jiang",
        "Zhejun Jiang",
        "Xinyi Jin",
        "Yongsheng Kang*",
        "Guokun Lai",
        "Cheng Li",
        "Fang Li",
        "Haoyang Li",
        "Ming Li",
        "Wentao Li",
        "Yanhao Li",
        "Yiwei Li",
        "Zhaowei Li",
        "Zheming Li",
        "Hongzhan Lin*",
        "Xiaohan Lin",
        "Zongyu Lin",
        "Chengyin Liu",
        "Chenyu Liu",
        "Hongzhang Liu",
        "Jingyuan Liu*",
        "Junqi Liu",
        "Liang Liu",
        "Shaowei Liu",
        "T.Y. Liu",
        "Tianwei Liu",
        "Weizhou Liu",
        "Yangyang Liu",
        "Yibo Liu",
        "Yiping Liu",
        "Yue Liu",
        "Zhengying Liu",
        "Enzhe Lu",
        "Lijun Lu",
        "Shengling Ma",
        "Xinyu Ma",
        "Yingwei Ma",
        "Shaoguang Mao",
        "Jie Mei",
        "Xin Men",
        "Yibo Miao",
        "Siyuan Pan",
        "Yebo Peng",
        "Ruoyu Qin",
        "Bowen Qu",
        "Zeyu Shang",
        "Lidong Shi",
        "Shengyuan Shi",
        "Feifan Song",
        "Jianlin Su",
        "Zhengyuan Su",
        "Xinjie Sun*",
        "Flood Sung",
        "Heyi Tang",
        "Jiawen Tao",
        "Qifeng Teng",
        "Chensi Wang",
        "Dinglu Wang",
        "Feng Wang",
        "Haiming Wang",
        "Jianzhou Wang*",
        "Jiaxing Wang",
        "Jinhong Wang",
        "Shengjie Wang",
        "Shuyi Wang",
        "Yao Wang",
        "Yejie Wang",
        "Yiqin Wang",
        "Yuxin Wang",
        "Yuzhi Wang",
        "Zhaoji Wang",
        "Zhengtao Wang",
        "Chu Wei",
        "Qianqian Wei",
        "Wenhao Wu",
        "Xingzhe Wu",
        "Yuxin Wu",
        "Chenjun Xiao",
        "Xiaotong Xie",
        "Weimin Xiong*",
        "Boyu Xu",
        "Jing Xu*",
        "Jinjing Xu",
        "L.H. Xu",
        "Lin Xu",
        "Suting Xu",
        "Weixin Xu",
        "Xinran Xu",
        "Yangchuan Xu",
        "Ziyao Xu",
        "Junjie Yan",
        "Yuzi Yan",
        "Xiaofei Yang",
        "Ying Yang",
        "Zhen Yang",
        "Zhilin Yang",
        "Zonghan Yang",
        "Haotian Yao",
        "Xingcheng Yao",
        "Wenjie Ye",
        "Zhuorui Ye",
        "Bohong Yin",
        "Longhui Yu",
        "Enming Yuan",
        "Hongbang Yuan*",
        "Mengjie Yuan",
        "Haobing Zhan",
        "Dehao Zhang",
        "Hao Zhang",
        "Wanlu Zhang",
        "Xiaobin Zhang",
        "Yangkun Zhang",
        "Yizhi Zhang",
        "Yongting Zhang",
        "Yu Zhang",
        "Yutao Zhang",
        "Yutong Zhang",
        "Zheng Zhang",
        "Haotian Zhao",
        "Yikai Zhao",
        "Huabin Zheng",
        "Shaojie Zheng",
        "Jianren Zhou",
        "Xinyu Zhou",
        "Zaida Zhou",
        "Zhen Zhu",
        "Weiyu Zhuang",
        "Xinxing Zu",
        "Kimi K2"
      ],
      "year": 2025,
      "conference": "arXiv",
      "category": "自然语言处理",
      "keywords": [
        "大语言模型",
        "Agentic Intelligence",
        "Mixture-of-Experts",
        "强化学习",
        "工具使用",
        "MuonClip优化器",
        "数据合成"
      ],
      "abstract": "We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32 billion activated parameters and 1 trillion total parameters. We propose the MuonClip optimizer, which improves upon Muon with a novel QK-clip technique to address training instability while enjoying the advanced token efficiency of Muon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero loss spike. During post-training, K2 undergoes a multi-stage post-training process, highlighted by a large-scale agentic data synthesis pipeline and a joint reinforcement learning (RL) stage, where the model improves its capabilities through interactions with real and synthetic environments.\n\nKimi K2 achieves state-of-the-art performance among open-source non-thinking models, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on Tau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench Verified, and 47.3 on SWE-Bench Multilingual -- surpassing most open and closed-sourced baselines in non-thinking settings. It also exhibits strong capabilities in coding, mathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6, 49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on OJBench, all without extended thinking. These results position Kimi K2 as one of the most capable open-source large language models to date, particularly in software engineering and agentic tasks. We release our base and post-trained model checkpoints1 to facilitate future research and applications of agentic intelligence.\n\nFootnote 1: https://huggingface.co/moonshotai/Kimi-K2-Instruct\n\n2All models evaluated above are non-thinking models. For SWE-bench Multilingual, we evaluated only Claude 4 Sonnet because the cost of Claude 4 Opus was prohibitive.",
      "htmlFile": "2025/e4c218e5b0c3/index.html"
    },
    {
      "id": "e7a9d3b5c2f12025Accelera",
      "title": "Accelerating Large-Scale Reasoning Model Inference: Self-Speculative Decoding with Sparse Attention",
      "authors": [
        "Yilong Zhao",
        "Jiaming Tang",
        "Kan Zhu",
        "Zihao Ye",
        "Chi-Chih Chang",
        "Chaofan Lin",
        "Jongseok Park",
        "Guangxuan Xiao",
        "Mohamed S. Abdelfattah",
        "Mingyu Gao",
        "Baris Kasikci",
        "Song Han",
        "Ion Stoica"
      ],
      "year": 2025,
      "conference": "arXiv",
      "category": "自然语言处理",
      "keywords": [
        "推理语言模型",
        "推测解码",
        "稀疏注意力",
        "自推测",
        "KV-Cache",
        "内存带宽",
        "系统优化"
      ],
      "abstract": "Reasoning language models have demonstrated remarkable capabilities on challenging tasks by generating elaborate chain-of-thought solutions. However, such lengthy generation shifts the inference bottleneck from compute-bound to memory-bound. To generate each token, the model applies full attention to all previously generated tokens, requiring memory access to an increasingly large KV-Cache. Consequently, longer generations demand more memory access for every step, leading to substantial pressure on memory bandwidth.\n\nTo address this, we introduce SparseSpec, a speculative decoding framework that reuses the same model as both the draft and target models (i.e., self-speculation). SparseSpec features a novel sparse attention mechanism, PillarAttn, as the draft model, which accurately selects critical tokens via elegantly reusing information from the verification stage. Furthermore, SparseSpec co-designs self-speculation with three system optimizations: (1) a unified scheduler to batch both draft and verification phases to maximize parallelism, (2) delayed verification for CPU/GPU overlap, and (3) dynamic KV-Cache management to enable host memory offload to maximize GPU memory utilization. Across various models and datasets, SparseSpec outperforms state-of-the-art solutions, with an up to 2.13× throughput gain. Code is open-sourced at github.com/sspec-project/SparseSpec.",
      "htmlFile": "2025/e7a9d3b5c2f12025Accelera/index.html"
    },
    {
      "id": "e7c5d9a4b1f92025FlashInfe",
      "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving",
      "authors": [
        "Zihao Ye",
        "Lequn Chen",
        "Ruihang Lai",
        "Wuwei Lin",
        "Yineng Zhang",
        "Stephanie Wang",
        "Tianqi Chen",
        "Baris Kasikci",
        "Vinod Grover",
        "Arvind Krishnamurthy",
        "Luis Ceze"
      ],
      "year": 2025,
      "conference": "MLSys",
      "category": "机器学习系统",
      "keywords": [
        "LLM推理",
        "大语言模型服务",
        "注意力机制优化",
        "GPU核函数",
        "FlashAttention",
        "JIT编译",
        "动态调度"
      ],
      "abstract": "Transformers, driven by attention mechanisms, form the foundation of large language models (LLMs). As these models scale up, efficient GPU attention kernels become essential for high-throughput and low-latency inference. Diverse LLM applications demand flexible and high-performance attention solutions. We present FlashInfer: a customizable and efficient attention engine for LLM serving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse format and composable formats to optimize memory access and reduce redundancy. It also offers a customizable attention template, enabling adaptation to various settings through Just-In-Time (JIT) compilation. Additionally, FlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user requests while maintaining compatibility with CUDAGraph which requires static configuration. FlashInfer have been integrated into leading LLM serving frameworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and end-to-end evaluations demonstrate FlashInfer's ability to significantly boost kernel performance across diverse inference scenarios: compared to state-of-the-art LLM serving solutions, FlashInfer achieve 29-69% inter-token-latency reduction compared to compiler backends for LLM serving benchmark, 28-30% latency reduction for long-context inference, and 13-17% speedup for LLM serving with parallel generation.",
      "htmlFile": "2025/e7c5d9a4b1f92025FlashInfe/index.html"
    },
    {
      "id": "e9c6b3a8f4d2",
      "title": "TickTock: Verified Isolation in a Production Embedded OS",
      "authors": [
        "Rindisbacher, Vivien",
        "Johnson, Evan",
        "Pannuto, Pat",
        "Savage, Stefan"
      ],
      "year": 2025,
      "conference": "SOSP",
      "category": "操作系统安全",
      "keywords": [
        "验证",
        "进程隔离",
        "内核安全",
        "嵌入式系统",
        "安全系统",
        "精化类型",
        "形式化验证"
      ],
      "abstract": "We present a case study formally verifying process isolation in the Tock production microcontroller OS kernel. Tock combines hardware memory protection units and language-level techniques—by writing the kernel in Rust—to enforce isolation between user and kernel code. Our effort to verify Tock’s process abstraction unearthed multiple, subtle bugs that broke isolation—many allowing malicious applications to compromise the whole OS. We describe this effort and TickTock, our fork of the Tock operating system kernel that eliminates isolation bugs by construction. TickTock uses Flux, an SMT-based Rust verifier, to formally specify and verify process isolation for all ARMy7-M platforms Tock supports and for three RISC-V 32-bit platforms. Our verification-guided design and implementation led to a new, granular process abstraction that is simpler than Tock’s, has formal security guarantees (that are verified in half a minute), and outperforms Tock on certain critical code paths.",
      "htmlFile": "2025/e9c6b3a8f4d2/index.html"
    },
    {
      "id": "f5f9a1a6c9e7",
      "title": "Parallel kd-tree with Batch Updates",
      "authors": [
        "Ziyang Men",
        "Zheqi Shen",
        "Yan Gu",
        "Yihan Sun"
      ],
      "year": 2025,
      "conference": "SIGMOD",
      "category": "数据库",
      "keywords": [
        "并行计算",
        "kd-tree",
        "批量更新",
        "缓存优化",
        "空间数据结构"
      ],
      "abstract": "The kd-tree is one of the most widely used data structures to manage multi-dimensional data. Due to the ever-growing data volume, it is imperative to consider parallelism in kd-trees. However, we observed challenges in existing parallel kd-tree implementations, for both constructions and updates.\nThe goal of this paper is to develop efficient in-memory kd-trees by supporting high parallelism and cache-efficiency. We propose the Pkd-tree (Parallel kd-tree), a parallel kd-tree that is efficient both in theory and in practice. The Pkd-tree supports parallel tree construction, batch update (insertion and deletion), and various queries including k-nearest neighbor search, range query, and range count. We proved that our algorithms have strong theoretical bounds in work (sequential time complexity), span (parallelism), and cache complexity. Our key techniques include 1) an efficient construction algorithm that optimizes work, span, and cache complexity simultaneously, and 2) reconstruction-based update algorithms that guarantee the tree to be weight-balanced. With the new algorithmic insights and careful engineering effort, we achieved a highly optimized implementation of the Pkd-tree.\nWe tested Pkd-tree with various synthetic and real-world datasets, including both uniform and highly skewed data. We compare the Pkd-tree with state-of-the-art parallel kd-tree implementations. In all tests, with better or competitive query performance, Pkd-tree is much faster in construction and updates consistently than all baselines. We released our code.",
      "htmlFile": "2025/f5f9a1a6c9e7/index.html"
    },
    {
      "id": "f7c4e8a3b5d1",
      "title": "Defeating Nondeterminism in LLM Inference",
      "authors": [
        "Horace He"
      ],
      "year": 2025,
      "conference": "Blog",
      "category": "自然语言处理",
      "keywords": [
        "LLM推理",
        "非确定性",
        "批量不变性",
        "浮点非结合性",
        "可重现性"
      ],
      "abstract": "Reproducibility is a bedrock of scientific progress. However, it’s remarkably difficult to get reproducible results out of large language models. For example, you might observe that asking ChatGPT the same question multiple times provides different results. This by itself is not surprising, since getting a result from a language model involves \"sampling\", a process that converts the language model's output into a probability distribution and probabilistically selects a token. What might be more surprising is that even when we adjust the temperature down to 0 (thus making the sampling theoretically deterministic), LLM APIs are still not deterministic in practice. Even when running inference on your own hardware with an OSS inference library like vLLM or SGLang, sampling still isn't deterministic. In this post, we will explain why the \"concurrency + floating point\" hypothesis misses the mark, unmask the true culprit behind LLM inference nondeterminism, and explain how to defeat nondeterminism and obtain truly reproducible results in LLM inference.",
      "htmlFile": "2025/f7c4e8a3b5d1/index.html"
    },
    {
      "id": "f7f6f5e0f4d3",
      "title": "Cloud-Native Systems for Generative AI Applications: Current Trends and Open Challenges",
      "authors": [
        "Minchen Yu",
        "Wei Wang",
        "Yue Cheng",
        "Hong Xu"
      ],
      "year": 2025,
      "conference": "NeurIPS",
      "category": "人工智能系统",
      "keywords": [
        "生成式人工智能",
        "云原生系统",
        "模块化设计",
        "异构资源",
        "推理服务"
      ],
      "abstract": "In recent years, Generative Artificial Intelligence (GAI) has advanced rapidly and seen widespread adoption across a broad range of domains, placing unprecedented demands on cloud systems and infrastructure. We identify three key trends--spanning the model, application, and infrastructure levels--that are collectively driving a shift toward _modular design_ for GAI systems, akin to the microservice architecture widely used in cloud-native platforms. In this paradigm, GAI workloads are decomposed into specialized, fine-grained modules that can be deployed, managed, and scaled independently. This microservice-like modular approach enables precise resource allocation, efficient sharing of heterogeneous hardware, and improved scalability in response to dynamic workload characteristics and evolving application requirements. Building on this design principle, we outline several open challenges that warrant further investigation in future research.",
      "htmlFile": "2025/f7f6f5e0f4d3/index.html"
    },
    {
      "id": "f8e4b3a9c2d1",
      "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
      "authors": [
        "Jingwei Xu",
        "Junbin Kang",
        "Mingkai Dong",
        "Mingyu Liu",
        "Lu Zhang",
        "Shaohong Guo",
        "Ziyan Qiu",
        "Mingzhen You",
        "Ziyi Tian",
        "Anqi Yu",
        "Tianhong Ding",
        "Xinwei Hu",
        "Haibo Chen"
      ],
      "year": 2025,
      "conference": "arXiv",
      "category": "分布式系统",
      "keywords": [
        "分布式文件系统",
        "深度学习",
        "元数据管理",
        "无状态客户端",
        "高性能存储"
      ],
      "abstract": "Client-side metadata caching has long been considered an effective method for accelerating metadata operations in distributed file systems(DFSs). However, we have found thatclient-side state(e.g., caching) is not only ineffective but alsoconsumes valuable memory resources in the deep learning pipelines. We thus propose FalconFS, a DFS optimized for deep learning pipelines with the stateless-client architecture.Specifically, instead of performing client-side path resolution and caching, FalconFS efficiently resolves paths on the serverside using hybrid metadata indexing and lazy namespace replication. FalconFS also boosts server concurrency with concurrent request merging and provides easy deployment with VFS shortcut. Evaluations against CephFS and Lustreshow that FalconFS achievesup to 5.72× throughput for smallfile read/write and up to 12.81× throughput for deep learn-ing model training. FalconFS has been running in Huawei autonomous driving system’s production environment with10,000 NPUs for one year.",
      "htmlFile": "2025/f8e4b3a9c2d1/index.html"
    },
    {
      "id": "1a2b3c4d5e6f2024QUIC-Fuz",
      "title": "QUIC-Fuzz: An Effective Greybox Fuzzer For The QUIC Protocol",
      "authors": [
        "Kian Kai Ang",
        "Damith C. Ranasinghe"
      ],
      "year": 2024,
      "conference": "arXiv",
      "category": "网络与信息安全",
      "keywords": [
        "模糊测试",
        "网络协议",
        "QUIC",
        "greybox fuzzing",
        "network security",
        "协议安全",
        "漏洞发现"
      ],
      "abstract": "Network applications are routinely under attack. We consider the problem of developing an effective and efficient fuzzer for the recently ratified QUIC network protocol to uncover security vulnerabilities. QUIC offers a _unified_ transport layer for low latency, reliable transport _streams_ that is _inherently_ secure, ultimately representing a complex protocol design characterised by new features and capabilities for the Internet. Fuzzing a _secure transport layer_ protocol is not trivial. The interactive, strict, rule-based, asynchronous nature of communications with a target, the stateful nature of interactions, security mechanisms to protect communications (such as integrity checks and encryption), and inherent overheads (such as target initialisation) challenge _generic_ network protocol fuzzers. We discuss and address the challenges pertinent to fuzzing transport layer protocols (like QUIC), developing mechanisms that enable fast, effective fuzz testing of QUIC implementations to build a prototype _grey-box_ mutation-based fuzzer--QUIC-Fuzz. We test 6, _well-maintained_ server-side implementations, including from Google and Alibaba with QUIC-Fuzz. The results demonstrate the fuzzer is both highly effective and generalisable. Our testing uncovered _10 new_ security vulnerabilities, precipitating 2 CVE assignments thus far. In code coverage, QUIC-Fuzz outperforms other existing state-of-the-art network protocol fuzzers--Fuzziruction-Net, CharAFL, and ALFNet--with up to an 84% increase in code coverage where QUIC-Fuzz outperformed _statistically significantly_ across all targets and with _a majority of bugs_ only discoverable by QUIC-Fuzz. We open-source QUIC-Fuzz on GitHub https://github.com/QUICTester/QUIC-Fuzz.",
      "htmlFile": "2024/1a2b3c4d5e6f2024QUICFuzz/index.html"
    },
    {
      "id": "1c9e9d7f0f5d",
      "title": "Exploiting Undefined Behavior in C/C++ Programs for Optimization: A Study on the Performance Impact",
      "authors": [
        "LUCIAN POPESCU",
        "NUNO P. LOPES"
      ],
      "year": 2024,
      "conference": "PLDI",
      "category": "编译优化",
      "keywords": [
        "Undefined Behavior",
        "Compiler Optimizations",
        "C",
        "C++",
        "LLVM",
        "性能分析",
        "代码安全"
      ],
      "abstract": "The C and C++ languages define hundreds of cases as having undefined behavior (UB). These include, for example, corner cases where different CPU architectures disagree on the semantics of an instruction and the language does not want to force a specific implementation (e.g., shift by a value larger than the bitwidth). Another class of UB involves errors that the language chooses not to detect because it would be too expensive or impractical, such as dereferencing out-of-bounds pointers.\n\nAlthough there is a common belief within the compiler community that UB enables certain optimizations that would not be possible otherwise, no rigorous large-scale studies have been conducted on this subject. At the same time, there is growing interest in eliminating UB from programming languages to improve security.\n\nIn this paper, we present the first comprehensive study that examines the performance impact of exploiting UB in C and C++ applications across multiple CPU architectures. Using LLVM, a compiler known for its extensive use of UB for optimizations, we demonstrate that, for the benchmarks and UB categories that we evaluated, the end-to-end performance gains are minimal. Moreover, when performance regresses, it can often be recovered through small improvements to optimization algorithms or by using link-time optimizations.",
      "htmlFile": "2024/1c9e9d7f0f5d/index.html"
    },
    {
      "id": "6b7d49e3f9a220249045In",
      "title": "In-Memory Key-Value Store Live Migration with NetMigrate",
      "authors": [
        "Zeying Zhu",
        "Yibo Zhao",
        "Zaoxing Liu"
      ],
      "year": 2024,
      "conference": "FAST",
      "category": "存储系统",
      "keywords": [
        "内存键值存储",
        "在线迁移",
        "可编程数据平面",
        "NetMigrate",
        "键值分片迁移"
      ],
      "abstract": "Distributed key-value stores today require frequent key-value shard migration between nodes to react to dynamic workload changes for load balancing, data locality, and service elasticity. In this paper, we propose NetMigrate, a live migration approach for in-memory key-value stores based on programmable network data planes. NetMigrate migrates shards between nodes with zero service interruption and minimal performance impact. During migration, the switch data plane monitors the migration process in a fine-grained manner and directs client queries to the right server in real time, eliminating the overhead of pulling data between nodes. We implement a NetMigrate prototype on a testbed consisting of a programmable switch and several commodity servers running Redis, and evaluate it under YCSB workloads. Our experiments demonstrate that NetMigrate improves the query throughput from 6.5% to 416% and maintains low access latency during migration, compared to the state-of-the-art migration approaches.",
      "htmlFile": "2024/6b7d49e3f9a220249045In/index.html"
    },
    {
      "id": "8f7a2c1b0d9e2024TheDesi",
      "title": "The Design and Implementation of a Capacity-Variant Storage System",
      "authors": [
        "Ziyang Jiao",
        "Xiangqun Zhang",
        "Hojin Shin",
        "Jongmoo Choi",
        "Bryan S. Kim"
      ],
      "year": 2024,
      "conference": "FAST",
      "category": "存储系统",
      "keywords": [
        "容量可变存储系统",
        "固态硬盘",
        "性能衰减",
        "磨损均衡",
        "生命周期管理",
        "容量-性能折衷",
        "fail-slow"
      ],
      "abstract": "We present the design and implementation of a capacity-variant storage system (CVSS) for flash-based solid-state drives (SSDs). CVSS aims to maintain high performance throughout the lifetime of an SSD by allowing storage capacity to gracefully reduce over time, thus preventing fail-slow symptoms. The CVSS comprises three key components: (1) CV-SSD, an SSD that minimizes write amplification and gracefully reduces its exported capacity with age; (2) CV-FS, a log-structured file system for elastic logical partition; and (3) CV-manager, a user-level program that orchestrates system components based on the state of the storage system. We demonstrate the effectiveness of CVSS with synthetic and real workloads, and show its significant improvements in latency, throughput, and lifetime compared to a fixed-capacity storage system. Specifically, under real workloads, CVSS reduces the latency, improves the throughput, and extends the lifetime by 8–53%, 49–316%, and 268–327%, respectively.",
      "htmlFile": "2024/8f7a2c1b0d9e2024TheDesi/index.html"
    },
    {
      "id": "a1b2c3d4e5f62024AnIntro",
      "title": "An Introduction to the Compute Express Link (CXL) Interconnect",
      "authors": [
        "Debendra Das Sharma",
        "Robert Blankenship",
        "Daniel Berger"
      ],
      "year": 2024,
      "conference": "ACM Comput. Surv.",
      "category": "计算机体系结构",
      "keywords": [
        "Compute Express Link",
        "CXL",
        "内存互连",
        "缓存一致性",
        "资源池化",
        "数据中心"
      ],
      "abstract": "The Compute Express Link (CXL) is an open industry-standard interconnect between processors and devices such as accelerators, memory buffers, smart network interfaces, persistent memory, and solid-state drives. CXL offers coherency and memory semantics with bandwidth that scales with PCIe bandwidth while achieving significantly lower latency than PCIe. All major CPU vendors, device vendors, and datacenter operators have adopted CXL as a common standard. This enables an inter-operable ecosystem that supports key computing use cases including highly efficient accelerators, server memory bandwidth and capacity expansion, multi-server resource pooling and sharing, and efficient peer-to-peer communication. This survey provides an introduction to CXL covering the standards CXL 1.0, CXL 2.0, and CXL 3.0. We further survey CXL implementations, discuss CXL’s impact on the datacenter landscape, and future directions.",
      "htmlFile": "2024/a1b2c3d4e5f62024AnIntro/index.html"
    },
    {
      "id": "a3cee8c5e2e8",
      "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
      "authors": [
        "LEI HUANG",
        "WEIJIANG YU",
        "WEITAO MA",
        "WEIHONG ZHONG",
        "ZHANGYIN FENG",
        "HAOTIAN WANG",
        "QIANGLONG CHEN",
        "WEIHUA PENG",
        "XIAOCHENG FENG",
        "BING QIN",
        "TING LIU"
      ],
      "year": 2024,
      "conference": "ACM TOIS",
      "category": "自然语言处理",
      "keywords": [
        "大语言模型",
        "Hallucination",
        "事实性",
        "忠实性",
        "检索增强生成",
        "幻觉检测",
        "幻觉缓解"
      ],
      "abstract": "The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.",
      "htmlFile": "2024/a3cee8c5e2e8/index.html"
    },
    {
      "id": "a4a3b2b3c4d5",
      "title": "LLMs Can Get \"Brain Rot\"!",
      "authors": [
        "Shuo Xing",
        "Junyuan Hong",
        "Yifan Wang",
        "Runjin Chen",
        "Zhenyu Zhang",
        "Ananth Grama",
        "Zhengzhong Tu",
        "Zhangyang Wang"
      ],
      "year": 2024,
      "conference": "arXiv",
      "category": "自然语言处理",
      "keywords": [
        "大语言模型",
        "数据质量",
        "认知衰退",
        "持续预训练",
        "安全性",
        "推理能力",
        "思维跳跃"
      ],
      "abstract": "We propose and test the LLM Brain Rot Hypothesis: continual exposure to junk web text induces lasting cognitive decline in large language models (LLMs). To causally isolate data quality, we run controlled experiments on real Twitter/X corpora, constructing junk and reversely controlled datasets via two orthogonal operationalizations: M1 (engagement degree) and M2 (semantic quality), with matched token scale and training operations across conditions. Contrary to the control group, continual pre-training of 4 LLMs on the junk dataset causes non-trivial declines (Hedges' g>0.3) on reasoning, long-context understanding, safety, and inflating \"dark traits\" (e.g., psychopathy, narcissism). The gradual mixtures of junk and control datasets also yield dose-response cognition decay: for example, under M1, ARC-Challenge with Chain Of Thoughts drops 74.9 to 57.2 and RULER-CWE 84.4 to 52.3 as junk ratio rises from 0% to 100%.\n\nError forensics reveal several key insights. First, we identify thought-skipping as the primary lesion: models increasingly truncate or skip reasoning chains, explaining most of the error growth. Second, partial but incomplete healing is observed: scaling instruction tuning and clean data pre-training improve the declined cognition yet cannot restore baseline capability, suggesting persistent representational drift rather than format mismatch. Finally, we discover that the popularity, a non-semantic metric, of a tweet is a better indicator of the Brain Rot effect than the length in M1. Together, the results provide significant, multi-perspective evidence that data quality is a causal driver of LLM capability decay, reframing curation for continual pretraining as a training-time safety problem and motivating routine \"cognitive health checks\" for deployed LLMs.",
      "htmlFile": "2024/a4a3b2b3c4d5/index.html"
    },
    {
      "id": "a7b9c1e2f4d5102025Make",
      "title": "Make LLM Inference Affordable to Everyone: Augmenting GPU Memory with NDP-DIMM",
      "authors": [
        "Lian Liu",
        "Shixin Zhao",
        "Bing Li",
        "Haimeng Ren",
        "Zhaohui Xu",
        "Mengdi Wang",
        "Xiaowei Li",
        "Yinhe Han",
        "Ying Wang"
      ],
      "year": 2024,
      "conference": "arXiv preprint",
      "category": "计算机系统与体系结构",
      "keywords": [
        "大语言模型推理",
        "近数据计算",
        "激活稀疏性",
        "热/冷神经元",
        "异构计算",
        "内存增强",
        "GPU"
      ],
      "abstract": "The billion-scale Large Language Models (LLMs) necessitate deployment on expensive server-grade GPUs with large-storage HBMs and abundant computation capability. As LLM-assisted services become popular, achieving cost-effective LLM inference on budget-friendly hardware becomes the current trend. This has sparked extensive research into relocating LLM parameters from expensive GPUs to external host memory. However, the restricted bandwidth between the host and GPU memory limits the inference performance of existing solutions.\nThis work introduces Hermes, a budget-friendly system that leverages the near-data processing units (NDP) within commodity DRAM DIMMs to enhance the performance of a single consumer-grade GPU, achieving efficient LLM inference. We recognize that the inherent activation sparsity in LLMs naturally divides weight parameters into two categories, termed \"hot\" and \"cold\" neurons, respectively. Hot neurons, which consist of only approximately 20% of all weight parameters, account for 80% of the total computational load. In contrast, cold neurons make up the other 80% of parameters but are responsible for just 20% of the computational workload. Leveraging this observation, we propose a heterogeneous computing strategy: mapping hot neurons to a single computation-efficient GPU without large-capacity HBMs, while offloading cold neurons to NDP-DIMMs, which offer large memory size but limited computation capabilities. In addition, the dynamic nature of activation sparsity necessitates a real-time partition of hot and cold neurons and adaptive remapping of cold neurons across multiple NDP-DIMM modules. To tackle these issues, we introduce a lightweight predictor that ensures optimal real-time neuron partition and adjustment between GPU and NDP-DIMMs. Furthermore, we utilize a window-based online scheduling mechanism to maintain load balance among multiple NDP-DIMM modules. In summary, Hermes facilitates the deployment of LLaMA2-70B on consumer-grade hardware at a rate of 13.75 tokens/s and realizes an average 75.24× speedup over the state-of-the-art offloading-based inference system on popular LLMs.",
      "htmlFile": "2024/a7b9c1e2f4d5102025Make/index.html"
    },
    {
      "id": "a7e3c9b1f2d8",
      "title": "Comprehensive Review of Performance Optimization Strategies for Serverless Applications on AWS Lambda",
      "authors": [
        "Mohamed Lemine El Bechir",
        "Cheikh Sad Bouh",
        "Abbbakr Shuwail"
      ],
      "year": 2024,
      "conference": "arXiv",
      "category": "云计算",
      "keywords": [
        "Serverless Computing",
        "AWS Lambda",
        "Performance Optimization",
        "Resource Management",
        "Cost Efficiency",
        "Observability"
      ],
      "abstract": "This review paper synthesizes the latest research on performance optimization strategies for serverless applications deployed on AWS Lambda. By examining recent studies, we highlight the challenges, solutions, and best practices for enhancing the performance, cost-efficiency, and scalability of serverless applications. The review covers a range of optimization techniques including resource management, runtime selection, observability improvements, and workload-aware operations.",
      "htmlFile": "2024/a7e3c9b1f2d8/index.html"
    },
    {
      "id": "a8e8d5f7d5d5",
      "title": "A Survey on Efficient Inference for Large Language Models",
      "authors": [
        "Zixuan Zhou",
        "Xuefei Ning",
        "Ke Hong",
        "Tianyu Fu",
        "Jiaming Xu",
        "Shiyao Li",
        "Yuming Lou",
        "Luning Wang",
        "Zhihang Yuan",
        "Xiuhong Li",
        "Shengen Yan",
        "Guohao Dai",
        "Xiao-Ping Zhang",
        "Huazhong Yang",
        "Yuhan Dong",
        "Yu Wang"
      ],
      "year": 2024,
      "conference": "arXiv",
      "category": "自然语言处理",
      "keywords": [
        "大语言模型",
        "高效推理",
        "模型压缩",
        "系统优化",
        "量化",
        "注意力机制",
        "推测解码"
      ],
      "abstract": "Large Language Models (LLMs) have attracted extensive attention due to their remarkable performance across various tasks. However, the substantial computational and memory requirements of LLM inference pose challenges for deployment in resource-constrained scenarios. Efforts within the field have been directed towards developing techniques aimed at enhancing the efficiency of LLM inference. This paper presents a comprehensive survey of the existing literature on efficient LLM inference. We start by analyzing the primary causes of the inefficient LLM inference, i.e., the large model size, the quadratic-complexity attention operation, and the auto-regressive decoding approach. Then, we introduce a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization. Moreover, the paper includes comparative experiments on representative methods within critical sub-fields to provide quantitative insights. Last but not least, we provide some knowledge summary and discuss future research directions.",
      "htmlFile": "2024/a8e8d5f7d5d5/index.html"
    },
    {
      "id": "b1c5d6c5f6f4",
      "title": "λScale: Enabling Fast Scaling for Serverless Large Language Model Inference",
      "authors": [
        "Minchen Yu",
        "Rui Yang",
        "Chaobo Jia",
        "Zhaoyuan Su",
        "Sheng Yao",
        "Tingfeng Lan",
        "Yuchen Yang",
        "Yue Cheng",
        "Wei Wang",
        "Ao Wang",
        "Ruichuan Chen"
      ],
      "year": 2024,
      "conference": "OSDI",
      "category": "分布式系统",
      "keywords": [
        "Serverless计算",
        "大语言模型推理",
        "快速扩缩容",
        "RDMA",
        "执行即加载",
        "分布式推理",
        "λPipe"
      ],
      "abstract": "Serverless computing has emerged as a compelling solution for cloud-based model inference. However, as modern large language models (LLMs) continue to grow in size, existing serverless platforms often face substantial model startup overhead. This poses a significant challenge in efficiently scaling model instances to accommodate dynamic, bursty workloads commonly observed in real-world inference services. In this paper, we introduce λScale, an efficient serverless inference system to achieve fast model scaling. The key idea behind λScale is to leverage high-speed RDMA networks between GPU nodes for fast model multicast, while enabling distributed inference execution during model transmission--referred to as \"execute-while-load\". λScale proposes an efficient model scaling scheme, λPipe, which supports adaptive model multicast and dynamically constructs execution pipelines across receiving nodes for collaborative, distributed inference. Additionally, λScale supports efficient model management across GPU and host memory, allowing fast scaling for models across different storage tiers. Evaluation results show that λScale enables fast model scaling and effectively handles load spikes, achieving up to 5× tail-latency improvement and 31.3% cost reduction compared to state-of-the-art solutions on real-world LLM inference traces.",
      "htmlFile": "2024/b1c5d6c5f6f4/index.html"
    },
    {
      "id": "b5f8d2a3c1e702024TileLin",
      "title": "TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives",
      "authors": [
        "Size Zheng",
        "Jin Fang",
        "Xuegui Zheng",
        "Qi Hou",
        "Wenlei Bao",
        "Ningxin Zheng",
        "Ziheng Jiang",
        "Dongyang Wang",
        "Jianxi Ye",
        "Haibin Lin",
        "Li-Wen Chang",
        "Xin Liu"
      ],
      "year": 2024,
      "conference": "arXiv",
      "category": "高性能计算与分布式系统",
      "keywords": [
        "计算通信重叠 (Compute-Communication Overlapping)",
        "编译器 (Compiler)",
        "图块 (Tile)",
        "并行计算 (Parallel Computing)",
        "大语言模型 (Large Language Models)",
        "TileLink",
        "算子融合 (Kernel Fusion)"
      ],
      "abstract": "Large deep learning models have achieved state-of-the-art performance in a wide range of tasks. These models often necessitate distributed systems for efficient training and inference. The fundamental building blocks for distributed model execution are intra-layer parallel operators. The most effective approach to enhancing the performance of intra-layer parallel operators involves overlapping computation with communication. The overlapping can be achieved through either operator decomposition or kernel fusion. While decomposing operators is straightforward to implement, it often results in suboptimal performance. On the other hand, fusing communication kernels with compute kernels demands significant expertise and is error-prone.\n\nIn this paper, we propose TileLink to enable efficient compilation and generation of overlapped compute-communication kernels. TileLink is composed of frontend and backend. In the frontend, TileLink decouples the design space of communication and computation, linking these two parts via tile-centric primitives. In the backend, TileLink translates these primitives into low-level communication instructions, integrating the communication and computation components to achieve overlapped execution. In experiments, TileLink achieves from 1.17\\u00d7 to 20.76\\u00d7 speedup to non-overlapping baseline and achieves performance comparable to state-of-the-art overlapping libraries on GPUs.",
      "htmlFile": "2024/b5f8d2a3c1e702024TileLin/index.html"
    },
    {
      "id": "b7f2a8c3d5e12024gemma",
      "title": "Gemma: Open Models Based on Gemini Research and Technology",
      "authors": [
        "Gemma Team, Google DeepMind"
      ],
      "year": 2024,
      "conference": "arXiv",
      "category": "自然语言处理",
      "keywords": [
        "Gemma",
        "大语言模型",
        "开放模型",
        "指令微调",
        "安全性评估",
        "RLHF"
      ],
      "abstract": "This work introduces Gemma, a family of lightweight, state- of- the art open models built from the research and technology used to create Gemini models. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine- tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text- based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations.",
      "htmlFile": "2024/b7f2a8c3d5e12024gemma/index.html"
    },
    {
      "id": "b8b5e5a5d3b5",
      "title": "Cloud abstractions for AI workloads",
      "authors": [
        "Marco Canini",
        "Theophilus A. Benson",
        "Ricardo Bianchini",
        "Inigo Goiri",
        "Dejan Kostic",
        "Peter Pietzuch",
        "Simon Peter"
      ],
      "year": 2024,
      "conference": "arXiv",
      "category": "云计算系统",
      "keywords": [
        "云计算",
        "AI工作负载",
        "跨层优化",
        "租户-提供商协作",
        "HarmonAIze",
        "抽象设计",
        "性能优化"
      ],
      "abstract": "AI workloads, often hosted in multi-tenant cloud environments, require vast computational resources but suffer inefficiencies due to limited tenant-provider coordination. Tenants lack infrastructure insights, while providers lack workload details to optimize tasks like partitioning, scheduling, and fault tolerance. We propose HarmonAIze to redefine cloud abstractions, enabling cooperative optimization for improved performance, efficiency, resiliency, and sustainability. We outline key opportunities and challenges this vision faces.",
      "htmlFile": "2024/b8b5e5a5d3b5/index.html"
    },
    {
      "id": "b8d7f6e5a4c32024kosmoef",
      "title": "Kosmo: Efficient Online Miss Ratio Curve Generation for Eviction Policy Evaluation",
      "authors": [
        "Kia Shakiba",
        "Sari Sultan",
        "Michael Stumm"
      ],
      "year": 2024,
      "conference": "FAST",
      "category": "计算机系统",
      "keywords": [
        "缓存管理",
        "失效率曲线",
        "驱逐策略",
        "在线评估",
        "Kosmo算法",
        "eviction map",
        "内存缓存"
      ],
      "abstract": "In-memory caches play an important role in reducing the load on backend storage servers for many workloads. Miss ratio curves (MRCs) are an important tool for configuring these caches with respect to cache size and eviction policy. MRCs provide insight into the trade-off between cache size (and thus costs) and miss ratio for a specific eviction policy. Over the years, many MRC-generation algorithms have been developed. However, to date, only Miniature Simulations is capable of efficiently generating MRCs for popular eviction policies, such as Least Frequently Used (LFU), First-In-First-Out (FIFO), 2Q, and Least Recently/Frequently Used (LRFU), that do not adhere to the inclusion property. One critical downside of Miniature Simulations is that it incurs significant memory overhead, precluding its use for online cache analysis at runtime in many cases.\nIn this paper, we introduce Kosmo, an MRC generation algorithm that allows for the simultaneous generation of MRCs for a variety of eviction policies that do not adhere to the inclusion property. We evaluate Kosmo using 52 publicly-accessible cache access traces with a total of roughly 126 billion accesses. Compared to Miniature Simulations configured with 100 simulated caches, Kosmo has lower memory overhead by a factor of 3.6 on average, and as high as 36, and a higher throughput by a factor of 1.3 making it far more suitable for online MRC generation.",
      "htmlFile": "2024/b8d7f6e5a4c32024kosmoef/index.html"
    },
    {
      "id": "b8f0e0b7e6a4",
      "title": "LavaStore: ByteDance’s Purpose-built, High-performance, Cost-effective Local Storage Engine for Cloud Services",
      "authors": [
        "Hao Wang",
        "Jiaxin Ou",
        "Ming Zhao",
        "Sheng Qiu",
        "Yizheng Jiao",
        "Yi Wang",
        "Qizhong Mao",
        "Zhengyu Yang",
        "Yang Liu",
        "Jianshun Zhang",
        "Jianyang Hu",
        "Jingwei Zhang",
        "Jinrui Liu",
        "Jiaqiang Chen",
        "Yong Shen",
        "Lixun Cao",
        "Heng Zhang",
        "Hongde Li",
        "Ming Li",
        "Yue Ma",
        "Lei Zhang",
        "Jian Liu",
        "Guanghui Zhang",
        "Fei Liu",
        "Jianjun Chen"
      ],
      "year": 2024,
      "conference": "PVLDB",
      "category": "数据库系统",
      "keywords": [
        "键值存储",
        "LSM-tree",
        "RocksDB",
        "KV分离",
        "垃圾回收",
        "写性能优化",
        "LavaStore"
      ],
      "abstract": "Persistent key-value (KV) stores are widely used by cloud services at ByteDance as local storage engines, and RocksDB used to be the _de facto_ implementation since it can be tailored to a variety of workloads and requirements. In this paper, we provide key insights into local storage engine usage at ByteDance, explain why the combination of highly write-intensive workloads and stringent requirements on cost efficiency and point lookup tail latency may pose challenges to a general-purpose local storage engine such as RocksDB, and present the design and implementation of _LavaStore_, a high-performance cost-effective local storage engine purpose-built to address these challenges.\n\nLavaStore achieves its design goals by selectively customizing a few components of a RocksDB-based, general-purpose local storage engine, including a distinct KV separation design that decouples garbage collection from compaction, a specialized engine type for the commonly recurring Write-Ahead-Logging workload, and a customized user-space append-only filesystem. LavaStore has been deployed to production with hundreds of thousands of running instances, storing more than 100 PB of data and serving billions of requests per second, bringing significant performance improvements and cost reductions to customers over their original local storage engines. For example, a ByteDance proprietary distributed OLTP database service has experienced a reduction in average write and read latency by 61% and 16%, respectively, and a ByteDance proprietary caching service has gained an 87% increase in write throughput with no more than 6% space overhead.",
      "htmlFile": "2024/b8f0e0b7e6a4/index.html"
    },
    {
      "id": "b9d2f6a4c8e1",
      "title": "Property-Based Testing in Practice",
      "authors": [
        "Harrison Goldstein",
        "Joseph W. Cutler",
        "Daniel Dickstein",
        "Benjamin C. Pierce",
        "Andrew Head"
      ],
      "year": 2024,
      "conference": "ICSE",
      "category": "软件工程",
      "keywords": [
        "基于属性的测试",
        "实证研究",
        "随机测试",
        "测试生成",
        "软件质量"
      ],
      "abstract": "Property-based testing (PBT) is a testing methodology where users write executable formal specifications of software components and an automated harness checks these specifications against many automatically generated inputs. From its roots in the QuickCheck library in Haskell, PBT has made significant inroads in mainstream languages and industrial practice at companies such as Amazon, Volvo, and Stripe. As PBT extends its reach, it is important to understand how developers are using it in practice, where they see its strengths and weaknesses, and what innovations are needed to make it more effective.\nWe address these questions using data from 30 in-depth interviews with experienced users of PBT at Jane Street, a financial technology company making heavy and sophisticated use of PBT. These interviews provide empirical evidence that PBT’s main strengths lie in testing complex code and in increasing confidence beyond what is available through conventional testing methodologies, and, moreover, that most uses fall into a relatively small number of high-leverage idioms. Its main weaknesses, on the other hand, lie in the relative complexity of writing properties and random data generators and in the difficulty of evaluating their effectiveness. From these observations, we identify a number of potentially high-impact areas for future exploration, including performance improvements, differential testing, additional high-leverage testing scenarios, better techniques for generating random input data, test-case reduction, and methods for evaluating the effectiveness of tests.",
      "htmlFile": "2024/b9d2f6a4c8e1/index.html"
    },
    {
      "id": "c4b7e9b4c8d7",
      "title": "Vexless: A Serverless Vector Data Management System Using Cloud Functions",
      "authors": [
        "Yongye Su",
        "Yinqi Sun",
        "Minjia Zhang",
        "Jianguo Wang"
      ],
      "year": 2024,
      "conference": "SIGMOD",
      "category": "数据库系统",
      "keywords": [
        "向量数据库",
        "Serverless",
        "Cloud Functions",
        "近似最近邻搜索",
        "成本优化"
      ],
      "abstract": "Cloud functions, exemplified by AWS Lambda and Azure Functions, are emerging as a new computing paradigm in the cloud. They provide elastic, serverless, and low-cost cloud computing, making them highly suitable for bursty and sparse workloads, which are quite common in practice. Thus, there is a new trend in designing data systems that leverage cloud functions. In this paper, we focus on vector databases, which have recently gained significant attention partly due to large language models. In particular, we investigate how to use cloud functions to build high-performance and cost-efficient vector databases. This presents significant challenges in terms of how to perform sharding, how to reduce communication overhead, and how to minimize cold-start times.\nIn this paper, we introduce Vexless, the first vector database system optimized for cloud functions. We present three optimizations to address the challenges. To perform sharding, we propose a global coordinator (orchestrator) that assigns workloads to Cloud function instances based on their available hardware resources. To overcome communication overhead, we propose the use of stateful cloud functions, eliminating the need for costly communications during synchronization. To minimize cold-start overhead, we introduce a workload-aware Cloud function lifetime management strategy. Vexless has been implemented using Azure Functions. Experimental results demonstrate that Vexless can significantly reduce costs, especially on bursty and sparse workloads, compared to cloud VM instances, while achieving similar or higher query performance and accuracy.",
      "htmlFile": "2024/c4b7e9b4c8d7/index.html"
    },
    {
      "id": "c7a0c6a5c9d4",
      "title": "PALF: Replicated Write-Ahead Logging for Distributed Databases",
      "authors": [
        "Fusheng Han",
        "Hao Liu",
        "Bin Chen",
        "Debin Jia",
        "Jianfeng Zhou",
        "Xuwang Teng",
        "Chuanhui Yang",
        "Huafeng Xi",
        "Wei Tian",
        "Shuning Tao",
        "Sen Wang",
        "Quanqing Xu",
        "Zhenkun Yang"
      ],
      "year": 2024,
      "conference": "PVLDB",
      "category": "分布式数据库",
      "keywords": [
        "PALF",
        "复制日志",
        "分布式数据库",
        "Paxos协议",
        "OceanBase",
        "WAL",
        "共识算法"
      ],
      "abstract": "Distributed databases have been widely researched and developed in recent years due to their scalability, availability, and consistency guarantees. The write-ahead logging (WAL) system is one of the most vital components in a database. It is still a non-trivial problem to design a replicated logging system as the foundation of a distributed database with the power of ACID transactions. This paper proposes PALF, a Paxos-backed Append-only Log File System, to address these challenges. The basic idea behind PALF is to co-design the logging system with the entire database for supporting database-specific functions and to abstract the functions as PALF primitives to power other distributed systems. Many database functions, including transaction processing, database restore, and physical standby databases, have been built based on PALF primitives. Evaluation shows that PALF greatly outperforms well-known implementations of consensus protocols and is fully competent for distributed database workloads. PALF has been deployed as a component of the OceanBase 4.0 database and has been made open-source along with it.",
      "htmlFile": "2024/c7a0c6a5c9d4/index.html"
    },
    {
      "id": "c9c4e6b5c8b1",
      "title": "A Mess of Memory System Benchmarking, Simulation and Application Profiling",
      "authors": [
        "Pouya Esmaili-Dokht",
        "Francesco Sgherzi",
        "Valéria Soldera Girelli",
        "Isaac Boixaderas",
        "Mariana Carmin",
        "Alireza Monemi",
        "Adrià Armejach",
        "Estanislao Mercadal",
        "Germán Llort",
        "Petar Radojković",
        "Miquel Moreto",
        "Judit Giménez",
        "Xavier Martorell",
        "Eduard Ayguadé",
        "Jesus Labarta",
        "Emanuele Confalonieri",
        "Rishabh Dubey",
        "Jason Adlard"
      ],
      "year": 2024,
      "conference": "MICRO",
      "category": "计算机体系结构",
      "keywords": [
        "内存系统",
        "基准测试",
        "模拟",
        "性能分析",
        "带宽-延迟曲线",
        "CXL",
        "HBM"
      ],
      "abstract": "The Memory stress (Mess) framework provides a unified view of the memory system benchmarking, simulation and application profiling.\nThe Mess benchmark provides a holistic and detailed memory system characterization. It is based on hundreds of measurements that are represented as a family of bandwidth–latency curves. The benchmark increases the coverage of all the previous tools and leads to new findings in the behavior of the actual and simulated memory systems. We deploy the Mess benchmark to characterize Intel, AMD, IBM, Fujitsu, Amazon and NVIDIA servers with DDR4, DDR5, HBM2 and HBM2E memory. The Mess memory simulator uses bandwidth–latency concept for the memory performance simulation. We integrate Mess with widely-used CPUs simulators enabling modeling of all high-end memory technologies. The Mess simulator is fast, easy to integrate and it closely matches the actual system performance. By design, it enables a quick adoption of new memory technologies in hardware simulators. Finally, the Mess application profiling positions the application in the bandwidth–latency space of the target memory system. This information can be correlated with other application runtime activities and the source code, leading to a better overall understanding of the application’s behavior.",
      "htmlFile": "2024/c9c4e6b5c8b1/index.html"
    },
    {
      "id": "e92b8d4a2f1c2024Dragon",
      "title": "The Dragon Hatching: The Missing Link between the Transformer and Models of the Brain",
      "authors": [
        "Adrian Kosowski",
        "Przemyslaw Uznanski",
        "Jan Chorowski",
        "Zuzanna Stamirowska",
        "Michal Bartoszkiewicz"
      ],
      "year": 2024,
      "conference": "arXiv",
      "category": "人工智能",
      "keywords": [
        "Transformer",
        "脑科学模型",
        "可扩展AI",
        "可解释性",
        "注意力机制",
        "尺度自由网络",
        "赫布学习"
      ],
      "abstract": "The relationship between computing systems and the brain has served as motivation for pioneering theoreticians since John von Neumann and Alan Turing. Uniform, scale-free biological networks, such as the brain, have powerful properties, including generalizing over time, which is the main barrier for Machine Learning on the path to Universal Reasoning Models.\n\nWe introduce 'Dragon Hatching' (BDH), a new Large Language Model architecture based on a scale-free biologically inspired network of n locally-interacting neuron particles. BDH couples strong theoretical foundations and inherent interpretability without sacrificing Transformer-like performance.\n\nBDH is a practical, performant state-of-the-art attention-based state space sequence learning architecture. In addition to being a graph model, BDH admits a GPU-friendly formulation. It exhibits Transformer-like scaling laws: we find empirically that BDH rivals GPT2-architecture Transformer performance on language and translation tasks, at the same number of parameters (10M to 1B), for the same training data.\n\nBDH provides theoretical foundations for understanding model behavior in the limit of large size and reasoning time. Our results, formalized as a chain of reductions of expressiveness in the framework of computational Complexity Theory and Distributed Computing, and combined with findings on the BDH model, show a macro-to-micro correspondence of function between the general attention mechanisms in state-of-the-art Language Models, and attention mechanisms observed in the brain. These attention mechanisms formally converge as closed-form local graph dynamics at neurons and synapses: \"the equations of reasoning\".\n\nBDH can be represented as a brain model. It contains n neurons, organized as an excitatory circuit and an inhibitory circuit with integrate-and-fire thresholding of input signals at neurons. The working memory of BDH during inference entirely relies on synaptic plasticity with Hebbian learning using spiking neurons, at potentiation scales of minutes for the brain (up to hundreds of tokens). We confirm empirically that specific, individual synapses strengthen connection whenever BDH hears or reasons about a specific concept while processing language inputs. The neuron interaction network of BDH is a graph of high modularity with heavy-tailed degree distribution. The BDH model is biologically plausible, explaining one possible mechanism which human neurons could use to achieve speech.\n\nBDH is designed for interpretability. Activation vectors of BDH are sparse and positive. We demonstrate monosemanticity in BDH on language tasks, including representation of concept abstractions, which happens even for small models, below 100M-parameter scale. Interpretability of state, which goes beyond interpretability of neurons and model parameters, is an inherent feature of the BDH architecture.\n\nWe believe BDH opens the door to a new theory of \"Thermodynamic Limit\" behavior for language and reasoning models, with the ultimate goal of Probably Approximately Correct (PAC)-like bounds for generalization of reasoning over time.",
      "htmlFile": "2024/e92b8d4a2f1c2024Dragon/index.html"
    },
    {
      "id": "f6e0b9d8c4a7",
      "title": "Incremental Bidirectional Typing via Order Maintenance",
      "authors": [
        "Thomas J. Porter",
        "Marisa Kirisame",
        "Ivan Wei",
        "Pavel Panchekha",
        "Cyrus Omar"
      ],
      "year": 2024,
      "conference": "POPL",
      "category": "编程语言",
      "keywords": [
        "增量类型检查",
        "双向类型系统",
        "实时编程环境",
        "顺序维护数据结构",
        "标记λ演算",
        "错误定位",
        "程序编辑"
      ],
      "abstract": "Live programming environments provide various semantic services, including type checking and evaluation, continuously as the user is editing the program. The live paradigm promises to improve the developer experience, but liveness is an implementation challenge particularly when working with large programs. This paper specifies and efficiently implements a system the is able to incrementally update type information for a live program in response to fine-grained program edits. This information includes type error marks and information about the expected and actual type on every expression. The system is specified type-theoretically as a small-step dynamics that propagates updates through the marked and annotated program. Most updates flow according to a base bidirectional type system. Additional pointers are maintained to connect bound variables to their binding locations, with type updates traversing these pointers directly. Order maintenance data structures are employed to efficiently maintain these pointers and to prioritize the order of update propagation. We prove this system is equivalent to naive reanalysis in the Agda theorem prover, along with other important metatheoretic properties. We then provide an efficient OCaml implementation, detailing a number of impactful optimizations. We evaluate this implementation’s performance with a large stress-test and find that it is able to achieve dramatic speed-ups of 275.96× compared to from-scratch reanalysis.",
      "htmlFile": "2024/f6e0b9d8c4a7/index.html"
    },
    {
      "id": "f8a9b2c1d3e52024Seraph",
      "title": "Seraph: Towards Scalable and Efficient Fully-external Graph Computation via On-demand Processing",
      "authors": [
        "Tsun-Yu Yang",
        "Yizou Chen",
        "Yuhong Liang",
        "Ming-Chang Yang"
      ],
      "year": 2024,
      "conference": "FAST",
      "category": "存储系统",
      "keywords": [
        "图计算",
        "外存计算",
        "按需处理",
        "可扩展性",
        "效率优化",
        "混合格式",
        "顶点传递"
      ],
      "abstract": "Fully-external graph computation systems exhibit optimal scalability by computing the ever-growing, large-scale graph with constant amount of memory on a single machine. In particular, they keep the entire massive graph data in storage and iteratively load parts of them into memory for computation. Nevertheless, despite the merit of optimal scalability, their unreasonably-low efficiency often makes them uncompetitive, and even unpractical, to the other types of graph computation systems. The key rationale is that most existing fully-external graph computation systems over-emphasize retrieving graph data from storage through sequential access. Although this principle achieves high storage bandwidth, it often causes reading excessive and irrelevant data, which can severely degrade their overall efficiency.\nTherefore, this work presents Seraph, a fully-external graph computation system that achieves optimal Scalability while toward satisfactory Efficiency improvement. Particularly, inspired by the modern storage offering comparable sequential and random access speeds, Seraph adopts the principle of _on-demand processing_ to access the necessary graph data for saving I/O while enjoying the decent speed in random access. On the basis of this principle, Seraph further devises three practical designs to bring excellent performance leap to fully-external graph computation: 1) the hybrid format to represent the graph data for striking a good balance between I/O amount and access locality, 2) the vertex passing to enable efficient vertex updates on top of hybrid format, and 3) the selective pre-computation to re-use the loaded data for I/O reduction. Our evaluations reveal that Seraph notably outperforms other state-of-the-art fully-external systems under all the evaluated billion-scale graphs and representative graph algorithms by up to two orders of magnitude.",
      "htmlFile": "2024/f8a9b2c1d3e52024Seraph/index.html"
    },
    {
      "id": "f8c5b9d6d9b8",
      "title": "Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning",
      "authors": [
        "Wei An",
        "Xiao Bi",
        "Guanting Chen",
        "Shanhuang Chen",
        "Chengqi Deng",
        "Honghui Ding",
        "Kai Dong",
        "Qiushi Du",
        "Wenjun Gao",
        "Kang Guan",
        "Jianzhong Guo",
        "Yongqiang Guo",
        "Zhe Fu",
        "Ying He",
        "Panpan Huang",
        "Jiashi Li",
        "Wenfeng Liang",
        "Xiaodong Liu",
        "Xin Liu",
        "Yiyuan Liu",
        "Yuxuan Liu",
        "Shanghao Lu",
        "Xuan Lu",
        "Xiaotao Nie",
        "Tian Pei",
        "Junjie Qiu",
        "Hui Qu",
        "Zehui Ren",
        "Zhangli Sha",
        "Xuecheng Su",
        "Xiaowen Sun",
        "Yixuan Tan",
        "Minghui Tang",
        "Shiyu Wang",
        "Yaohui Wang",
        "Yongji Wang",
        "Ziwei Xie",
        "Yiliang Xiong",
        "Yanhong Xu",
        "Shengfeng Ye",
        "Shuiping Yu",
        "Yukun Zha",
        "Liyue Zhang*",
        "Haowei Zhang",
        "Mingchuan Zhang",
        "Wentao Zhang",
        "Yichao Zhang",
        "Chenggang Zhao",
        "Yao Zhao",
        "Shangyan Zhou",
        "Shunfeng Zhou",
        "Yuheng Zou"
      ],
      "year": 2024,
      "conference": "arXiv",
      "category": "高性能计算",
      "keywords": [
        "高性能计算",
        "深度学习",
        "大语言模型",
        "AI Infra"
      ],
      "abstract": "The rapid progress in Deep Learning (DL) and Large Language Models (LLMs) has exponentially increased demands of computational power and bandwidth. This, combined with the high costs of faster computing chips and interconnects, has significantly inflated High Performance Computing (HPC) construction costs. To address these challenges, we introduce the Fire-Flyer AI-HPC architecture, a synergistic hardware-software co-design framework and its best practices. For DL training, we deployed the Fire-Flyer 2 with 10,000 PCIe A100 GPUs, achieved performance approximating the DGX-A100 while reducing costs by half and energy consumption by 40%. We specifically engineered IFFReduce to accelerate allreduce communication and implemented numerous measures to keep our Computation-Storage Integrated Network congestion-free. Through our software stack, including HaiScale, 3FS, and HAI-Platform, we achieved substantial scalability by overlapping computation and communication. Our system-oriented experience from DL training provides valuable insights to drive future advancements in AI-HPC.",
      "htmlFile": "2024/f8c5b9d6d9b8/index.html"
    },
    {
      "id": "a1b2c3d4e5f62023ALPAda",
      "title": "ALP: Adaptive Lossless floating-Point Compression",
      "authors": [
        "Azim Afroozeh",
        "Leonardo X. Kuffo",
        "Peter Boncz"
      ],
      "year": 2023,
      "conference": "SIGMOD",
      "category": "数据压缩",
      "keywords": [
        "无损压缩",
        "浮点数压缩",
        "轻量级压缩",
        "向量化执行",
        "列式存储",
        "自适应编码",
        "ALP"
      ],
      "abstract": "IEEE 754 doubles do not exactly represent most real values, introducing rounding errors in computations and [de]serialization to text. These rounding errors inhibit the use of existing lightweight compression schemes such as Delta and Frame Of Reference (FOR), but recently new schemes were proposed: Gorilla, Chimp128, PseudoDecimals (PDE), Elf and Patas. However, their compression ratios are not better than those of general-purpose compressors such as Zstd; while [de]compression is much slower than Delta and FOR. We propose and evaluate ALP, that significantly improves these previous schemes in both speed and compression ratio (Figure 1). We created ALP after carefully studying the datasets used to evaluate the previous schemes. To obtain speed, ALP is designed to fit _vectorized execution_. This turned out to be key for also improving the compression ratio, as we found in-vector commonalities to create compression opportunities. ALP is an adaptive scheme that uses a strongly enhanced version of PseudoDecimals [31] to losslessly encode doubles as integers if they originated as decimals, and otherwise uses vectorized compression of the doubles' front bits. Its high speeds stem from our implementation in scalar code that auto-vectorizes, using building blocks provided by our FastLanes library [6], and an efficient two-stage compression algorithm that first samples row-groups and then vectors.",
      "htmlFile": "2023/a1b2c3d4e5f62023ALPada/index.html"
    },
    {
      "id": "a1b2c3d4e5f62023Learned ",
      "title": "Learned Index: A Comprehensive Experimental Evaluation",
      "authors": [
        "Zhaoyan Sun",
        "Xuanhe Zhou",
        "Guoliang Li"
      ],
      "year": 2023,
      "conference": "PVLDB",
      "category": "数据库系统",
      "keywords": [
        "学习索引",
        "机器学习模型",
        "性能评估",
        "查询处理",
        "索引结构",
        "实验框架",
        "并发操作"
      ],
      "abstract": "Indexes can improve query-processing performance by avoiding full table scans. Although traditional indexes (e.g., B+-tree) have been widely used, learned indexes are proposed to adopt machine learning models to reduce the query latency and index size. However, existing learned indexes are (1) not thoroughly evaluated under the same experimental framework and are (2) not comprehensively compared with different settings (e.g., key lookup, key insert, concurrent operations, bulk loading). Moreover, it is hard to select appropriate learned indexes for practitioners in different settings. To address those problems, this paper detailedly reviews existing learned indexes and discusses the design choices of key components in learned indexes, including key lookup (position inference which predicts the position of a key, and position refinement which re-searches the position if the predicted position is incorrect), key insert, concurrency, and bulk loading. Moreover, we provide a testbed to facilitate the design and test of new learned indexes for researchers. We compare state-of-the-art learned indexes in the same experimental framework, and provide findings to select suitable learned indexes under various practical scenarios.",
      "htmlFile": "2023/a1b2c3d4e5f62023Learned /index.html"
    },
    {
      "id": "a5f4b3e2c1d0",
      "title": "An Empirical Evaluation of Columnar Storage Formats",
      "authors": [
        "Xinyu Zeng",
        "Yulong Hui",
        "Jiahong Shen",
        "Andrew Pavlo",
        "Wes McKinney",
        "Huanchen Zhang"
      ],
      "year": 2023,
      "conference": "PVLDB",
      "category": "数据库系统",
      "keywords": [
        "列式存储",
        "Parquet",
        "ORC",
        "性能评估",
        "数据压缩",
        "机器学习工作负载",
        "GPU解码"
      ],
      "abstract": "Columnar storage is a core component of a modern data analytics system. Although many database management systems (DBMSs) have proprietary storage formats, most provide extensive support to open-source storage formats such as Parquet and ORC to facilitate cross-platform data sharing. But these formats were developed over a decade ago, in the early 2010s, for the Hadoop ecosystem. Since then, both the hardware and workload landscapes have changed.\nIn this paper, we revisit the most widely adopted open-source columnar storage formats (Parquet and ORC) with a deep dive into their internals. We designed a benchmark to stress-test the formats’ performance and space efficiency under different workload configurations. From our comprehensive evaluation of Parquet and ORC, we identify design decisions advantageous with modern hardware and real-world data distributions. These include using dictionary encoding by default, favoring decoding speed over compression ratio for integer encoding algorithms, making block compression optional, and embedding finer-grained auxiliary data structures. We also point out the inefficiencies in the format designs when handling common machine learning workloads and using GPUs for decoding. Our analysis identified important considerations that may guide future formats to better fit modern technology trends.",
      "htmlFile": "2023/a5f4b3e2c1d0/index.html"
    },
    {
      "id": "a7c3e9b1f2042023FastLane",
      "title": "The FastLanes Compression Layout: Decoding >100 Billion Integers per Second with Scalar Code",
      "authors": [
        "Azim Afroozeh",
        "Peter Boncz"
      ],
      "year": 2023,
      "conference": "PVLDB",
      "category": "数据库系统",
      "keywords": [
        "数据压缩",
        "SIMD",
        "列式存储",
        "向量化执行",
        "轻量级压缩",
        "位打包",
        "解码性能"
      ],
      "abstract": "The open-source FastLanes project aims to improve big data formats, such as Parquet, ORC and columnar database formats, in multiple ways. In this paper, we significantly accelerate decoding of all common Light-Weight Compression (LWC) schemes: DICT, FOR, DELTA and RLE through better data-parallelism. We do so by re-designing the compression layout using two main ideas: (i) generalizing the value interleaving technique in the basic operation of bit-(un)packing by targeting a virtual 1024-bits SIMD register, (ii) reordering the tuples in all columns of a table in the same Unified Transposed Layout that puts tuple chunks in a common \"04261537\" order (explained in the paper); allowing for maximum independent work for all possible basic SIMD lane widths: 8, 16, 32, and 64 bits.\n\nWe address the software development, maintenance and future-proofness challenges of increasing hardware diversity, by defining a virtual 1024-bits instruction set that consists of simple operators supported by all SIMD dialects; and also, importantly, by scalar code. The interleaved and tuple-reordered layout actually makes scalar decoding faster, extracting more data-parallelism from today's wide-issue CPUs. Importantly, the scalar version can be fully auto-vectorized by modern compilers, eliminating technical debt in software caused by platform-specific SIMD intrinsics.\n\nMicro-benchmarks on Intel, AMD, Apple and AWS CPUs show that FastLanes accelerates decoding by factors (decoding >40 values per CPU cycle). FastLanes can make queries faster, as compressing the data reduces bandwidth needs, while decoding is almost free.",
      "htmlFile": "2023/a7c3e9b1f2042023FastLane/index.html"
    },
    {
      "id": "a7c3e9b8f12d",
      "title": "Binary Fuse Filters: Fast and Smaller Than Xor Filters",
      "authors": [
        "Thomas Mueller Graf",
        "Daniel Lemire"
      ],
      "year": 2023,
      "conference": "arXiv",
      "category": "数据结构与算法",
      "keywords": [
        "概率过滤器",
        "二进制融合过滤器",
        "XOR过滤器",
        "空间效率",
        "查询性能",
        "构造时间",
        "指纹技术"
      ],
      "abstract": "我们提出了二进制融合过滤器，这是一种新的概率过滤器，比XOR过滤器更快、更小。二进制融合过滤器在空间上仅比理论下界高出约12%（XOR过滤器为23%），同时保持了同样良好的查询时间。二进制融合过滤器还具有更快的构造速度，在某些情况下构造速度可以是XOR过滤器的两倍。此外，我们还引入了4-wise二进制融合过滤器，进一步将空间开销降至约8%，同时查询时间比XOR过滤器增加约30%。实验表明，二进制融合过滤器在构造时间、空间效率和查询性能方面均优于多种现有过滤器。",
      "htmlFile": "2023/a7c3e9b8f12d/index.html"
    },
    {
      "id": "b3c6b9a5e7a9",
      "title": "The Cambridge Report on Database Research",
      "authors": [
        "Anastasia Ailamaki",
        "Samuel Madden",
        "Daniel Abadi",
        "Gustavo Alonso",
        "Shiem Amer-Yahia",
        "Magdalena Balazinska",
        "Philip A. Bernstein",
        "Peter Boncz",
        "Michael Cafarella",
        "Surajit Chaudhuri",
        "Susan Davidson",
        "David DeWitt",
        "Yanlei Diao",
        "Xin Luna Dong",
        "Michael Franklin",
        "Juliana Freire",
        "Johannes Gehrke",
        "Alon Halevy",
        "Joseph M. Hellerstein",
        "Mark D. Hill",
        "Stratos Idreos",
        "Yannis Ioannidis",
        "Christoph Koch",
        "Donald Kossmann",
        "Tim Kraska",
        "Arun Kumar",
        "Guoliang Li",
        "Volker Markl",
        "Renee Miller",
        "C. Mohan",
        "Thomas Neumann",
        "Beng Chin Ooi",
        "Fatma Ozcan",
        "Aditya Parameswaran",
        "Ippokratis Pandis",
        "Jignesh M. Patel",
        "Andrew Pavlo",
        "Danica Porobic",
        "Viktor Sanca",
        "Michael Stonebraker",
        "Julia Stoyanovich",
        "Dan Suciu",
        "Wang-Chiew Tan",
        "Shiv Venkataraman",
        "Matei Zaharia",
        "Stanley B. Zdonik"
      ],
      "year": 2023,
      "conference": "SIGMOD",
      "category": "数据库系统",
      "keywords": [
        "数据库研究",
        "云计算",
        "大语言模型",
        "数据治理",
        "硬件加速",
        "数据科学",
        "负责任数据管理"
      ],
      "abstract": "On October 19-20, 2023, the authors of this report convened in Cambridge, MA, to discuss the state of the database research field1, its recent accomplishments and ongoing challenges, and future directions for research and community engagement. This gathering continues a long-standing tradition in the database community, dating back to the late 1980s, in which researchers meet roughly every five years to produce a forward-looking report [1, 2, 3, 4, 5, 6, 7, 8, 9].\nFootnote 1: Broadly defined as the community of researchers publishing in ACM SIGMOD, VLDB, and related conferences, journals, and workshops.\nThis report summarizes the key takeaways from our discussions. We begin with a retrospective on the community's academic, open-source, and commercial successes over the past five years. We then turn to future opportunities, with a focus on core data systems-particularly in the context of cloud computing and emerging hardware-as well as on the growing impact of data science, data governance, and generative AI.\nThis document is not intended as an exhaustive survey of all technical challenges or industry innovations in the field. Rather, it reflects the perspectives of senior community members on the most pressing challenges and promising opportunities ahead.",
      "htmlFile": "2023/b3c6b9a5e7a9/index.html"
    },
    {
      "id": "b5c5e3c6b0a5",
      "title": "Generative Agents: Interactive Simulacra of Human Behavior",
      "authors": [
        "Joon Sung Park",
        "Joseph C. O'Brien",
        "Carrie J. Cai",
        "Meredith Ringel Morris",
        "Percy Liang",
        "Michael S. Bernstein"
      ],
      "year": 2023,
      "conference": "UIST",
      "category": "人机交互",
      "keywords": [
        "生成式智能体",
        "人类行为模拟",
        "大型语言模型",
        "记忆架构",
        "社会模拟",
        "人机交互",
        "可信代理"
      ],
      "abstract": "Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent’s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine’s Day party, the agents autonomously spread invitations to the party over the next two\ndays, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.",
      "htmlFile": "2023/b5c5e3c6b0a5/index.html"
    },
    {
      "id": "b7f2e9d41c6a2023Perseus",
      "title": "Perseus: A Fail-Slow Detection Framework for Cloud Storage Systems",
      "authors": [
        "Ruiming Lu",
        "Erci Xu",
        "Yiming Zhang",
        "Fengyi Zhu",
        "Zhaosheng Zhu",
        "Mengtian Wang",
        "Zongpeng Zhu",
        "Guangtao Xue",
        "Jiwu Shu",
        "Minglu Li",
        "Jiesheng Wu"
      ],
      "year": 2023,
      "conference": "FAST",
      "category": "存储系统",
      "keywords": [
        "故障检测",
        "fail-slow",
        "云存储",
        "性能诊断",
        "机器学习",
        "存储设备",
        "根因分析"
      ],
      "abstract": "The newly-emerging \"fail-slow\" failures plague both software and hardware where the victim components are still functioning yet with degraded performance. To address this problem, this paper presents Perseus, a practical fail-slow detection framework for storage devices. Perseus leverages a light regression-based model to fast pinpoint and analyze fail-slow failures at the granularity of drives. Within a 10-month close monitoring on 248K drives, Perseus managed to find 304 fail-slow cases. Isolating them can reduce the (node-level) 99.99th tail latency by 48%. We assemble a large-scale fail-slow dataset (including 41K normal drives and 315 verified fail-slow drives) from our production traces, based on which we provide root cause analysis on fail-slow drives covering a variety of ill-implemented scheduling, hardware defects, and environmental factors. We have released the dataset to the public for fail-slow study.",
      "htmlFile": "2023/b7f2e9d41c6a2023Perseus/index.html"
    },
    {
      "id": "b8245f7a9c3e",
      "title": "Efficiency in the Serverless Cloud Paradigm: A Survey on the Reusing and Approximation Aspects",
      "authors": [
        "Chavit Denninnart",
        "Thanawat Chanikaphon",
        "Mohsen Amini Salehi"
      ],
      "year": 2023,
      "conference": "arXiv",
      "category": "分布式系统",
      "keywords": [
        "无服务器计算",
        "FaaS",
        "计算重用",
        "近似计算",
        "云计算",
        "资源效率",
        "云原生"
      ],
      "abstract": "Serverless computing along with Function-as-a-Service (FaaS is forming a new computing paradigm that is anticipated to found the next generation of cloud systems.The popularity of this paradigm is due to offering a highly transparent infrastructure that enables user applications to scale in the granularity of their functions.Since these often small and single-purpose functions are managed on shared computing resources behind the scene,a great potential for computational reuse and approximate computing emerges that if unleashed,can remarkably improve the efficiency of serverless cloud systems—both from the user's QoS and system's (energy consumption and incurred cost)perspectives.Accordingly,the goal of this survey study is to,first,unfold the internal mechanics of serverless computing and,second,explore the scope for efficiency within this paradigm via studying function reuse and approximation approaches and discussing the pros and cons of each one.Next,we outline potential future research directions within this paradigm that can either unlock new use cases or make the paradigm more",
      "htmlFile": "2023/b8245f7a9c3e/index.html"
    },
    {
      "id": "b8a8e9c8c9b5",
      "title": "Gorilla: Large Language Model Connected with Massive APIs",
      "authors": [
        "Shishir G. Patil",
        "Tianjun Zhang",
        "Xin Wang",
        "Joseph E. Gonzalez"
      ],
      "year": 2023,
      "conference": "arXiv",
      "category": "自然语言处理",
      "keywords": [
        "大语言模型",
        "API调用",
        "检索增强",
        "工具使用",
        "幻觉减少",
        "微调",
        "APIBench"
      ],
      "abstract": "Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla's code, model, data, and demo are available at https://gorilla.cs.berkeley.edu",
      "htmlFile": "2023/b8a8e9c8c9b5/index.html"
    },
    {
      "id": "b9c5f8b8b9e3",
      "title": "B-Trees Are Back: Engineering Fast and Pageable Node Layouts",
      "authors": [
        "Marcus Muller",
        "Lawrence Benson",
        "Viktor Leis"
      ],
      "year": 2023,
      "conference": "SIGMOD",
      "category": "数据库系统",
      "keywords": [
        "B-Tree",
        "索引结构",
        "内存数据库",
        "存储引擎"
      ],
      "abstract": "Large main memory capacity and even larger data sets have motivated hybrid storage systems, which serve most transactions from memory, but can seamlessly transition to flash storage. In such systems, the data structure of choice is usually a B-Tree with pageable nodes. Most academic B-Tree work considers only fixed size records, making them unsuitable for most practical applications. Given the prevalence of B-Trees, surprisingly few available implementations and benchmarks of optimized B-Trees cover variable-sized records. In this paper, we describe an efficient B-Tree implementation supporting variable-sized records containing six known node layout optimizations. We evaluate each optimization to guide future implementations, and propose an optimized adaptive layout that can even compete with pure in-memory structures for many workloads. Our results show that well-engineered B-Trees can efficiently handle both in-memory and out-of-memory workloads.",
      "htmlFile": "2023/b9c5f8b8b9e3/index.html"
    },
    {
      "id": "c5a9d1b4c3a2",
      "title": "Virtual-Memory Assisted Buffer Management",
      "authors": [
        "Viktor Leis",
        "Adnan Alhomssi",
        "Tobias Ziegler",
        "Yannick Loeck",
        "Christian Dietrich"
      ],
      "year": 2023,
      "conference": "SIGMOD",
      "category": "数据库系统",
      "keywords": [
        "数据库管理系统",
        "操作系统",
        "缓存",
        "缓冲区管理",
        "虚拟内存"
      ],
      "abstract": "Most database management systems cache pages from storage in a main memory buffer pool. To do this, they either rely on a hash table that translates page identifiers into pointers, or on pointer swizzling which avoids this translation. In this work, we propose _vmcache_, a buffer manager design that instead uses hardware-supported virtual memory to translate page identifiers to virtual memory addresses. In contrast to existing mmap-based approaches, the DBMS retains control over page faulting and eviction. Our design is portable across modern operating systems, supports arbitrary graph data, enables variable-sized pages, and is easy to implement. One downside of relying on virtual memory is that with fast storage devices the existing operating system primitives for manipulating the page table can become a performance bottleneck. As a second contribution, we therefore propose _exmap_, which implements scalable page table manipulation on Linux. Together, vmcache and exmap provide flexible, efficient, and scalable buffer management on multi-core CPUs and fast storage devices.",
      "htmlFile": "2023/c5a9d1b4c3a2/index.html"
    },
    {
      "id": "b5c8d9e2f1a32022Spooky",
      "title": "Spooky: Granulating LSM-Tree Compactions Correctly",
      "authors": [
        "Niv Dayan",
        "Tamar Weiss",
        "Shmuel Dashevsky",
        "Michael Pan",
        "Edward Bortnikov",
        "Moshe Twitto"
      ],
      "year": 2022,
      "conference": "VLDB",
      "category": "数据库系统",
      "keywords": [
        "LSM-Tree",
        "Compaction",
        "Write Amplification",
        "Space Amplification",
        "Storage Utilization",
        "Garbage Collection",
        "Key-Value Stores"
      ],
      "abstract": "Existing methods for granulating LSM-tree compactions either waste most of the available storage capacity or involve exorbitant write-amplification. We introduce Spooky, the first approach to achieve high storage utilization and moderate write-amplification at the same time.",
      "htmlFile": "2022/b5c8d9e2f1a32022Spooky/index.html"
    },
    {
      "id": "b7e4a2c8198f2024XFuzz",
      "title": "XFuzz: Machine Learning Guided Cross-Contract Fuzzing",
      "authors": [
        "Yinxing Xue",
        "Jiaming Ye",
        "Wei Zhang",
        "Jun Sun",
        "Lei Ma",
        "Haijun Wang",
        "Jianjun Zhao"
      ],
      "year": 2022,
      "conference": "ICSE",
      "category": "软件安全",
      "keywords": [
        "智能合约",
        "模糊测试",
        "跨合约漏洞",
        "机器学习",
        "XFuzz",
        "Ethereum"
      ],
      "abstract": "Smart contract transactions are increasingly interleaved by cross-contract calls. While many tools have been developed to identify a common set of vulnerabilities, the cross-contract vulnerability is overlooked by existing tools. Cross-contract vulnerabilities are exploitable bugs that manifest in the presence of more than two interacting contracts. Existing methods are however limited to analyze a maximum of two contracts at the same time. Detecting cross-contract vulnerabilities is highly non-trivial. With multiple interacting contracts, the search space is much larger than that of a single contract. To address this problem, we present XFuzz, a machine learning guided smart contract fuzzing framework. The machine learning models are trained with novel features (e.g., word vectors and instructions) and are used to filter likely benign program paths. Comparing with existing static tools, machine learning model is proven to be more robust, avoiding directly adopting manually-defined rules in specific tools. We compare XFuzz with three state-of-the-art tools on 7,391 contracts. XFuzz detects 18 exploitable cross-contract vulnerabilities, of which 15 vulnerabilities are exposed for the first time. Furthermore, our approach is shown to be efficient in detecting non-cross-contract vulnerabilities as well--using less than 20% time as that of other fuzzing tools, XFuzz detects twice as many vulnerabilities.",
      "htmlFile": "2022/b7e4a2c8198f2024XFuzz/index.html"
    },
    {
      "id": "f7b5c9e12a8d2022dissecti",
      "title": "Dissecting Tensor Cores via Microbenchmarks: Latency, Throughput and Numeric Behaviors",
      "authors": [
        "Wei Sun",
        "Ang Li",
        "Tong Geng",
        "Sander Stujik",
        "Henk Corporaal"
      ],
      "year": 2022,
      "conference": "arXiv",
      "category": "高性能计算与计算机体系结构",
      "keywords": [
        "Tensor Cores",
        "微基准测试",
        "延迟",
        "吞吐量",
        "数值行为",
        "Ampere架构",
        "低精度浮点运算"
      ],
      "abstract": "Tensor Cores have been an important unit to accelerate Fused Matrix Multiplication Accumulation (MMA) in all NVIDIA GPUs since Volta Architecture. To program Tensor Cores, users have to use either legacy wmma APIs or current mma APIs. Legacy wmma APIs are more easy-to-use but can only exploit limited features and power of Tensor Cores. Specifically, wmma APIs support fewer operand shapes and can not leverage the new sparse matrix multiplication feature of the newest Ampere Tensor Cores. However, the performance of current programming interface has not been well explored. Furthermore, the computation numeric behaviors of low-precision floating points (TF32, BF16, and FP16) supported by the newest Ampere Tensor Cores are also mysterious. In this paper, we explore the throughput and latency of current programming APIs. We also intuitively study the numeric behaviors of Tensor Cores MMA and profile the intermediate operations including multiplication, addition of inner product, and accumulation. All codes used in this work can be found in https://github.com/sunlex0717/DissectingTensorCores.",
      "htmlFile": "2022/f7b5c9e12a8d2022dissecti/index.html"
    },
    {
      "id": "f8b4a12c6d9e2024efficient",
      "title": "Efficient Transformers: A Survey",
      "authors": [
        "Yi Tay",
        "Mostafa Dehghani",
        "Dara Bahri",
        "Donald Metzler"
      ],
      "year": 2022,
      "conference": "arXiv",
      "category": "机器学习",
      "keywords": [
        "深度学习",
        "自然语言处理",
        "Transformer模型",
        "注意力模型",
        "神经网络",
        "高效Transformer"
      ],
      "abstract": "Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of \"X-former\" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored \"X-former\" models, providing an organized and comprehensive overview of existing work and models across multiple domains.",
      "htmlFile": "2022/f8b4a12c6d9e2024efficient/index.html"
    },
    {
      "id": "5d2a2d2c5d2a",
      "title": "Efficient Stepping Algorithms and Implementations for Parallel Shortest Paths",
      "authors": [
        "Xiaojun Dong",
        "Yan Gu",
        "Yihan Sun",
        "Yunming Zhang"
      ],
      "year": 2021,
      "conference": "SPAA",
      "category": "图算法",
      "keywords": [
        "并行算法",
        "最短路径",
        "SSSP",
        "Δ-stepping",
        "ρ-stepping",
        "LaB-PQ",
        "单源最短路径"
      ],
      "abstract": "The single-source shortest-path (SSSP) problem is a notoriously hard problem in the parallel context. In practice, the Δ-stepping algorithm of Meyer and Sanders has been widely adopted. However, Δ-stepping has no known worst-case bounds for general graphs, and the performance highly relies on the parameter Δ, which requires exhaustive tuning. The parallel SSSP algorithms with provable bounds, such as Radius-stepping, either have no implementations available or are much slower than Δ-stepping in practice.\nWe propose the stepping algorithm framework that generalizes existing algorithms such as Δ-stepping and Radius-stepping. The framework allows for similar analysis and implementations for all stepping algorithms. We also propose a new abstract data type, lazy-batched priority queue (LAB-PQ) that abstracts the semantics of the priority queue needed by the stepping algorithms. We provide two data structures for LAB-PQ, focusing on theoretical and practical efficiency, respectively. Based on the new framework and LAB-PQ, we show two new stepping algorithms, ρ-stepping and Δ∗-stepping, that are simple, with non-trivial worst-case bounds, and fast in practice. We also show improved bounds for a list of existing algorithms such as Radius-Stepping.\nBased on our framework, we implement three algorithms: Bellman-Ford, Δ∗-stepping, and ρ-stepping. We compare the performance with four state-of-the-art implementations. On five social and web graphs, ρ-stepping is 1.3-2.6x faster than all the existing implementations. On two road graphs, our Δ∗-stepping is at least 14% faster than existing ones, while ρ-stepping is also competitive. The almost identical implementations for stepping algorithms also allow for in-depth analyses among the stepping algorithms in practice.",
      "htmlFile": "2021/5d2a2d2c5d2a/index.html"
    },
    {
      "id": "a7b2c4d8e9f12020umbraad",
      "title": "Umbra: A Disk-Based System with In-Memory Performance",
      "authors": [
        "Thomas Neumann",
        "Michael Freitag"
      ],
      "year": 2020,
      "conference": "CIDR",
      "category": "数据库系统",
      "keywords": [
        "数据库系统",
        "存储管理",
        "缓冲区管理",
        "可变量页",
        "SSD",
        "混合存储架构",
        "查询执行"
      ],
      "abstract": "The increases in main-memory sizes over the last decade have made pure in-memory database systems feasible, and in-memory systems offer unprecedented performance. However, DRAM is still relatively expensive, and the growth of main-memory sizes has slowed down. In contrast, the prices for SSDs have fallen substantially in the last years, and their read bandwidth has increased to gigabytes per second. This makes it attractive to combine a large in-memory buffer with fast SSDs as storage devices, combining the excellent performance for the in-memory working set with the scalability of a disk-based system.\n\nIn this paper we present the Umbra system, an evolution of the pure in-memory HyPer system towards a disk-based, or rather SSD-based, system. We show that by introducing a novel low-overhead buffer manager with variable-size pages we can achieve comparable performance to an in-memory database system for the cached working set, while handling accesses to uncached data gracefully. We discuss the changes and techniques that were necessary to handle the out-of-memory case gracefully and with low overhead, offering insights into the design of a memory optimized disk-based system.",
      "htmlFile": "2020/a7b2c4d8e9f12020umbraad/index.html"
    },
    {
      "id": "a7c3e9b4f12d",
      "title": "Partitioned Learned Bloom Filters",
      "authors": [
        "Kapil Vaidya",
        "Eric Knorr",
        "Tim Kraska",
        "Michael Mitzenmacher"
      ],
      "year": 2020,
      "conference": "arXiv",
      "category": "机器学习",
      "keywords": [
        "Learned Bloom Filter",
        "分区优化",
        "KL散度",
        "动态规划",
        "空间效率",
        "假阳性率"
      ],
      "abstract": "Bloom filters are space-efficient probabilistic data structures that are used to test whether an element is a member of a set, and may return false positives. Recently, variations referred to as learned Bloom filters were developed that can provide improved performance in terms of the rate of false positives, by using a learned model for the represented set. However, previous methods for learned Bloom filters do not take full advantage of the learned model. Here we show how to frame the problem of optimal model utilization as an optimization problem, and using our framework derive algorithms that can achieve near-optimal performance in many cases. Experimental results from both simulated and real-world datasets show significant performance improvements from our optimization approach over both the original learned Bloom filter constructions and previously proposed heuristic improvements.",
      "htmlFile": "2020/a7c3e9b4f12d/index.html"
    },
    {
      "id": "b3c6e4e7c5a5",
      "title": "Umbra: A Disk-Based System with In-Memory Performance",
      "authors": [
        "Thomas Neumann",
        "Michael Freitag"
      ],
      "year": 2020,
      "conference": "CIDR",
      "category": "数据库系统",
      "keywords": [
        "数据库系统",
        "缓冲管理",
        "可变大小页",
        "SSD",
        "查询执行",
        "内存性能",
        "磁盘存储"
      ],
      "abstract": "The increases in main-memory sizes over the last decade have made pure in-memory database systems feasible, and in-memory systems offer unprecedented performance. However, DRAM is still relatively expensive, and the growth of main-memory sizes has slowed down. In contrast, the prices for SSDs have fallen substantially in the last years, and their read bandwidth has increased to gigabytes per second. This makes it attractive to combine a large in-memory buffer with fast SSDs as storage devices, combining the excellent performance for the in-memory working set with the scalability of a disk-based system.\nIn this paper we present the Umbra system, an evolution of the pure in-memory HyPer system towards a disk-based, or rather SSD-based, system. We show that by introducing a novel low-overhead buffer manager with variable-size pages we can achieve comparable performance to an in-memory database system for the cached working set, while handling accesses to uncached data gracefully. We discuss the changes and techniques that were necessary to handle the out-of-memory case gracefully and with low overhead, offering insights into the design of a memory optimized disk-based system.",
      "htmlFile": "2020/b3c6e4e7c5a5/index.html"
    },
    {
      "id": "f7f9c4c0c8c2",
      "title": "Firecracker: Lightweight Virtualization for Serverless Applications",
      "authors": [
        "Alexandru Agache",
        "Marc Brooker",
        "Andreea Florescu",
        "Alexandra Iordache",
        "Anthony Liguori",
        "Rolf Neugebauer",
        "Phil Piwonka",
        "Diana-Maria Popa"
      ],
      "year": 2020,
      "conference": "NSDI",
      "category": "虚拟化与云计算",
      "keywords": [
        "Firecracker",
        "轻量级虚拟化",
        "无服务器计算",
        "微虚拟机",
        "安全隔离",
        "KVM",
        "容器"
      ],
      "abstract": "Serverless containers and functions are widely used for deploying and managing software in the cloud. Their popularity is due to reduced cost of operations, improved utilization of hardware, and faster scaling than traditional deployment methods. The economics and scale of serverless applications demand that workloads from multiple customers run on the same hardware with minimal overhead, while preserving strong security and performance isolation. The traditional view is that there is a choice between virtualization with strong security and high overhead, and container technologies with weaker security and minimal overhead. This tradeoff is unacceptable to public infrastructure providers, who need both strong security and minimal overhead. To meet this need, we developed Firecracker, a new open source Virtual Machine Monitor (VMM) specialized for serverless workloads, but generally useful for containers, functions and other compute workloads within a reasonable set of constraints. We have deployed Firecracker in two publically-available serverless compute services at Amazon Web Services (Lambda and Fargate), where it supports millions of production workloads, and trillions of requests per month. We describe how specializing for serverless informed the design of Firecracker, and what we learned from seamlessly migrating Lambda customers to Firecracker.",
      "htmlFile": "2020/f7f9c4c0c8c2/index.html"
    },
    {
      "id": "8b8e2c31c9a42019AutoShuf",
      "title": "AutoShuffleNet: Learning Permutation Matrices via an Exact Lipschitz Continuous Penalty in Deep Convolutional Neural Networks",
      "authors": [
        "Jiancheng Lyu",
        "Shuai Zhang",
        "Yingyong Qi",
        "Jack Xin"
      ],
      "year": 2019,
      "conference": "arXiv",
      "category": "计算机视觉",
      "keywords": [
        "AutoShuffleNet",
        "permutation learning",
        "Lipschitz continuous penalty",
        "channel shuffling",
        "ShuffleNet",
        "深度学习"
      ],
      "abstract": "ShuffleNet is a state-of-the-art light weight convolutional neural network architecture. Its basic operations include group, channel-wise convolution and channel shuffling. However, channel shuffling is manually designed empirically. Mathematically, shuffling is a multiplication by a permutation matrix. In this paper, we propose to automate channel shuffling by learning permutation matrices in network training. We introduce an exact Lipschitz continuous non-convex penalty so that it can be incorporated in the stochastic gradient descent to approximate permutation at high precision. Exact permutations are obtained by simple rounding at the end of training and are used in inference. The resulting network, referred to as AutoShuffleNet, achieved improved classification accuracies on CIFAR-10 and ImageNet data sets. In addition, we found experimentally that the standard convex relaxation of permutation matrices into stochastic matrices leads to poor performance. We prove theoretically the exactness (error bounds) in recovering permutation matrices when our penalty function is zero (very small). We present examples of permutation optimization through graph matching and two-layer neural network models where the loss functions are calculated in closed analytical form. In the examples, convex relaxation failed to capture permutations whereas our penalty succeeded.",
      "htmlFile": "2019/8b8e2c31c9a42019AutoShuf/index.html"
    },
    {
      "id": "a7c3e9b12f8d",
      "title": "Xor Filters: Faster and Smaller Than Bloom and Cuckoo Filters",
      "authors": [
        "Thomas Mueller Graf",
        "Daniel Lemire"
      ],
      "year": 2019,
      "conference": "CoNEXT",
      "category": "数据结构",
      "keywords": [
        "Xor过滤器",
        "Bloom过滤器",
        "Cuckoo过滤器",
        "近似集合成员",
        "空间优化",
        "性能比较",
        "静态集合"
      ],
      "abstract": "The Bloom filter provides fast approximate set membership while using little memory. Engineers often use these filters to avoid slow operations such as disk or network accesses. As an alternative, a cuckoo filter may need less space than a Bloom filter and it is faster. Chazelle et al. proposed a generalization of the Bloom filter called the Bloomier filter. Dietzfelbinger and Pagh described a variation on the Bloomier filter that can answer approximate membership queries over immutable sets. It has never been tested empirically, to our knowledge. We review an efficient implementation of their approach, which we call the xor filter. We find that xor filters can be faster than Bloom and cuckoo filters while using less memory. We further show that a more compact version of xor filters (xor+) can use even less space than highly compact alternatives (e.g., Golomb-compressed sequences) while providing speeds competitive with Bloom filters.",
      "htmlFile": "2019/a7c3e9b12f8d/index.html"
    },
    {
      "id": "f54a1c03bcd9201901Softwar",
      "title": "Software Prefetching for Indirect Memory Accesses: A Microarchitectural Perspective",
      "authors": [
        "SAM AINSWORTH",
        "TIMOTHY M. JONES"
      ],
      "year": 2019,
      "conference": "ACM TOCS",
      "category": "计算机体系结构与编译器优化",
      "keywords": [
        "软件预取",
        "间接内存访问",
        "编译器优化",
        "微架构",
        "内存延迟",
        "software prefetching",
        "indirect memory accesses"
      ],
      "abstract": "Many modern data processing and HPC workloads are heavily memory-latency bound. A tempting proposition to solve this is software prefetching, where special non-blocking loads are used to bring data into the cache hierarchy just before being required. However, these are difficult to insert to effectively improve performance, and techniques for automatic insertion are currently limited.\n\nThis paper develops a novel compiler pass to automatically generate software prefectches for indirect memory accesses, a special class of irregular memory accesses often seen in high-performance workloads. We evaluate this across a wide set of systems, all of which gain benefit from the technique. We then evaluate the extent to which good prefetch instructions are architecture dependent, and the class of programs that are particularly amenable. Across a set of memory-bound benchmarks, our automated pass achieves average speedups of 1.3x for an Intel Haswell processor, 1.1x for both an ARM Cortex-A57 and Qualcomm Kryo, 1.2x for a Cortex-72 and an Intel Kaby Lake, and 1.35x for an Intel Xeon Phi Knight's Landing, each of which is an out-of-order core, and performance improvements of 2.1x and 2.7x for the in-order ARM Cortex-A53 and first generation Intel Xeon Phi.",
      "htmlFile": "2019/f54a1c03bcd9201901Softwar/index.html"
    },
    {
      "id": "a5e7c39d8b4f2024Conflict",
      "title": "Conflict-free Replicated Data Types",
      "authors": [
        "Nuno Preguica",
        "Carlos Baquero",
        "Marc Shapiro"
      ],
      "year": 2018,
      "conference": "ArXiv",
      "category": "分布式系统",
      "keywords": [
        "冲突无关复制数据类型",
        "CRDT",
        "最终一致性",
        "复制协议",
        "弱一致性模型",
        "并发语义",
        "分布式数据"
      ],
      "abstract": "A conflict-free replicated data type (CRDT) is an abstract data type, with a well defined interface, designed to be replicated at multiple processes and exhibiting the following properties: (i) any replica can be modified without coordinating with another replicas; (ii) when any two replicas have received the same set of updates, they reach the same state, deterministically, by adopting mathematically sound rules to guarantee state convergence.",
      "htmlFile": "2018/a5e7c39d8b4f2024Conflict/index.html"
    },
    {
      "id": "43438b3c8e8f",
      "title": "Large-scale cluster management at Google with Borg",
      "authors": [
        "Abhishek Verma",
        "Luis Pedrosa",
        "Madhukar Korupolu",
        "David Oppenheimer",
        "Eric Tune",
        "John Wilkes"
      ],
      "year": 2015,
      "conference": "OSDI",
      "category": "分布式系统",
      "keywords": [
        "集群管理",
        "资源调度",
        "资源隔离",
        "资源回收",
        "Borg",
        "容器",
        "任务调度"
      ],
      "abstract": "Google's Borg system is a cluster manager that runs hundreds of thousands of jobs, from many thousands of different applications, across a number of clusters each with up to tens of thousands of machines.\nIt achieves high utilization by combining admission control, efficient task-packing, over-commitment, and machine sharing with process-level performance isolation. It supports high-availability applications with runtime features that minimize fault-recovery time, and scheduling policies that reduce the probability of correlated failures. Borg simplifies life for its users by offering a declarative job specification language, name service integration, real-time job monitoring, and tools to analyze and simulate system behavior.\nWe present a summary of the Borg system architecture and features, important design decisions, a quantitative analysis of some of its policy decisions, and a qualitative examination of lessons learned from a decade of operational experience with it.",
      "htmlFile": "2015/43438b3c8e8f/index.html"
    },
    {
      "id": "e1b9d8f2a4c720241607Delt",
      "title": "Delta State Replicated Data Types",
      "authors": [
        "Paulo Sergio Almeida",
        "Ali Shoker",
        "Carlos Baquero"
      ],
      "year": 2015,
      "conference": "NETYS",
      "category": "分布式系统",
      "keywords": [
        "Delta-CRDT",
        "无冲突复制数据类型",
        "最终一致性",
        "因果一致性",
        "状态融合",
        "抗熵算法",
        "delta-mutators"
      ],
      "abstract": "CRDTs are distributed data types that make eventual consistency of a distributed object possible and non ad-hoc. Specifically, state-based CRDTs ensure convergence through disseminating the entire state, that may be large, and merging it to other replicas; whereas operation-based CRDTs disseminate operations (i.e., small states) assuming an exactly-once reliable dissemination layer. We introduce _Delta State Conflict-Free Replicated Data Types_ (δ-CRDT) that can achieve the best of both worlds: small messages with an incremental nature, as in operation-based CRDTs, disseminated over unreliable communication channels, as in traditional state-based CRDTs. This is achieved by defining _delta-mutators_ to return a _delta-state_, typically with a much smaller size than the full state, that to be joined with both local and remote states. We introduce the δ-CRDT framework, and we explain it through establishing a correspondence to current state-based CRDTs. In addition, we present an anti-entropy algorithm for eventual convergence, and another one that ensures causal consistency. Finally, we introduce several δ-CRDT specifications of both well-known replicated datatypes and novel datatypes, including a generic map composition.",
      "htmlFile": "2015/e1b9d8f2a4c720241607Delt/index.html"
    },
    {
      "id": "8a3b5c7d9e1f",
      "title": "Track Join: Distributed Joins with Minimal Network Traffic",
      "authors": [
        "Orestis Polychroniou",
        "Rajkumar Sen",
        "Kenneth A. Ross"
      ],
      "year": 2014,
      "conference": "SIGMOD",
      "category": "数据库系统",
      "keywords": [
        "分布式连接",
        "网络通信优化",
        "Track Join",
        "查询处理",
        "并行数据库"
      ],
      "abstract": "Network communication is the slowest component of many operators in distributed parallel databases deployed for large-scale analytics. Whereas considerable work has focused on speeding up databases on modern hardware, communication reduction has received less attention. Existing parallel DBMSs rely on algorithms designed for disks with minor modifications for networks. A more complicated algorithm may burden the CPUs, but could avoid redundant transfers of tuples across the network. We introduce _track join_, a novel distributed join algorithm that minimizes network traffic by generating an optimal transfer schedule for each distinct join key. Track join extends the trade-off options between CPU and network. Our evaluation based on real and synthetic data shows that track join adapts to diverse cases and degrees of locality. Considering both network traffic and execution time, even with no locality, track join outperforms hash join on the most expensive queries of real workloads.",
      "htmlFile": "2014/8a3b5c7d9e1f/index.html"
    },
    {
      "id": "a3b9d1e8f2c7",
      "title": "BeyondCorp: A New Approach to Enterprise Security",
      "authors": [
        "Rory Ward",
        "Betsy Beyer"
      ],
      "year": 2014,
      "conference": "USENIX ;login:",
      "category": "网络安全",
      "keywords": [
        "零信任",
        "企业安全",
        "访问控制",
        "设备管理",
        "身份认证",
        "BeyondCorp",
        "无特权网络"
      ],
      "abstract": "Virtually every company today uses firewalls to enforce perimeter security. However, this security model is problematic because, when that perimeter is breached, an attacker has relatively easy access to a company’s privileged intranet. As companies adopt mobile and cloud technologies, the perimeter is becoming increasingly difficult to enforce. Google is taking a different approach to network security. We are removing the requirement for a privileged intranet and moving our corporate applications to the Internet.\n\nGoogle’s BeyondCorp initiative is moving to a new model that dispenses with a privileged corporate network. Instead, access depends solely on device and user credentials, regardless of a user’s network location—be it an enterprise location, a home network, or a hotel or coffee shop. All access to enterprise resources is fully authenticated, fully authorized, and fully encrypted based upon device state and user credentials. We can enforce fine-grained access to different parts of enterprise resources. As a result, all Google employees can work successfully from any network, and without the need for a traditional VPN connection into the privileged network. The user experience between local and remote access to enterprise resources is effectively identical, apart from potential differences in latency.",
      "htmlFile": "2014/a3b9d1e8f2c7/index.html"
    },
    {
      "id": "a7c3e9b12f8a",
      "title": "Cuckoo Filter: Practically Better Than Bloom",
      "authors": [
        "Bin Fan",
        "David G. Andersen",
        "Michael Kaminsky",
        "Michael D. Mitzenmacher"
      ],
      "year": 2014,
      "conference": "CoNEXT",
      "category": "数据结构与算法",
      "keywords": [
        "Cuckoo Filter",
        "Bloom Filter",
        "集合成员测试",
        "删除支持",
        "空间效率",
        "部分键Cuckoo哈希",
        "近似数据结构"
      ],
      "abstract": "In many networking systems, Bloom filters are used for high-speed set membership tests. They permit a small fraction of false positive answers with very good space efficiency. However, they do not permit deletion of items from the set, and previous attempts to extend \"standard\" Bloom filters to support deletion all degrade either space or performance.\nWe propose a new data structure called the _cuckoo filter_ that can replace Bloom filters for approximate set membership tests. Cuckoo filters support adding and removing items dynamically while achieving even higher performance than Bloom filters. For applications that store many items and target moderately low false positive rates, cuckoo filters have lower space overhead than space-optimized Bloom filters. Our experimental results also show that cuckoo filters outperform previous data structures that extend Bloom filters to support deletions substantially in both time and space.",
      "htmlFile": "2014/a7c3e9b12f8a/index.html"
    },
    {
      "id": "f9a7c3d8b4e12012PaxosLease",
      "title": "PaxosLease: Diskless Paxos for Leases",
      "authors": [
        "Marton Trencseni",
        "Attila Gazso",
        "Holger Reinhardt"
      ],
      "year": 2012,
      "conference": "Arxiv",
      "category": "分布式系统",
      "keywords": [
        "分布式算法",
        "租约",
        "Paxos",
        "容错",
        "一致性",
        "高可用性",
        "lease"
      ],
      "abstract": "This paper describes PaxosLease, a distributed algorithm for lease negotiation. PaxosLease is based on Paxos, but does not require disk writes or clock synchrony. PaxosLease is used for master lease negotiation in the open-source Keyspace and ScalienDB replicated key-value stores.",
      "htmlFile": "2012/f9a7c3d8b4e12012PaxosLease/index.html"
    },
    {
      "id": "a1b2c3d4e5f62014Compreh",
      "title": "Comprehensive Experimental Analyses of Automotive Attack Surfaces",
      "authors": [
        "Stephen Checkoway",
        "Damon McCoy",
        "Brian Kantor",
        "Danny Anderson",
        "Hovav Shacham",
        "Stefan Savage",
        "Karl Koscher",
        "Alexei Czeskis",
        "Franziska Roesner",
        "Tadayoshi Kohno"
      ],
      "year": 2011,
      "conference": "USENIX Security",
      "category": "网络安全",
      "keywords": [
        "汽车安全",
        "攻击面分析",
        "远程攻击",
        "漏洞利用",
        "车载网络",
        "ECU安全",
        "无线攻击"
      ],
      "abstract": "Modern automobiles are pervasively computerized, and hence potentially vulnerable to attack. However, while previous research has shown that the _internal_ networks within some modern cars are insecure, the associated threat model-- requiring _prior physical access_ --has justifiably been viewed as unrealistic. Thus, it remains an open question if automobiles can also be susceptible to _remote_ compromise. Our work seeks to put this question to rest by systematically analyzing the _external_ attack surface of a modern automobile. We discover that remote exploitation is feasible via a broad range of attack vectors (including mechanics tools, CD players, Bluetooth and cellular radio), and further, that wireless communications channels allow long distance vehicle control, location tracking, in-cabin audio exfiltration and theft. Finally, we discuss the structural characteristics of the automotive ecosystem that give rise to such problems and highlight the practical challenges in mitigating them.",
      "htmlFile": "2011/a1b2c3d4e5f62014Compreh/index.html"
    },
    {
      "id": "a1e2a3d4b5c6",
      "title": "A File is Not a File: Understanding the I/O Behavior of Apple Desktop Applications",
      "authors": [
        "Tyler Harter",
        "Chris Dragga",
        "Michael Vaughn",
        "Andrea C. Arpaci-Dusseau",
        "Remzi H. Arpaci-Dusseau"
      ],
      "year": 2011,
      "conference": "SOSP",
      "category": "存储系统",
      "keywords": [
        "I/O行为",
        "文件系统",
        "桌面应用",
        "工作负载分析",
        "Apple应用",
        "fsync",
        "原子操作"
      ],
      "abstract": "We analyze the I/O behavior of _iBench_, a new collection of productivity and multimedia application workloads. Our analysis reveals a number of differences between iBench and typical file-system workload studies, including the complex organization of modern files, the lack of pure sequential access, the influence of underlying frameworks on I/O patterns, the widespread use of file synchronization and atomic operations, and the prevalence of threads. Our results have strong ramifications for the design of next generation local and cloud-based storage systems.",
      "htmlFile": "2011/a1e2a3d4b5c6/index.html"
    },
    {
      "id": "a7c3e9b4f120",
      "title": "Network Applications of Bloom Filters: A Survey",
      "authors": [
        "Andrei Broder",
        "Michael Mitzenmacher"
      ],
      "year": 2004,
      "conference": "Internet Math.",
      "category": "计算机网络",
      "keywords": [
        "Bloom Filter",
        "网络应用",
        "数据摘要",
        "成员查询",
        "分布式系统",
        "资源路由",
        "包路由"
      ],
      "abstract": "A Bloom filter is a simple space-efficient randomized data structure for representing a set in order to support membership queries. Bloom filters allow false positives but the space savings often outweigh this drawback when the probability of an error is controlled. Bloom filters have been used in database applications since the 1970s, but only in recent years have they become popular in the networking literature. The aim of this paper is to survey the ways in which Bloom filters have been used and modified in a variety of network problems, with the aim of providing a unified mathematical and practical framework for understanding them and stimulating their use in future applications.",
      "htmlFile": "2004/a7c3e9b4f120/index.html"
    },
    {
      "id": "b6e2c4a9f8122023makingre",
      "title": "Making reliable distributed systems in the presence of software errors",
      "authors": [
        "Joe Armstrong"
      ],
      "year": 2003,
      "conference": "KTH Doctoral Thesis",
      "category": "分布式系统",
      "keywords": [
        "Erlang",
        "容错系统",
        "分布式系统",
        "软件错误",
        "并发编程",
        "OTP",
        "电信应用"
      ],
      "abstract": "The work described in this thesis is the result of a research program started in 1981 to find better ways of programming Telecom applications. These applications are large programs which despite careful testing will probably contain many errors when the program is put into service. We assume that such programs do contain errors, and investigate methods for building reliable systems despite such errors.\n\nThe research has resulted in the development of a new programming language (called Erlang), together with a design methodology, and set of libraries for building robust systems (called OTP). At the time of writing the technology described here is used in a number of major Ericsson, and Nortel products. A number of small companies have also been formed which exploit the technology.\n\nThe central problem addressed by this thesis is the problem of constructing reliable systems from programs which may themselves contain errors. Constructing such systems imposes a number of requirements on any programming language that is to be used for the construction. I discuss these language requirements, and show how they are satisfied by Erlang.\n\nProblems can be solved in a programming language, or in the standard libraries which accompany the language. I argue how certain of the requirements necessary to build a fault-tolerant system are solved in the language, and others are solved in the standard libraries. Together these form a basis for building fault-tolerant software systems.\n\nNo theory is complete without proof that the ideas work in practice. To demonstrate that these ideas work in practice I present a number of case studies of large commercially successful products which use this technology. At the time of writing the largest of these projects is a major Ericsson product, having over a million lines of Erlang code. This product (the AXD301) is thought to be one of the most reliable products ever made by Ericsson.\n\nFinally, I ask if the goal of finding better ways to program Telecom applications was fulfilled–I also point to areas where I think the system could be improved.",
      "htmlFile": "2003/b6e2c4a9f8122023makingre/index.html"
    },
    {
      "id": "b8d4e5f1a309",
      "title": "A comparison of file system workloads",
      "authors": [
        "Drew Roselli",
        "Jacob R. Lorch",
        "Thomas E. Anderson"
      ],
      "year": 2000,
      "conference": "ATC",
      "category": "文件系统",
      "keywords": [
        "文件系统",
        "工作负载分析",
        "磁盘I/O",
        "内存映射",
        "缓存性能",
        "块生命周期",
        "访问模式"
      ],
      "abstract": "In this paper, we describe the collection and analysis of file system traces from a variety of different environments, including both UNIX and NT systems, clients and servers, and instructional and production systems. Our goal is to understand how modern workloads affect the ability of file systems to provide high performance to users. Because of the increasing gap between processor speed and disk latency, file system performance is largely determined by its disk behavior. Therefore we primarily focus on the disk I/O aspects of the traces. We find that more processes access files via the memory-map interface than through the read interface. However, because many processes memory-map a small set of files, these files are likely to be cached. We also find that file access has a bimodal distribution pattern: some files are written repeatedly without being read; other files are almost exclusively read. We develop a new metric for measuring file lifetime that accounts for files that are never deleted. Using this metric, we find that the average block lifetime for some workloads is significantly longer than the 30-second write delay used by many file systems. However, all workloads show lifetime locality: the same files tend to be overwritten multiple times.",
      "htmlFile": "2000/b8d4e5f1a309/index.html"
    },
    {
      "id": "b5f2a1c8d9e7f42024QuickChec",
      "title": "QuickCheck: A Lightweight Tool for Random Testing of Haskell Programs",
      "authors": [
        "Koen Claessen",
        "John Hughes"
      ],
      "year": 1999,
      "conference": "ICFP",
      "category": "软件工程",
      "keywords": [
        "QuickCheck",
        "随机测试",
        "Haskell",
        "属性测试",
        "函数式编程",
        "测试工具",
        "规格说明"
      ],
      "abstract": "QuickCheck is a tool which aids the Haskell programmer in formulating and testing properties of programs. Properties are described as Haskell functions, and can be automatically tested on random input, but it is also possible to define custom test data generators. We present a number of case studies, in which the tool was successfully used, and also point out some pitfalls to avoid. Random testing is especially suitable for functional programs because properties can be stated at a fine grain. When a function is built from separately tested components, then random testing suffices to obtain good coverage of the definition under test.",
      "htmlFile": "1999/b5f2a1c8d9e7f42024QuickChec/index.html"
    },
    {
      "id": "9a5c4f0e9b5c",
      "title": "Dynamic File Allocation in Disk Arrays",
      "authors": [
        "Gerhard Weikum",
        "Peter Zabback",
        "Peter Scheuermann"
      ],
      "year": 1991,
      "conference": "SIGMOD",
      "category": "数据库系统",
      "keywords": [
        "磁盘阵列",
        "动态文件分配",
        "负载均衡",
        "数据布局",
        "去集群",
        "部分重组",
        "I/O性能"
      ],
      "abstract": "Large arrays of small disks are being considered as a promising approach to high performance I/O architectures. In this paper we deal with the problem of data placement in such a disk array. The prevalent approach is to decluster large files across a number of disks so as to minimize the access time to a file and balance the I/O load across the disks. The data placement problem entails determining the number of disks and the set of disks across which a file is declustered. Unlike previous work, this paper does not assume that all files are allocated at the same time but rather considers dynamic file creations. This makes the placement problem considerably harder because each placement decision has to take into account the current allocation state and the access frequencies of the disks and the existing files. As a result, file creation may involve partial reorganization on one or more disks. The paper proposes heuristic algorithms for the placement of dynamically created files. The algorithms provide a good compromise between maximizing I/O performance of the disk array and minimizing the work invested in partial reorganizations. The paper presents preliminary performance results of various alternative algorithms under a synthetic workload.",
      "htmlFile": "1991/9a5c4f0e9b5c/index.html"
    },
    {
      "id": "d3f1a8b7c9e2",
      "title": "What Every Computer Scientist Should Know About Floating-Point Arithmetic",
      "authors": [
        "DAVID GOLDBERG"
      ],
      "year": 1991,
      "conference": "ACM Comput. Surv.",
      "category": "计算机算术",
      "keywords": [
        "浮点运算",
        "IEEE 754",
        "舍入误差",
        "异常处理",
        "数值稳定性",
        "guard digit",
        "ULP"
      ],
      "abstract": "Floating-point arithmetic is considered an esoteric subject by many people. This is rather surprising, because floating-point is ubiquitous in computer systems: Almost every language has a floating-point datatype; computers from PCs to supercomputers have floating-point accelerators; most compilers will be called upon to compile floating-point algorithms from time to time, and virtually every operating system must respond to floating-point exceptions such as overflow. This paper presents a tutorial on the aspects of floating-point that have a direct impact on designers of computer systems. It begins with background on floating-point representation and rounding error, continues with a discussion of the IEEE floating-point standard, and concludes with examples of how computer system builders can better support floating point.",
      "htmlFile": "1991/d3f1a8b7c9e2/index.html"
    },
    {
      "id": "0d1b2a3c4e5f1981I",
      "title": "I/O COMPLEXITY: THE RED-BLUE PEBBLE GAME",
      "authors": [
        "Hong, Jia-Wei",
        "H. T. Kung"
      ],
      "year": 1981,
      "conference": "STOC",
      "category": "计算机理论",
      "keywords": [
        "I/O复杂度",
        "红蓝卵石游戏",
        "下界",
        "FFT",
        "矩阵乘法",
        "图划分"
      ],
      "abstract": "In this paper, the red-blue pebble game is proposed to model the input-output complexity of algorithms. Using the pebble game formulation, a number of lower bound results for the I/O requirement are proven. For example, it is shown that to perform the n-point FFT or the ordinary nxn matrix multiplication algorithm with O(S) memory, at least Ω(n log n/log S) or Ω(n^3/√S), respectively, time is needed for the I/O. Similar results are obtained for algorithms for several other problems. All of the lower bounds presented are the best possible in the sense that they are achievable by certain decomposition schemes.\n\nResults of this paper may provide insight into the difficult task of balancing I/O and computation in special-purpose system designs. For example, for the n-point FFT, the lower bound on I/O time implies that an S-point device achieving a speed-up ratio of order log S over the conventional O(n log n) time implementation is all one can hope for.",
      "htmlFile": "1981/0d1b2a3c4e5f1981I/index.html"
    },
    {
      "id": "a9a4e5d3b8c4",
      "title": "Time, Clocks, and the Ordering of Events in a Distributed System",
      "authors": [
        "Leslie Lamport"
      ],
      "year": 1978,
      "conference": "CACM",
      "category": "分布式系统",
      "keywords": [
        "分布式系统",
        "逻辑时钟",
        "事件排序",
        "时钟同步",
        "互斥问题",
        "happened before",
        "partial ordering"
      ],
      "abstract": "The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become.",
      "htmlFile": "1978/a9a4e5d3b8c4/index.html"
    },
    {
      "id": "e9f84b12a5c91978Can",
      "title": "Can Programming Be Liberated from the von Neumann Style? A Functional Style and Its Algebra of Programs",
      "authors": [
        "John Backus"
      ],
      "year": 1977,
      "conference": "Commun. ACM",
      "category": "编程语言理论",
      "keywords": [
        "函数式编程",
        "程序代数",
        "冯·诺依曼风格",
        "FP 系统",
        "结合形式"
      ],
      "abstract": "Conventional programming languages are growing ever more enormous, but not stronger. Inherent defects at the most basic level cause them to be both fat and weak: their primitive word-at-a-time style of programming inherited from their common ancestor—the von Neumann computer, their close coupling of semantics to state transitions, their division of programming into a world of expressions and a world of statements, their inability to effectively use powerful combining forms for building new programs from existing ones, and their lack of useful mathematical properties for reasoning about programs.\n\nAn alternative functional style of programming is founded on the use of combining forms for creating programs. Functional programs deal with structured data, are often nonrepetitive and nonrecursive, are hierarchically constructed, do not name their arguments, and do not require the complex machinery of procedure declarations to become generally applicable. Combining forms can use high level programs to build still higher level ones in a style not possible in conventional languages.\n\nAssociated with the functional style of programming is an algebra of programs whose variables range over programs and whose operations are combining forms. This algebra can be used to transform programs and to solve equations whose “unknowns” are programs in much the same way one transforms equations in high school algebra. These transformations are given by algebraic laws and are carried out in the same language in which programs are written. Combining forms are chosen not only for their programming power but also for the power of their associated algebraic laws. General theorems of the algebra give the detailed behavior and termination conditions for large classes of programs.\n\nA new class of computing systems uses the functional programming style both in its programming language and in its state transition rules. Unlike von Neumann languages, these systems have semantics loosely coupled to states—only one state transition occurs per major computation.",
      "htmlFile": "1977/e9f84b12a5c91978Can/index.html"
    },
    {
      "id": "d7f8a4d7b3b2",
      "title": "The UNIX Time-Sharing System",
      "authors": [
        "Dennis M. Ritchie",
        "Ken Thompson"
      ],
      "year": 1974,
      "conference": "CACM",
      "category": "操作系统",
      "keywords": [
        "UNIX",
        "文件系统",
        "进程管理",
        "Shell",
        "时间共享系统",
        "PDP-11",
        "操作系统设计"
      ],
      "abstract": "UNIX is a general-purpose, multi-user, interactive operating system for the Digital Equipment Corporation PDP-11/40 and 11/45 computers. It offers a number of features seldom found even in larger operating systems, including: (1) a hierarchical file system incorporating demountable volumes; (2) compatible file, device, and inter-process i/o; (3) the ability to initiate asynchronous processes; (4) system command language selectable on a per-user basis; and (5) over 100 subsystems including a dozen languages. This paper discusses the nature and implementation of the file system and of the user command interface.",
      "htmlFile": "1974/d7f8a4d7b3b2/index.html"
    }
  ]
}